{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Others"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     7
    ]
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "\n",
    "import logging\n",
    "\n",
    "logger = logging.getLogger()\n",
    "\n",
    "\n",
    "def init_logger(log_file=None, log_file_level=logging.NOTSET):\n",
    "    log_format = logging.Formatter(\"[%(asctime)s %(levelname)s] %(message)s\")\n",
    "    logger = logging.getLogger()\n",
    "    logger.setLevel(logging.INFO)\n",
    "\n",
    "    console_handler = logging.StreamHandler()\n",
    "    console_handler.setFormatter(log_format)\n",
    "    logger.handlers = [console_handler]\n",
    "\n",
    "    if log_file and log_file != '':\n",
    "        file_handler = logging.FileHandler(log_file)\n",
    "        file_handler.setLevel(log_file_level)\n",
    "        file_handler.setFormatter(log_format)\n",
    "        logger.addHandler(file_handler)\n",
    "\n",
    "    return logger"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python Rogue: Eval Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function, unicode_literals, division\n",
    "\n",
    "import os\n",
    "import re\n",
    "import codecs\n",
    "import platform\n",
    "\n",
    "from subprocess import check_output\n",
    "from tempfile import mkdtemp\n",
    "from functools import partial\n",
    "\n",
    "try:\n",
    "    from configparser import ConfigParser\n",
    "except ImportError:\n",
    "    from ConfigParser import ConfigParser\n",
    "\n",
    "#from pyrouge.utils import log\n",
    "#from pyrouge.utils.file_utils import verify_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     20,
     30,
     54,
     238,
     454,
     460,
     590
    ]
   },
   "outputs": [],
   "source": [
    "REMAP = {\"-lrb-\": \"(\", \"-rrb-\": \")\", \"-lcb-\": \"{\", \"-rcb-\": \"}\",\n",
    "         \"-lsb-\": \"[\", \"-rsb-\": \"]\", \"``\": '\"', \"''\": '\"'}\n",
    "\n",
    "\n",
    "def clean(x):\n",
    "    return re.sub(\n",
    "            r\"-lrb-|-rrb-|-lcb-|-rcb-|-lsb-|-rsb-|``|''\",\n",
    "            lambda m: REMAP.get(m.group()), x)\n",
    "\n",
    "\n",
    "class DirectoryProcessor:\n",
    "\n",
    "    @staticmethod\n",
    "    def process(input_dir, output_dir, function):\n",
    "        \"\"\"\n",
    "        Apply function to all files in input_dir and save the resulting ouput\n",
    "        files in output_dir.\n",
    "        \"\"\"\n",
    "        if not os.path.exists(output_dir):\n",
    "            os.makedirs(output_dir)\n",
    "        logger = log.get_global_console_logger()\n",
    "        logger.info(\"Processing files in {}.\".format(input_dir))\n",
    "        input_file_names = os.listdir(input_dir)\n",
    "        for input_file_name in input_file_names:\n",
    "            input_file = os.path.join(input_dir, input_file_name)\n",
    "            with codecs.open(input_file, \"r\", encoding=\"UTF-8\") as f:\n",
    "                input_string = f.read()\n",
    "            output_string = function(input_string)\n",
    "            output_file = os.path.join(output_dir, input_file_name)\n",
    "            with codecs.open(output_file, \"w\", encoding=\"UTF-8\") as f:\n",
    "                f.write(clean(output_string.lower()))\n",
    "        logger.info(\"Saved processed files to {}.\".format(output_dir))\n",
    "\n",
    "\n",
    "class Rouge155(object):\n",
    "    \"\"\"\n",
    "    This is a wrapper for the ROUGE 1.5.5 summary evaluation package.\n",
    "    This class is designed to simplify the evaluation process by:\n",
    "        1) Converting summaries into a format ROUGE understands.\n",
    "        2) Generating the ROUGE configuration file automatically based\n",
    "            on filename patterns.\n",
    "    This class can be used within Python like this:\n",
    "    rouge = Rouge155()\n",
    "    rouge.system_dir = 'test/systems'\n",
    "    rouge.model_dir = 'test/models'\n",
    "    # The system filename pattern should contain one group that\n",
    "    # matches the document ID.\n",
    "    rouge.system_filename_pattern = 'SL.P.10.R.11.SL062003-(\\d+).html'\n",
    "    # The model filename pattern has '#ID#' as a placeholder for the\n",
    "    # document ID. If there are multiple model summaries, pyrouge\n",
    "    # will use the provided regex to automatically match them with\n",
    "    # the corresponding system summary. Here, [A-Z] matches\n",
    "    # multiple model summaries for a given #ID#.\n",
    "    rouge.model_filename_pattern = 'SL.P.10.R.[A-Z].SL062003-#ID#.html'\n",
    "    rouge_output = rouge.evaluate()\n",
    "    print(rouge_output)\n",
    "    output_dict = rouge.output_to_dict(rouge_ouput)\n",
    "    print(output_dict)\n",
    "    ->    {'rouge_1_f_score': 0.95652,\n",
    "         'rouge_1_f_score_cb': 0.95652,\n",
    "         'rouge_1_f_score_ce': 0.95652,\n",
    "         'rouge_1_precision': 0.95652,\n",
    "        [...]\n",
    "    To evaluate multiple systems:\n",
    "        rouge = Rouge155()\n",
    "        rouge.system_dir = '/PATH/TO/systems'\n",
    "        rouge.model_dir = 'PATH/TO/models'\n",
    "        for system_id in ['id1', 'id2', 'id3']:\n",
    "            rouge.system_filename_pattern = \\\n",
    "                'SL.P/.10.R.{}.SL062003-(\\d+).html'.format(system_id)\n",
    "            rouge.model_filename_pattern = \\\n",
    "                'SL.P.10.R.[A-Z].SL062003-#ID#.html'\n",
    "            rouge_output = rouge.evaluate(system_id)\n",
    "            print(rouge_output)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, rouge_dir=None, rouge_args=None, temp_dir = None):\n",
    "        \"\"\"\n",
    "        Create a Rouge155 object.\n",
    "            rouge_dir:  Directory containing Rouge-1.5.5.pl\n",
    "            rouge_args: Arguments to pass through to ROUGE if you\n",
    "                        don't want to use the default pyrouge\n",
    "                        arguments.\n",
    "        \"\"\"\n",
    "        self.temp_dir=temp_dir\n",
    "        self.log = log.get_global_console_logger()\n",
    "        self.__set_dir_properties()\n",
    "        self._config_file = None\n",
    "        self._settings_file = self.__get_config_path()\n",
    "        self.__set_rouge_dir(rouge_dir)\n",
    "        self.args = self.__clean_rouge_args(rouge_args)\n",
    "        self._system_filename_pattern = None\n",
    "        self._model_filename_pattern = None\n",
    "\n",
    "    def save_home_dir(self):\n",
    "        config = ConfigParser()\n",
    "        section = 'pyrouge settings'\n",
    "        config.add_section(section)\n",
    "        config.set(section, 'home_dir', self._home_dir)\n",
    "        with open(self._settings_file, 'w') as f:\n",
    "            config.write(f)\n",
    "        self.log.info(\"Set ROUGE home directory to {}.\".format(self._home_dir))\n",
    "\n",
    "    @property\n",
    "    def settings_file(self):\n",
    "        \"\"\"\n",
    "        Path of the setttings file, which stores the ROUGE home dir.\n",
    "        \"\"\"\n",
    "        return self._settings_file\n",
    "\n",
    "    @property\n",
    "    def bin_path(self):\n",
    "        \"\"\"\n",
    "        The full path of the ROUGE binary (although it's technically\n",
    "        a script), i.e. rouge_home_dir/ROUGE-1.5.5.pl\n",
    "        \"\"\"\n",
    "        if self._bin_path is None:\n",
    "            raise Exception(\n",
    "                \"ROUGE path not set. Please set the ROUGE home directory \"\n",
    "                \"and ensure that ROUGE-1.5.5.pl exists in it.\")\n",
    "        return self._bin_path\n",
    "\n",
    "    @property\n",
    "    def system_filename_pattern(self):\n",
    "        \"\"\"\n",
    "        The regular expression pattern for matching system summary\n",
    "        filenames. The regex string.\n",
    "        E.g. \"SL.P.10.R.11.SL062003-(\\d+).html\" will match the system\n",
    "        filenames in the SPL2003/system folder of the ROUGE SPL example\n",
    "        in the \"sample-test\" folder.\n",
    "        Currently, there is no support for multiple systems.\n",
    "        \"\"\"\n",
    "        return self._system_filename_pattern\n",
    "\n",
    "    @system_filename_pattern.setter\n",
    "    def system_filename_pattern(self, pattern):\n",
    "        self._system_filename_pattern = pattern\n",
    "\n",
    "    @property\n",
    "    def model_filename_pattern(self):\n",
    "        \"\"\"\n",
    "        The regular expression pattern for matching model summary\n",
    "        filenames. The pattern needs to contain the string \"#ID#\",\n",
    "        which is a placeholder for the document ID.\n",
    "        E.g. \"SL.P.10.R.[A-Z].SL062003-#ID#.html\" will match the model\n",
    "        filenames in the SPL2003/system folder of the ROUGE SPL\n",
    "        example in the \"sample-test\" folder.\n",
    "        \"#ID#\" is a placeholder for the document ID which has been\n",
    "        matched by the \"(\\d+)\" part of the system filename pattern.\n",
    "        The different model summaries for a given document ID are\n",
    "        matched by the \"[A-Z]\" part.\n",
    "        \"\"\"\n",
    "        return self._model_filename_pattern\n",
    "\n",
    "    @model_filename_pattern.setter\n",
    "    def model_filename_pattern(self, pattern):\n",
    "        self._model_filename_pattern = pattern\n",
    "\n",
    "    @property\n",
    "    def config_file(self):\n",
    "        return self._config_file\n",
    "\n",
    "    @config_file.setter\n",
    "    def config_file(self, path):\n",
    "        config_dir, _ = os.path.split(path)\n",
    "        verify_dir(config_dir, \"configuration file\")\n",
    "        self._config_file = path\n",
    "\n",
    "    def split_sentences(self):\n",
    "        \"\"\"\n",
    "        ROUGE requires texts split into sentences. In case the texts\n",
    "        are not already split, this method can be used.\n",
    "        \"\"\"\n",
    "        from pyrouge.utils.sentence_splitter import PunktSentenceSplitter\n",
    "        self.log.info(\"Splitting sentences.\")\n",
    "        ss = PunktSentenceSplitter()\n",
    "        sent_split_to_string = lambda s: \"\\n\".join(ss.split(s))\n",
    "        process_func = partial(\n",
    "            DirectoryProcessor.process, function=sent_split_to_string)\n",
    "        self.__process_summaries(process_func)\n",
    "\n",
    "    @staticmethod\n",
    "    def convert_summaries_to_rouge_format(input_dir, output_dir):\n",
    "        \"\"\"\n",
    "        Convert all files in input_dir into a format ROUGE understands\n",
    "        and saves the files to output_dir. The input files are assumed\n",
    "        to be plain text with one sentence per line.\n",
    "            input_dir:  Path of directory containing the input files.\n",
    "            output_dir: Path of directory in which the converted files\n",
    "                        will be saved.\n",
    "        \"\"\"\n",
    "        DirectoryProcessor.process(\n",
    "            input_dir, output_dir, Rouge155.convert_text_to_rouge_format)\n",
    "\n",
    "    @staticmethod\n",
    "    def convert_text_to_rouge_format(text, title=\"dummy title\"):\n",
    "        \"\"\"\n",
    "        Convert a text to a format ROUGE understands. The text is\n",
    "        assumed to contain one sentence per line.\n",
    "            text:   The text to convert, containg one sentence per line.\n",
    "            title:  Optional title for the text. The title will appear\n",
    "                    in the converted file, but doesn't seem to have\n",
    "                    any other relevance.\n",
    "        Returns: The converted text as string.\n",
    "        \"\"\"\n",
    "        # sentences = text.split(\"\\n\")\n",
    "        sentences = text.split(\"<q>\")\n",
    "        sent_elems = [\n",
    "            \"<a name=\\\"{i}\\\">[{i}]</a> <a href=\\\"#{i}\\\" id={i}>\"\n",
    "            \"{text}</a>\".format(i=i, text=sent)\n",
    "            for i, sent in enumerate(sentences, start=1)]\n",
    "        html = \"\"\"<html>\n",
    "<head>\n",
    "<title>{title}</title>\n",
    "</head>\n",
    "<body bgcolor=\"white\">\n",
    "{elems}\n",
    "</body>\n",
    "</html>\"\"\".format(title=title, elems=\"\\n\".join(sent_elems))\n",
    "\n",
    "        return html\n",
    "\n",
    "    @staticmethod\n",
    "    def write_config_static(system_dir, system_filename_pattern,\n",
    "                            model_dir, model_filename_pattern,\n",
    "                            config_file_path, system_id=None):\n",
    "        \"\"\"\n",
    "        Write the ROUGE configuration file, which is basically a list\n",
    "        of system summary files and their corresponding model summary\n",
    "        files.\n",
    "        pyrouge uses regular expressions to automatically find the\n",
    "        matching model summary files for a given system summary file\n",
    "        (cf. docstrings for system_filename_pattern and\n",
    "        model_filename_pattern).\n",
    "            system_dir:                 Path of directory containing\n",
    "                                        system summaries.\n",
    "            system_filename_pattern:    Regex string for matching\n",
    "                                        system summary filenames.\n",
    "            model_dir:                  Path of directory containing\n",
    "                                        model summaries.\n",
    "            model_filename_pattern:     Regex string for matching model\n",
    "                                        summary filenames.\n",
    "            config_file_path:           Path of the configuration file.\n",
    "            system_id:                  Optional system ID string which\n",
    "                                        will appear in the ROUGE output.\n",
    "        \"\"\"\n",
    "        system_filenames = [f for f in os.listdir(system_dir)]\n",
    "        system_models_tuples = []\n",
    "\n",
    "        system_filename_pattern = re.compile(system_filename_pattern)\n",
    "        for system_filename in sorted(system_filenames):\n",
    "            match = system_filename_pattern.match(system_filename)\n",
    "            if match:\n",
    "                id = match.groups(0)[0]\n",
    "                model_filenames = [model_filename_pattern.replace('#ID#',id)]\n",
    "                # model_filenames = Rouge155.__get_model_filenames_for_id(\n",
    "                #     id, model_dir, model_filename_pattern)\n",
    "                system_models_tuples.append(\n",
    "                    (system_filename, sorted(model_filenames)))\n",
    "        if not system_models_tuples:\n",
    "            raise Exception(\n",
    "                \"Did not find any files matching the pattern {} \"\n",
    "                \"in the system summaries directory {}.\".format(\n",
    "                    system_filename_pattern.pattern, system_dir))\n",
    "\n",
    "        with codecs.open(config_file_path, 'w', encoding='utf-8') as f:\n",
    "            f.write('<ROUGE-EVAL version=\"1.55\">')\n",
    "            for task_id, (system_filename, model_filenames) in enumerate(\n",
    "                    system_models_tuples, start=1):\n",
    "\n",
    "                eval_string = Rouge155.__get_eval_string(\n",
    "                    task_id, system_id,\n",
    "                    system_dir, system_filename,\n",
    "                    model_dir, model_filenames)\n",
    "                f.write(eval_string)\n",
    "            f.write(\"</ROUGE-EVAL>\")\n",
    "\n",
    "    def write_config(self, config_file_path=None, system_id=None):\n",
    "        \"\"\"\n",
    "        Write the ROUGE configuration file, which is basically a list\n",
    "        of system summary files and their matching model summary files.\n",
    "        This is a non-static version of write_config_file_static().\n",
    "            config_file_path:   Path of the configuration file.\n",
    "            system_id:          Optional system ID string which will\n",
    "                                appear in the ROUGE output.\n",
    "        \"\"\"\n",
    "        if not system_id:\n",
    "            system_id = 1\n",
    "        if (not config_file_path) or (not self._config_dir):\n",
    "            self._config_dir = mkdtemp(dir=self.temp_dir)\n",
    "            config_filename = \"rouge_conf.xml\"\n",
    "        else:\n",
    "            config_dir, config_filename = os.path.split(config_file_path)\n",
    "            verify_dir(config_dir, \"configuration file\")\n",
    "        self._config_file = os.path.join(self._config_dir, config_filename)\n",
    "        Rouge155.write_config_static(\n",
    "            self._system_dir, self._system_filename_pattern,\n",
    "            self._model_dir, self._model_filename_pattern,\n",
    "            self._config_file, system_id)\n",
    "        self.log.info(\n",
    "            \"Written ROUGE configuration to {}\".format(self._config_file))\n",
    "\n",
    "    def evaluate(self, system_id=1, rouge_args=None):\n",
    "        \"\"\"\n",
    "        Run ROUGE to evaluate the system summaries in system_dir against\n",
    "        the model summaries in model_dir. The summaries are assumed to\n",
    "        be in the one-sentence-per-line HTML format ROUGE understands.\n",
    "            system_id:  Optional system ID which will be printed in\n",
    "                        ROUGE's output.\n",
    "        Returns: Rouge output as string.\n",
    "        \"\"\"\n",
    "        self.write_config(system_id=system_id)\n",
    "        options = self.__get_options(rouge_args)\n",
    "        command = [self._bin_path] + options\n",
    "        self.log.info(\n",
    "            \"Running ROUGE with command {}\".format(\" \".join(command)))\n",
    "        rouge_output = check_output(command).decode(\"UTF-8\")\n",
    "        return rouge_output\n",
    "\n",
    "    def convert_and_evaluate(self, system_id=1,\n",
    "                             split_sentences=False, rouge_args=None):\n",
    "        \"\"\"\n",
    "        Convert plain text summaries to ROUGE format and run ROUGE to\n",
    "        evaluate the system summaries in system_dir against the model\n",
    "        summaries in model_dir. Optionally split texts into sentences\n",
    "        in case they aren't already.\n",
    "        This is just a convenience method combining\n",
    "        convert_summaries_to_rouge_format() and evaluate().\n",
    "            split_sentences:    Optional argument specifying if\n",
    "                                sentences should be split.\n",
    "            system_id:          Optional system ID which will be printed\n",
    "                                in ROUGE's output.\n",
    "        Returns: ROUGE output as string.\n",
    "        \"\"\"\n",
    "        if split_sentences:\n",
    "            self.split_sentences()\n",
    "        self.__write_summaries()\n",
    "        rouge_output = self.evaluate(system_id, rouge_args)\n",
    "        return rouge_output\n",
    "\n",
    "    def output_to_dict(self, output):\n",
    "        \"\"\"\n",
    "        Convert the ROUGE output into python dictionary for further\n",
    "        processing.\n",
    "        \"\"\"\n",
    "        #0 ROUGE-1 Average_R: 0.02632 (95%-conf.int. 0.02632 - 0.02632)\n",
    "        pattern = re.compile(\n",
    "            r\"(\\d+) (ROUGE-\\S+) (Average_\\w): (\\d.\\d+) \"\n",
    "            r\"\\(95%-conf.int. (\\d.\\d+) - (\\d.\\d+)\\)\")\n",
    "        results = {}\n",
    "        for line in output.split(\"\\n\"):\n",
    "            match = pattern.match(line)\n",
    "            if match:\n",
    "                sys_id, rouge_type, measure, result, conf_begin, conf_end = \\\n",
    "                    match.groups()\n",
    "                measure = {\n",
    "                    'Average_R': 'recall',\n",
    "                    'Average_P': 'precision',\n",
    "                    'Average_F': 'f_score'\n",
    "                    }[measure]\n",
    "                rouge_type = rouge_type.lower().replace(\"-\", '_')\n",
    "                key = \"{}_{}\".format(rouge_type, measure)\n",
    "                results[key] = float(result)\n",
    "                results[\"{}_cb\".format(key)] = float(conf_begin)\n",
    "                results[\"{}_ce\".format(key)] = float(conf_end)\n",
    "        return results\n",
    "\n",
    "    ###################################################################\n",
    "    # Private methods\n",
    "\n",
    "    def __set_rouge_dir(self, home_dir=None):\n",
    "        \"\"\"\n",
    "        Verfify presence of ROUGE-1.5.5.pl and data folder, and set\n",
    "        those paths.\n",
    "        \"\"\"\n",
    "        if not home_dir:\n",
    "            self._home_dir = self.__get_rouge_home_dir_from_settings()\n",
    "        else:\n",
    "            self._home_dir = home_dir\n",
    "            self.save_home_dir()\n",
    "        self._bin_path = os.path.join(self._home_dir, 'ROUGE-1.5.5.pl')\n",
    "        self.data_dir = os.path.join(self._home_dir, 'data')\n",
    "        if not os.path.exists(self._bin_path):\n",
    "            raise Exception(\n",
    "                \"ROUGE binary not found at {}. Please set the \"\n",
    "                \"correct path by running pyrouge_set_rouge_path \"\n",
    "                \"/path/to/rouge/home.\".format(self._bin_path))\n",
    "\n",
    "    def __get_rouge_home_dir_from_settings(self):\n",
    "        config = ConfigParser()\n",
    "        with open(self._settings_file) as f:\n",
    "            if hasattr(config, \"read_file\"):\n",
    "                config.read_file(f)\n",
    "            else:\n",
    "                # use deprecated python 2.x method\n",
    "                config.readfp(f)\n",
    "        rouge_home_dir = config.get('pyrouge settings', 'home_dir')\n",
    "        return rouge_home_dir\n",
    "\n",
    "    @staticmethod\n",
    "    def __get_eval_string(\n",
    "            task_id, system_id,\n",
    "            system_dir, system_filename,\n",
    "            model_dir, model_filenames):\n",
    "        \"\"\"\n",
    "        ROUGE can evaluate several system summaries for a given text\n",
    "        against several model summaries, i.e. there is an m-to-n\n",
    "        relation between system and model summaries. The system\n",
    "        summaries are listed in the <PEERS> tag and the model summaries\n",
    "        in the <MODELS> tag. pyrouge currently only supports one system\n",
    "        summary per text, i.e. it assumes a 1-to-n relation between\n",
    "        system and model summaries.\n",
    "        \"\"\"\n",
    "        peer_elems = \"<P ID=\\\"{id}\\\">{name}</P>\".format(\n",
    "            id=system_id, name=system_filename)\n",
    "\n",
    "        model_elems = [\"<M ID=\\\"{id}\\\">{name}</M>\".format(\n",
    "            id=chr(65 + i), name=name)\n",
    "            for i, name in enumerate(model_filenames)]\n",
    "\n",
    "        model_elems = \"\\n\\t\\t\\t\".join(model_elems)\n",
    "        eval_string = \"\"\"\n",
    "    <EVAL ID=\"{task_id}\">\n",
    "        <MODEL-ROOT>{model_root}</MODEL-ROOT>\n",
    "        <PEER-ROOT>{peer_root}</PEER-ROOT>\n",
    "        <INPUT-FORMAT TYPE=\"SEE\">\n",
    "        </INPUT-FORMAT>\n",
    "        <PEERS>\n",
    "            {peer_elems}\n",
    "        </PEERS>\n",
    "        <MODELS>\n",
    "            {model_elems}\n",
    "        </MODELS>\n",
    "    </EVAL>\n",
    "\"\"\".format(\n",
    "            task_id=task_id,\n",
    "            model_root=model_dir, model_elems=model_elems,\n",
    "            peer_root=system_dir, peer_elems=peer_elems)\n",
    "        return eval_string\n",
    "\n",
    "    def __process_summaries(self, process_func):\n",
    "        \"\"\"\n",
    "        Helper method that applies process_func to the files in the\n",
    "        system and model folders and saves the resulting files to new\n",
    "        system and model folders.\n",
    "        \"\"\"\n",
    "        temp_dir = mkdtemp(dir=self.temp_dir)\n",
    "        new_system_dir = os.path.join(temp_dir, \"system\")\n",
    "        os.mkdir(new_system_dir)\n",
    "        new_model_dir = os.path.join(temp_dir, \"model\")\n",
    "        os.mkdir(new_model_dir)\n",
    "        self.log.info(\n",
    "            \"Processing summaries. Saving system files to {} and \"\n",
    "            \"model files to {}.\".format(new_system_dir, new_model_dir))\n",
    "        process_func(self._system_dir, new_system_dir)\n",
    "        process_func(self._model_dir, new_model_dir)\n",
    "        self._system_dir = new_system_dir\n",
    "        self._model_dir = new_model_dir\n",
    "\n",
    "    def __write_summaries(self):\n",
    "        self.log.info(\"Writing summaries.\")\n",
    "        self.__process_summaries(self.convert_summaries_to_rouge_format)\n",
    "\n",
    "    @staticmethod\n",
    "    def __get_model_filenames_for_id(id, model_dir, model_filenames_pattern):\n",
    "        pattern = re.compile(model_filenames_pattern.replace('#ID#', id))\n",
    "        model_filenames = [\n",
    "            f for f in os.listdir(model_dir) if pattern.match(f)]\n",
    "        if not model_filenames:\n",
    "            raise Exception(\n",
    "                \"Could not find any model summaries for the system\"\n",
    "                \" summary with ID {}. Specified model filename pattern was: \"\n",
    "                \"{}\".format(id, model_filenames_pattern))\n",
    "        return model_filenames\n",
    "\n",
    "    def __get_options(self, rouge_args=None):\n",
    "        \"\"\"\n",
    "        Get supplied command line arguments for ROUGE or use default\n",
    "        ones.\n",
    "        \"\"\"\n",
    "        if self.args:\n",
    "            options = self.args.split()\n",
    "        elif rouge_args:\n",
    "            options = rouge_args.split()\n",
    "        else:\n",
    "            options = [\n",
    "                '-e', self._data_dir,\n",
    "                '-c', 95,\n",
    "                # '-2',\n",
    "                # '-1',\n",
    "                # '-U',\n",
    "                '-m',\n",
    "                # '-v',\n",
    "                '-r', 1000,\n",
    "                '-n', 2,\n",
    "                # '-w', 1.2,\n",
    "                '-a',\n",
    "                ]\n",
    "            options = list(map(str, options))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        options = self.__add_config_option(options)\n",
    "        return options\n",
    "\n",
    "    def __create_dir_property(self, dir_name, docstring):\n",
    "        \"\"\"\n",
    "        Generate getter and setter for a directory property.\n",
    "        \"\"\"\n",
    "        property_name = \"{}_dir\".format(dir_name)\n",
    "        private_name = \"_\" + property_name\n",
    "        setattr(self, private_name, None)\n",
    "\n",
    "        def fget(self):\n",
    "            return getattr(self, private_name)\n",
    "\n",
    "        def fset(self, path):\n",
    "            verify_dir(path, dir_name)\n",
    "            setattr(self, private_name, path)\n",
    "\n",
    "        p = property(fget=fget, fset=fset, doc=docstring)\n",
    "        setattr(self.__class__, property_name, p)\n",
    "\n",
    "    def __set_dir_properties(self):\n",
    "        \"\"\"\n",
    "        Automatically generate the properties for directories.\n",
    "        \"\"\"\n",
    "        directories = [\n",
    "            (\"home\", \"The ROUGE home directory.\"),\n",
    "            (\"data\", \"The path of the ROUGE 'data' directory.\"),\n",
    "            (\"system\", \"Path of the directory containing system summaries.\"),\n",
    "            (\"model\", \"Path of the directory containing model summaries.\"),\n",
    "            ]\n",
    "        for (dirname, docstring) in directories:\n",
    "            self.__create_dir_property(dirname, docstring)\n",
    "\n",
    "    def __clean_rouge_args(self, rouge_args):\n",
    "        \"\"\"\n",
    "        Remove enclosing quotation marks, if any.\n",
    "        \"\"\"\n",
    "        if not rouge_args:\n",
    "            return\n",
    "        quot_mark_pattern = re.compile('\"(.+)\"')\n",
    "        match = quot_mark_pattern.match(rouge_args)\n",
    "        if match:\n",
    "            cleaned_args = match.group(1)\n",
    "            return cleaned_args\n",
    "        else:\n",
    "            return rouge_args\n",
    "\n",
    "    def __add_config_option(self, options):\n",
    "        return options + [self._config_file]\n",
    "\n",
    "    def __get_config_path(self):\n",
    "        if platform.system() == \"Windows\":\n",
    "            parent_dir = os.getenv(\"APPDATA\")\n",
    "            config_dir_name = \"pyrouge\"\n",
    "        elif os.name == \"posix\":\n",
    "            parent_dir = os.path.expanduser(\"~\")\n",
    "            config_dir_name = \".pyrouge\"\n",
    "        else:\n",
    "            parent_dir = os.path.dirname(__file__)\n",
    "            config_dir_name = \"\"\n",
    "        config_dir = os.path.join(parent_dir, config_dir_name)\n",
    "        if not os.path.exists(config_dir):\n",
    "            os.makedirs(config_dir)\n",
    "        return os.path.join(config_dir, 'settings.ini')\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import argparse\n",
    "    from utils.argparsers import rouge_path_parser\n",
    "\n",
    "    parser = argparse.ArgumentParser(parents=[rouge_path_parser])\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    rouge = Rouge155(args.rouge_home)\n",
    "    rouge.save_home_dir()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     7,
     11,
     17,
     53,
     93
    ]
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import shutil\n",
    "import time\n",
    "\n",
    "#from others import pyrouge\n",
    "\n",
    "REMAP = {\"-lrb-\": \"(\", \"-rrb-\": \")\", \"-lcb-\": \"{\", \"-rcb-\": \"}\",\n",
    "         \"-lsb-\": \"[\", \"-rsb-\": \"]\", \"``\": '\"', \"''\": '\"'}\n",
    "\n",
    "\n",
    "def clean(x):\n",
    "    return re.sub(\n",
    "        r\"-lrb-|-rrb-|-lcb-|-rcb-|-lsb-|-rsb-|``|''\",\n",
    "        lambda m: REMAP.get(m.group()), x)\n",
    "\n",
    "\n",
    "def process(params):\n",
    "    temp_dir, data = params\n",
    "    candidates, references, pool_id = data\n",
    "    cnt = len(candidates)\n",
    "    current_time = time.strftime('%Y-%m-%d-%H-%M-%S', time.localtime())\n",
    "    tmp_dir = os.path.join(temp_dir, \"rouge-tmp-{}-{}\".format(current_time, pool_id))\n",
    "    if not os.path.isdir(tmp_dir):\n",
    "        os.mkdir(tmp_dir)\n",
    "        os.mkdir(tmp_dir + \"/candidate\")\n",
    "        os.mkdir(tmp_dir + \"/reference\")\n",
    "    try:\n",
    "\n",
    "        for i in range(cnt):\n",
    "            if len(references[i]) < 1:\n",
    "                continue\n",
    "            with open(tmp_dir + \"/candidate/cand.{}.txt\".format(i), \"w\",\n",
    "                      encoding=\"utf-8\") as f:\n",
    "                f.write(candidates[i])\n",
    "            with open(tmp_dir + \"/reference/ref.{}.txt\".format(i), \"w\",\n",
    "                      encoding=\"utf-8\") as f:\n",
    "                f.write(references[i])\n",
    "        r = pyrouge.Rouge155(temp_dir=temp_dir)\n",
    "        r.model_dir = tmp_dir + \"/reference/\"\n",
    "        r.system_dir = tmp_dir + \"/candidate/\"\n",
    "        r.model_filename_pattern = 'ref.#ID#.txt'\n",
    "        r.system_filename_pattern = r'cand.(\\d+).txt'\n",
    "        rouge_results = r.convert_and_evaluate()\n",
    "        print(rouge_results)\n",
    "        results_dict = r.output_to_dict(rouge_results)\n",
    "    finally:\n",
    "        pass\n",
    "        if os.path.isdir(tmp_dir):\n",
    "            shutil.rmtree(tmp_dir)\n",
    "    return results_dict\n",
    "\n",
    "\n",
    "def test_rouge(temp_dir, cand, ref):\n",
    "    candidates = [line.strip() for line in open(cand, encoding='utf-8')]\n",
    "    references = [line.strip() for line in open(ref, encoding='utf-8')]\n",
    "    print(len(candidates))\n",
    "    print(len(references))\n",
    "    assert len(candidates) == len(references)\n",
    "\n",
    "    cnt = len(candidates)\n",
    "    current_time = time.strftime('%Y-%m-%d-%H-%M-%S', time.localtime())\n",
    "    tmp_dir = os.path.join(temp_dir, \"rouge-tmp-{}\".format(current_time))\n",
    "    if not os.path.isdir(tmp_dir):\n",
    "        os.mkdir(tmp_dir)\n",
    "        os.mkdir(tmp_dir + \"/candidate\")\n",
    "        os.mkdir(tmp_dir + \"/reference\")\n",
    "    try:\n",
    "\n",
    "        for i in range(cnt):\n",
    "            if len(references[i]) < 1:\n",
    "                continue\n",
    "            with open(tmp_dir + \"/candidate/cand.{}.txt\".format(i), \"w\",\n",
    "                      encoding=\"utf-8\") as f:\n",
    "                f.write(candidates[i])\n",
    "            with open(tmp_dir + \"/reference/ref.{}.txt\".format(i), \"w\",\n",
    "                      encoding=\"utf-8\") as f:\n",
    "                f.write(references[i])\n",
    "        r = pyrouge.Rouge155(temp_dir=temp_dir)\n",
    "        r.model_dir = tmp_dir + \"/reference/\"\n",
    "        r.system_dir = tmp_dir + \"/candidate/\"\n",
    "        r.model_filename_pattern = 'ref.#ID#.txt'\n",
    "        r.system_filename_pattern = r'cand.(\\d+).txt'\n",
    "        rouge_results = r.convert_and_evaluate()\n",
    "        print(rouge_results)\n",
    "        results_dict = r.output_to_dict(rouge_results)\n",
    "    finally:\n",
    "        pass\n",
    "        if os.path.isdir(tmp_dir):\n",
    "            shutil.rmtree(tmp_dir)\n",
    "    return results_dict\n",
    "\n",
    "\n",
    "def rouge_results_to_str(results_dict):\n",
    "    return \">> ROUGE-F(1/2/3/l): {:.2f}/{:.2f}/{:.2f}\\nROUGE-R(1/2/3/l): {:.2f}/{:.2f}/{:.2f}\\n\".format(\n",
    "        results_dict[\"rouge_1_f_score\"] * 100,\n",
    "        results_dict[\"rouge_2_f_score\"] * 100,\n",
    "        results_dict[\"rouge_l_f_score\"] * 100,\n",
    "        results_dict[\"rouge_1_recall\"] * 100,\n",
    "        results_dict[\"rouge_2_recall\"] * 100,\n",
    "        results_dict[\"rouge_l_recall\"] * 100\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Builder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     20,
     42,
     63,
     97,
     136,
     143,
     199,
     218,
     247,
     277,
     321
    ]
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "import glob\n",
    "import hashlib\n",
    "import itertools\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "import subprocess\n",
    "import time\n",
    "from os.path import join as pjoin\n",
    "\n",
    "import torch\n",
    "from multiprocess import Pool\n",
    "from pytorch_pretrained_bert import BertTokenizer\n",
    "\n",
    "#from others.logging import logger\n",
    "#from others.utils import clean\n",
    "#from prepro.utils import _get_word_ngrams\n",
    "\n",
    "\n",
    "def load_json(p, lower):\n",
    "    source = []\n",
    "    tgt = []\n",
    "    flag = False\n",
    "    for sent in json.load(open(p))['sentences']:\n",
    "        tokens = [t['word'] for t in sent['tokens']]\n",
    "        if (lower):\n",
    "            tokens = [t.lower() for t in tokens]\n",
    "        if (tokens[0] == '@highlight'):\n",
    "            flag = True\n",
    "            continue\n",
    "        if (flag):\n",
    "            tgt.append(tokens)\n",
    "            flag = False\n",
    "        else:\n",
    "            source.append(tokens)\n",
    "\n",
    "    source = [clean(' '.join(sent)).split() for sent in source]\n",
    "    tgt = [clean(' '.join(sent)).split() for sent in tgt]\n",
    "    return source, tgt\n",
    "\n",
    "\n",
    "def cal_rouge(evaluated_ngrams, reference_ngrams):\n",
    "    reference_count = len(reference_ngrams)\n",
    "    evaluated_count = len(evaluated_ngrams)\n",
    "\n",
    "    overlapping_ngrams = evaluated_ngrams.intersection(reference_ngrams)\n",
    "    overlapping_count = len(overlapping_ngrams)\n",
    "\n",
    "    if evaluated_count == 0:\n",
    "        precision = 0.0\n",
    "    else:\n",
    "        precision = overlapping_count / evaluated_count\n",
    "\n",
    "    if reference_count == 0:\n",
    "        recall = 0.0\n",
    "    else:\n",
    "        recall = overlapping_count / reference_count\n",
    "\n",
    "    f1_score = 2.0 * ((precision * recall) / (precision + recall + 1e-8))\n",
    "    return {\"f\": f1_score, \"p\": precision, \"r\": recall}\n",
    "\n",
    "\n",
    "def combination_selection(doc_sent_list, abstract_sent_list, summary_size):\n",
    "    def _rouge_clean(s):\n",
    "        return re.sub(r'[^a-zA-Z0-9 ]', '', s)\n",
    "\n",
    "    max_rouge = 0.0\n",
    "    max_idx = (0, 0)\n",
    "    abstract = sum(abstract_sent_list, [])\n",
    "    abstract = _rouge_clean(' '.join(abstract)).split()\n",
    "    sents = [_rouge_clean(' '.join(s)).split() for s in doc_sent_list]\n",
    "    evaluated_1grams = [_get_word_ngrams(1, [sent]) for sent in sents]\n",
    "    reference_1grams = _get_word_ngrams(1, [abstract])\n",
    "    evaluated_2grams = [_get_word_ngrams(2, [sent]) for sent in sents]\n",
    "    reference_2grams = _get_word_ngrams(2, [abstract])\n",
    "\n",
    "    impossible_sents = []\n",
    "    for s in range(summary_size + 1):\n",
    "        combinations = itertools.combinations([i for i in range(len(sents)) if i not in impossible_sents], s + 1)\n",
    "        for c in combinations:\n",
    "            candidates_1 = [evaluated_1grams[idx] for idx in c]\n",
    "            candidates_1 = set.union(*map(set, candidates_1))\n",
    "            candidates_2 = [evaluated_2grams[idx] for idx in c]\n",
    "            candidates_2 = set.union(*map(set, candidates_2))\n",
    "            rouge_1 = cal_rouge(candidates_1, reference_1grams)['f']\n",
    "            rouge_2 = cal_rouge(candidates_2, reference_2grams)['f']\n",
    "\n",
    "            rouge_score = rouge_1 + rouge_2\n",
    "            if (s == 0 and rouge_score == 0):\n",
    "                impossible_sents.append(c[0])\n",
    "            if rouge_score > max_rouge:\n",
    "                max_idx = c\n",
    "                max_rouge = rouge_score\n",
    "    return sorted(list(max_idx))\n",
    "\n",
    "\n",
    "def greedy_selection(doc_sent_list, abstract_sent_list, summary_size):\n",
    "    def _rouge_clean(s):\n",
    "        return re.sub(r'[^a-zA-Z0-9 ]', '', s)\n",
    "\n",
    "    max_rouge = 0.0\n",
    "    abstract = sum(abstract_sent_list, [])\n",
    "    abstract = _rouge_clean(' '.join(abstract)).split()\n",
    "    sents = [_rouge_clean(' '.join(s)).split() for s in doc_sent_list]\n",
    "    evaluated_1grams = [_get_word_ngrams(1, [sent]) for sent in sents]\n",
    "    reference_1grams = _get_word_ngrams(1, [abstract])\n",
    "    evaluated_2grams = [_get_word_ngrams(2, [sent]) for sent in sents]\n",
    "    reference_2grams = _get_word_ngrams(2, [abstract])\n",
    "\n",
    "    selected = []\n",
    "    for s in range(summary_size):\n",
    "        cur_max_rouge = max_rouge\n",
    "        cur_id = -1\n",
    "        for i in range(len(sents)):\n",
    "            if (i in selected):\n",
    "                continue\n",
    "            c = selected + [i]\n",
    "            candidates_1 = [evaluated_1grams[idx] for idx in c]\n",
    "            candidates_1 = set.union(*map(set, candidates_1))\n",
    "            candidates_2 = [evaluated_2grams[idx] for idx in c]\n",
    "            candidates_2 = set.union(*map(set, candidates_2))\n",
    "            rouge_1 = cal_rouge(candidates_1, reference_1grams)['f']\n",
    "            rouge_2 = cal_rouge(candidates_2, reference_2grams)['f']\n",
    "            rouge_score = rouge_1 + rouge_2\n",
    "            if rouge_score > cur_max_rouge:\n",
    "                cur_max_rouge = rouge_score\n",
    "                cur_id = i\n",
    "        if (cur_id == -1):\n",
    "            return selected\n",
    "        selected.append(cur_id)\n",
    "        max_rouge = cur_max_rouge\n",
    "\n",
    "    return sorted(selected)\n",
    "\n",
    "\n",
    "def hashhex(s):\n",
    "    \"\"\"Returns a heximal formated SHA1 hash of the input string.\"\"\"\n",
    "    h = hashlib.sha1()\n",
    "    h.update(s.encode('utf-8'))\n",
    "    return h.hexdigest()\n",
    "\n",
    "\n",
    "class BertData():\n",
    "    def __init__(self, args):\n",
    "        self.args = args\n",
    "        self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
    "        self.sep_vid = self.tokenizer.vocab['[SEP]']\n",
    "        self.cls_vid = self.tokenizer.vocab['[CLS]']\n",
    "        self.pad_vid = self.tokenizer.vocab['[PAD]']\n",
    "\n",
    "    def preprocess(self, src, tgt, oracle_ids):\n",
    "\n",
    "        if (len(src) == 0):\n",
    "            return None\n",
    "\n",
    "        original_src_txt = [' '.join(s) for s in src]\n",
    "\n",
    "        labels = [0] * len(src)\n",
    "        for l in oracle_ids:\n",
    "            labels[l] = 1\n",
    "\n",
    "        idxs = [i for i, s in enumerate(src) if (len(s) > self.args.min_src_ntokens)]\n",
    "\n",
    "        src = [src[i][:self.args.max_src_ntokens] for i in idxs]\n",
    "        labels = [labels[i] for i in idxs]\n",
    "        src = src[:self.args.max_nsents]\n",
    "        labels = labels[:self.args.max_nsents]\n",
    "\n",
    "        if (len(src) < self.args.min_nsents):\n",
    "            return None\n",
    "        if (len(labels) == 0):\n",
    "            return None\n",
    "\n",
    "        src_txt = [' '.join(sent) for sent in src]\n",
    "        # text = [' '.join(ex['src_txt'][i].split()[:self.args.max_src_ntokens]) for i in idxs]\n",
    "        # text = [_clean(t) for t in text]\n",
    "        text = ' [SEP] [CLS] '.join(src_txt)\n",
    "        src_subtokens = self.tokenizer.tokenize(text)\n",
    "        src_subtokens = src_subtokens[:510]\n",
    "        src_subtokens = ['[CLS]'] + src_subtokens + ['[SEP]']\n",
    "\n",
    "        src_subtoken_idxs = self.tokenizer.convert_tokens_to_ids(src_subtokens)\n",
    "        _segs = [-1] + [i for i, t in enumerate(src_subtoken_idxs) if t == self.sep_vid]\n",
    "        segs = [_segs[i] - _segs[i - 1] for i in range(1, len(_segs))]\n",
    "        segments_ids = []\n",
    "        for i, s in enumerate(segs):\n",
    "            if (i % 2 == 0):\n",
    "                segments_ids += s * [0]\n",
    "            else:\n",
    "                segments_ids += s * [1]\n",
    "        cls_ids = [i for i, t in enumerate(src_subtoken_idxs) if t == self.cls_vid]\n",
    "        labels = labels[:len(cls_ids)]\n",
    "\n",
    "        tgt_txt = '<q>'.join([' '.join(tt) for tt in tgt])\n",
    "        src_txt = [original_src_txt[i] for i in idxs]\n",
    "        return src_subtoken_idxs, labels, segments_ids, cls_ids, src_txt, tgt_txt\n",
    "\n",
    "\n",
    "def format_to_bert(args):\n",
    "    if (args.dataset != ''):\n",
    "        datasets = [args.dataset]\n",
    "    else:\n",
    "        datasets = ['train', 'valid', 'test']\n",
    "    for corpus_type in datasets:\n",
    "        a_lst = []\n",
    "        for json_f in glob.glob(pjoin(args.raw_path, '*' + corpus_type + '.*.json')):\n",
    "            real_name = json_f.split('/')[-1]\n",
    "            a_lst.append((json_f, args, pjoin(args.save_path, real_name.replace('json', 'bert.pt'))))\n",
    "        print(a_lst)\n",
    "        pool = Pool(args.n_cpus)\n",
    "        for d in pool.imap(_format_to_bert, a_lst):\n",
    "            pass\n",
    "\n",
    "        pool.close()\n",
    "        pool.join()\n",
    "\n",
    "\n",
    "def tokenize(args):\n",
    "    stories_dir = os.path.abspath(args.raw_path)\n",
    "    tokenized_stories_dir = os.path.abspath(args.save_path)\n",
    "\n",
    "    print(\"Preparing to tokenize %s to %s...\" % (stories_dir, tokenized_stories_dir))\n",
    "    stories = os.listdir(stories_dir)\n",
    "    # make IO list file\n",
    "    print(\"Making list of files to tokenize...\")\n",
    "    with open(\"mapping_for_corenlp.txt\", \"w\") as f:\n",
    "        for s in stories:\n",
    "            if (not s.endswith('story')):\n",
    "                continue\n",
    "            f.write(\"%s\\n\" % (os.path.join(stories_dir, s)))\n",
    "    command = ['java', 'edu.stanford.nlp.pipeline.StanfordCoreNLP' ,'-annotators', 'tokenize,ssplit', '-ssplit.newlineIsSentenceBreak', 'always', '-filelist', 'mapping_for_corenlp.txt', '-outputFormat', 'json', '-outputDirectory', tokenized_stories_dir]\n",
    "    print(\"Tokenizing %i files in %s and saving in %s...\" % (len(stories), stories_dir, tokenized_stories_dir))\n",
    "    subprocess.call(command)\n",
    "    print(\"Stanford CoreNLP Tokenizer has finished.\")\n",
    "    os.remove(\"mapping_for_corenlp.txt\")\n",
    "\n",
    "    # Check that the tokenized stories directory contains the same number of files as the original directory\n",
    "    num_orig = len(os.listdir(stories_dir))\n",
    "    num_tokenized = len(os.listdir(tokenized_stories_dir))\n",
    "    if num_orig != num_tokenized:\n",
    "        raise Exception(\n",
    "            \"The tokenized stories directory %s contains %i files, but it should contain the same number as %s (which has %i files). Was there an error during tokenization?\" % (\n",
    "            tokenized_stories_dir, num_tokenized, stories_dir, num_orig))\n",
    "    print(\"Successfully finished tokenizing %s to %s.\\n\" % (stories_dir, tokenized_stories_dir))\n",
    "\n",
    "\n",
    "def _format_to_bert(params):\n",
    "    json_file, args, save_file = params\n",
    "    if (os.path.exists(save_file)):\n",
    "        logger.info('Ignore %s' % save_file)\n",
    "        return\n",
    "\n",
    "    bert = BertData(args)\n",
    "\n",
    "    logger.info('Processing %s' % json_file)\n",
    "    jobs = json.load(open(json_file))\n",
    "    datasets = []\n",
    "    for d in jobs:\n",
    "        source, tgt = d['src'], d['tgt']\n",
    "        if (args.oracle_mode == 'greedy'):\n",
    "            oracle_ids = greedy_selection(source, tgt, 3)\n",
    "        elif (args.oracle_mode == 'combination'):\n",
    "            oracle_ids = combination_selection(source, tgt, 3)\n",
    "        b_data = bert.preprocess(source, tgt, oracle_ids)\n",
    "        if (b_data is None):\n",
    "            continue\n",
    "        indexed_tokens, labels, segments_ids, cls_ids, src_txt, tgt_txt = b_data\n",
    "        b_data_dict = {\"src\": indexed_tokens, \"labels\": labels, \"segs\": segments_ids, 'clss': cls_ids,\n",
    "                       'src_txt': src_txt, \"tgt_txt\": tgt_txt}\n",
    "        datasets.append(b_data_dict)\n",
    "    logger.info('Saving to %s' % save_file)\n",
    "    torch.save(datasets, save_file)\n",
    "    datasets = []\n",
    "    gc.collect()\n",
    "\n",
    "\n",
    "def format_to_lines(args):\n",
    "    corpus_mapping = {}\n",
    "    for corpus_type in ['valid', 'test', 'train']:\n",
    "        temp = []\n",
    "        for line in open(pjoin(args.map_path, 'mapping_' + corpus_type + '.txt')):\n",
    "            temp.append(hashhex(line.strip()))\n",
    "        corpus_mapping[corpus_type] = {key.strip(): 1 for key in temp}\n",
    "    train_files, valid_files, test_files = [], [], []\n",
    "    for f in glob.glob(pjoin(args.raw_path, '*.json')):\n",
    "        real_name = f.split('/')[-1].split('.')[0]\n",
    "        if (real_name in corpus_mapping['valid']):\n",
    "            valid_files.append(f)\n",
    "        elif (real_name in corpus_mapping['test']):\n",
    "            test_files.append(f)\n",
    "        elif (real_name in corpus_mapping['train']):\n",
    "            train_files.append(f)\n",
    "\n",
    "    corpora = {'train': train_files, 'valid': valid_files, 'test': test_files}\n",
    "    for corpus_type in ['train', 'valid', 'test']:\n",
    "        a_lst = [(f, args) for f in corpora[corpus_type]]\n",
    "        pool = Pool(args.n_cpus)\n",
    "        dataset = []\n",
    "        p_ct = 0\n",
    "        for d in pool.imap_unordered(_format_to_lines, a_lst):\n",
    "            dataset.append(d)\n",
    "            if (len(dataset) > args.shard_size):\n",
    "                pt_file = \"{:s}.{:s}.{:d}.json\".format(args.save_path, corpus_type, p_ct)\n",
    "                with open(pt_file, 'w') as save:\n",
    "                    # save.write('\\n'.join(dataset))\n",
    "                    save.write(json.dumps(dataset))\n",
    "                    p_ct += 1\n",
    "                    dataset = []\n",
    "\n",
    "        pool.close()\n",
    "        pool.join()\n",
    "        if (len(dataset) > 0):\n",
    "            pt_file = \"{:s}.{:s}.{:d}.json\".format(args.save_path, corpus_type, p_ct)\n",
    "            with open(pt_file, 'w') as save:\n",
    "                # save.write('\\n'.join(dataset))\n",
    "                save.write(json.dumps(dataset))\n",
    "                p_ct += 1\n",
    "                dataset = []\n",
    "\n",
    "\n",
    "def _format_to_lines(params):\n",
    "    f, args = params\n",
    "    print(f)\n",
    "    source, tgt = load_json(f, args.lower)\n",
    "    return {'src': source, 'tgt': tgt}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0,
     16
    ]
   },
   "outputs": [],
   "source": [
    "def _get_ngrams(n, text):\n",
    "    \"\"\"Calcualtes n-grams.\n",
    "    Args:\n",
    "      n: which n-grams to calculate\n",
    "      text: An array of tokens\n",
    "    Returns:\n",
    "      A set of n-grams\n",
    "    \"\"\"\n",
    "    ngram_set = set()\n",
    "    text_length = len(text)\n",
    "    max_index_ngram_start = text_length - n\n",
    "    for i in range(max_index_ngram_start + 1):\n",
    "        ngram_set.add(tuple(text[i:i + n]))\n",
    "    return ngram_set\n",
    "\n",
    "\n",
    "def _get_word_ngrams(n, sentences):\n",
    "    \"\"\"Calculates word n-grams for multiple sentences.\n",
    "    \"\"\"\n",
    "    assert len(sentences) > 0\n",
    "    assert n > 0\n",
    "\n",
    "    # words = _split_into_words(sentences)\n",
    "\n",
    "    words = sum(sentences, [])\n",
    "    # words = [w for w in words if w not in stopwords]\n",
    "    return _get_ngrams(n, words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import glob\n",
    "import random\n",
    "\n",
    "import torch\n",
    "\n",
    "#from others.logging import logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0,
     43,
     59,
     90
    ]
   },
   "outputs": [],
   "source": [
    "class Batch(object):\n",
    "    def _pad(self, data, pad_id, width=-1):\n",
    "        if (width == -1):\n",
    "            width = max(len(d) for d in data)\n",
    "        rtn_data = [d + [pad_id] * (width - len(d)) for d in data]\n",
    "        return rtn_data\n",
    "\n",
    "    def __init__(self, data=None, device=None,  is_test=False):\n",
    "        \"\"\"Create a Batch from a list of examples.\"\"\"\n",
    "        if data is not None:\n",
    "            self.batch_size = len(data)\n",
    "            pre_src = [x[0] for x in data]\n",
    "            pre_labels = [x[1] for x in data]\n",
    "            pre_segs = [x[2] for x in data]\n",
    "            pre_clss = [x[3] for x in data]\n",
    "\n",
    "            src = torch.tensor(self._pad(pre_src, 0))\n",
    "\n",
    "            labels = torch.tensor(self._pad(pre_labels, 0))\n",
    "            segs = torch.tensor(self._pad(pre_segs, 0))\n",
    "            mask = 1 - (src == 0)\n",
    "\n",
    "            clss = torch.tensor(self._pad(pre_clss, -1))\n",
    "            mask_cls = 1 - (clss == -1)\n",
    "            clss[clss == -1] = 0\n",
    "\n",
    "            setattr(self, 'clss', clss.to(device))\n",
    "            setattr(self, 'mask_cls', mask_cls.to(device))\n",
    "            setattr(self, 'src', src.to(device))\n",
    "            setattr(self, 'labels', labels.to(device))\n",
    "            setattr(self, 'segs', segs.to(device))\n",
    "            setattr(self, 'mask', mask.to(device))\n",
    "\n",
    "            if (is_test):\n",
    "                src_str = [x[-2] for x in data]\n",
    "                setattr(self, 'src_str', src_str)\n",
    "                tgt_str = [x[-1] for x in data]\n",
    "                setattr(self, 'tgt_str', tgt_str)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.batch_size\n",
    "\n",
    "\n",
    "def batch(data, batch_size):\n",
    "    \"\"\"Yield elements from data in chunks of batch_size.\"\"\"\n",
    "    minibatch, size_so_far = [], 0\n",
    "    for ex in data:\n",
    "        minibatch.append(ex)\n",
    "        size_so_far = simple_batch_size_fn(ex, len(minibatch))\n",
    "        if size_so_far == batch_size:\n",
    "            yield minibatch\n",
    "            minibatch, size_so_far = [], 0\n",
    "        elif size_so_far > batch_size:\n",
    "            yield minibatch[:-1]\n",
    "            minibatch, size_so_far = minibatch[-1:], simple_batch_size_fn(ex, 1)\n",
    "    if minibatch:\n",
    "        yield minibatch\n",
    "\n",
    "\n",
    "def load_dataset(args, corpus_type, shuffle):\n",
    "    \"\"\"\n",
    "    Dataset generator. Don't do extra stuff here, like printing,\n",
    "    because they will be postponed to the first loading time.\n",
    "    Args:\n",
    "        corpus_type: 'train' or 'valid'\n",
    "    Returns:\n",
    "        A list of dataset, the dataset(s) are lazily loaded.\n",
    "    \"\"\"\n",
    "    assert corpus_type in [\"train\", \"valid\", \"test\"]\n",
    "\n",
    "    def _lazy_dataset_loader(pt_file, corpus_type):\n",
    "        dataset = torch.load(pt_file)\n",
    "        logger.info('Loading %s dataset from %s, number of examples: %d' %\n",
    "                    (corpus_type, pt_file, len(dataset)))\n",
    "        return dataset\n",
    "\n",
    "    # Sort the glob output by file name (by increasing indexes).\n",
    "    pts = sorted(glob.glob(args.bert_data_path + '.' + corpus_type + '.[0-9]*.pt'))\n",
    "    if pts:\n",
    "        if (shuffle):\n",
    "            random.shuffle(pts)\n",
    "\n",
    "        for pt in pts:\n",
    "            yield _lazy_dataset_loader(pt, corpus_type)\n",
    "    else:\n",
    "        # Only one inputters.*Dataset, simple!\n",
    "        pt = args.bert_data_path + '.' + corpus_type + '.pt'\n",
    "        yield _lazy_dataset_loader(pt, corpus_type)\n",
    "\n",
    "\n",
    "def simple_batch_size_fn(new, count):\n",
    "    src, labels = new[0], new[1]\n",
    "    global max_n_sents, max_n_tokens, max_size\n",
    "    if count == 1:\n",
    "        max_size = 0\n",
    "        max_n_sents=0\n",
    "        max_n_tokens=0\n",
    "    max_n_sents = max(max_n_sents, len(src))\n",
    "    max_size = max(max_size, max_n_sents)\n",
    "    src_elements = count * max_size\n",
    "    return src_elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0,
     39
    ]
   },
   "outputs": [],
   "source": [
    "class Dataloader(object):\n",
    "    def __init__(self, args, datasets,  batch_size,\n",
    "                 device, shuffle, is_test):\n",
    "        self.args = args\n",
    "        self.datasets = datasets\n",
    "        self.batch_size = batch_size\n",
    "        self.device = device\n",
    "        self.shuffle = shuffle\n",
    "        self.is_test = is_test\n",
    "        self.cur_iter = self._next_dataset_iterator(datasets)\n",
    "\n",
    "        assert self.cur_iter is not None\n",
    "\n",
    "    def __iter__(self):\n",
    "        dataset_iter = (d for d in self.datasets)\n",
    "        while self.cur_iter is not None:\n",
    "            for batch in self.cur_iter:\n",
    "                yield batch\n",
    "            self.cur_iter = self._next_dataset_iterator(dataset_iter)\n",
    "\n",
    "\n",
    "    def _next_dataset_iterator(self, dataset_iter):\n",
    "        try:\n",
    "            # Drop the current dataset for decreasing memory\n",
    "            if hasattr(self, \"cur_dataset\"):\n",
    "                self.cur_dataset = None\n",
    "                gc.collect()\n",
    "                del self.cur_dataset\n",
    "                gc.collect()\n",
    "\n",
    "            self.cur_dataset = next(dataset_iter)\n",
    "        except StopIteration:\n",
    "            return None\n",
    "\n",
    "        return DataIterator(args = self.args,\n",
    "            dataset=self.cur_dataset,  batch_size=self.batch_size,\n",
    "            device=self.device, shuffle=self.shuffle, is_test=self.is_test)\n",
    "\n",
    "\n",
    "class DataIterator(object):\n",
    "    def __init__(self, args, dataset,  batch_size,  device=None, is_test=False,\n",
    "                 shuffle=True):\n",
    "        self.args = args\n",
    "        self.batch_size, self.is_test, self.dataset = batch_size, is_test, dataset\n",
    "        self.iterations = 0\n",
    "        self.device = device\n",
    "        self.shuffle = shuffle\n",
    "\n",
    "        self.sort_key = lambda x: len(x[1])\n",
    "\n",
    "        self._iterations_this_epoch = 0\n",
    "\n",
    "    def data(self):\n",
    "        if self.shuffle:\n",
    "            random.shuffle(self.dataset)\n",
    "        xs = self.dataset\n",
    "        return xs\n",
    "\n",
    "\n",
    "    def preprocess(self, ex, is_test):\n",
    "        src = ex['src']\n",
    "        if('labels' in ex):\n",
    "            labels = ex['labels']\n",
    "        else:\n",
    "            labels = ex['src_sent_labels']\n",
    "\n",
    "        segs = ex['segs']\n",
    "        if(not self.args.use_interval):\n",
    "            segs=[0]*len(segs)\n",
    "        clss = ex['clss']\n",
    "        src_txt = ex['src_txt']\n",
    "        tgt_txt = ex['tgt_txt']\n",
    "\n",
    "        if(is_test):\n",
    "            return src,labels,segs, clss, src_txt, tgt_txt\n",
    "        else:\n",
    "            return src,labels,segs, clss\n",
    "\n",
    "    def batch_buffer(self, data, batch_size):\n",
    "        minibatch, size_so_far = [], 0\n",
    "        for ex in data:\n",
    "            if(len(ex['src'])==0):\n",
    "                continue\n",
    "            ex = self.preprocess(ex, self.is_test)\n",
    "            if(ex is None):\n",
    "                continue\n",
    "            minibatch.append(ex)\n",
    "            size_so_far = simple_batch_size_fn(ex, len(minibatch))\n",
    "            if size_so_far == batch_size:\n",
    "                yield minibatch\n",
    "                minibatch, size_so_far = [], 0\n",
    "            elif size_so_far > batch_size:\n",
    "                yield minibatch[:-1]\n",
    "                minibatch, size_so_far = minibatch[-1:], simple_batch_size_fn(ex, 1)\n",
    "        if minibatch:\n",
    "            yield minibatch\n",
    "\n",
    "    def create_batches(self):\n",
    "        \"\"\" Create batches \"\"\"\n",
    "        data = self.data()\n",
    "        for buffer in self.batch_buffer(data, self.batch_size * 50):\n",
    "\n",
    "            p_batch = sorted(buffer, key=lambda x: len(x[3]))\n",
    "            p_batch = batch(p_batch, self.batch_size)\n",
    "\n",
    "            p_batch = list(p_batch)\n",
    "            if (self.shuffle):\n",
    "                random.shuffle(p_batch)\n",
    "            for b in p_batch:\n",
    "                yield b\n",
    "\n",
    "    def __iter__(self):\n",
    "        while True:\n",
    "            self.batches = self.create_batches()\n",
    "            for idx, minibatch in enumerate(self.batches):\n",
    "                # fast-forward if loaded from state\n",
    "                if self._iterations_this_epoch > idx:\n",
    "                    continue\n",
    "                self.iterations += 1\n",
    "                self._iterations_this_epoch += 1\n",
    "                batch = Batch(minibatch, self.device, self.is_test)\n",
    "\n",
    "                yield batch\n",
    "            return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "#from models.neural import MultiHeadedAttention, PositionwiseFeedForward\n",
    "#from models.rnn import LayerNormLSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     1,
     14,
     43,
     44,
     66,
     98
    ]
   },
   "outputs": [],
   "source": [
    "# linear layer + sigmoid with mask \n",
    "class Classifier(nn.Module):\n",
    "    \n",
    "    def __init__(self, hidden_size):\n",
    "        super(Classifier, self).__init__()\n",
    "        self.linear1 = nn.Linear(hidden_size, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x, mask_cls):\n",
    "        h = self.linear1(x).squeeze(-1)\n",
    "        sent_scores = self.sigmoid(h) * mask_cls.float()\n",
    "        return sent_scores\n",
    "\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, dropout, dim, max_len=5000):\n",
    "        pe = torch.zeros(max_len, dim)\n",
    "        position = torch.arange(0, max_len).unsqueeze(1)\n",
    "        div_term = torch.exp((torch.arange(0, dim, 2, dtype=torch.float) *\n",
    "                              -(math.log(10000.0) / dim)))\n",
    "        pe[:, 0::2] = torch.sin(position.float() * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position.float() * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.register_buffer('pe', pe)\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        self.dim = dim\n",
    "\n",
    "    def forward(self, emb, step=None):\n",
    "        emb = emb * math.sqrt(self.dim)\n",
    "        if (step):\n",
    "            emb = emb + self.pe[:, step][:, None, :]\n",
    "\n",
    "        else:\n",
    "            emb = emb + self.pe[:, :emb.size(1)]\n",
    "        emb = self.dropout(emb)\n",
    "        return emb\n",
    "\n",
    "    def get_emb(self, emb):\n",
    "        return self.pe[:, :emb.size(1)]\n",
    "\n",
    "\n",
    "class TransformerEncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, heads, d_ff, dropout):\n",
    "        super(TransformerEncoderLayer, self).__init__()\n",
    "\n",
    "        self.self_attn = MultiHeadedAttention(\n",
    "            heads, d_model, dropout=dropout)\n",
    "        self.feed_forward = PositionwiseFeedForward(d_model, d_ff, dropout)\n",
    "        self.layer_norm = nn.LayerNorm(d_model, eps=1e-6)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, iter, query, inputs, mask):\n",
    "        if (iter != 0):\n",
    "            input_norm = self.layer_norm(inputs)\n",
    "        else:\n",
    "            input_norm = inputs\n",
    "\n",
    "        mask = mask.unsqueeze(1)\n",
    "        context = self.self_attn(input_norm, input_norm, input_norm,\n",
    "                                 mask=mask)\n",
    "        out = self.dropout(context) + inputs\n",
    "        return self.feed_forward(out)\n",
    "\n",
    "\n",
    "class TransformerInterEncoder(nn.Module):\n",
    "    def __init__(self, d_model, d_ff, heads, dropout, num_inter_layers=0):\n",
    "        super(TransformerInterEncoder, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.num_inter_layers = num_inter_layers\n",
    "        self.pos_emb = PositionalEncoding(dropout, d_model)\n",
    "        self.transformer_inter = nn.ModuleList(\n",
    "            [TransformerEncoderLayer(d_model, heads, d_ff, dropout)\n",
    "             for _ in range(num_inter_layers)])\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.layer_norm = nn.LayerNorm(d_model, eps=1e-6)\n",
    "        self.wo = nn.Linear(d_model, 1, bias=True)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, top_vecs, mask):\n",
    "        \"\"\" See :obj:`EncoderBase.forward()`\"\"\"\n",
    "\n",
    "        batch_size, n_sents = top_vecs.size(0), top_vecs.size(1)\n",
    "        pos_emb = self.pos_emb.pe[:, :n_sents]\n",
    "        x = top_vecs * mask[:, :, None].float()\n",
    "        x = x + pos_emb\n",
    "\n",
    "        for i in range(self.num_inter_layers):\n",
    "            x = self.transformer_inter[i](i, x, x, 1 - mask)  # all_sents * max_tokens * dim\n",
    "\n",
    "        x = self.layer_norm(x)\n",
    "        sent_scores = self.sigmoid(self.wo(x))\n",
    "        sent_scores = sent_scores.squeeze(-1) * mask.float()\n",
    "\n",
    "        return sent_scores\n",
    "\n",
    "#bidirectional LayerNormLSTM with drop out, fc, linear layer\n",
    "class RNNEncoder(nn.Module):\n",
    "\n",
    "    def __init__(self, bidirectional, num_layers, input_size,\n",
    "                 hidden_size, dropout=0.0):\n",
    "        super(RNNEncoder, self).__init__()\n",
    "        num_directions = 2 if bidirectional else 1\n",
    "        assert hidden_size % num_directions == 0\n",
    "        hidden_size = hidden_size // num_directions\n",
    "\n",
    "        self.rnn = LayerNormLSTM(\n",
    "            input_size=input_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            bidirectional=bidirectional)\n",
    "\n",
    "        self.wo = nn.Linear(num_directions * hidden_size, 1, bias=True)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        \"\"\"See :func:`EncoderBase.forward()`\"\"\"\n",
    "        x = torch.transpose(x, 1, 0)\n",
    "        memory_bank, _ = self.rnn(x)\n",
    "        memory_bank = self.dropout(memory_bank) + x\n",
    "        memory_bank = torch.transpose(memory_bank, 1, 0)\n",
    "\n",
    "        sent_scores = self.sigmoid(self.wo(memory_bank))\n",
    "        sent_scores = sent_scores.squeeze(-1) * mask.float()\n",
    "        return sent_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GELU: $$\\frac{x}{2}(1+tanh(\\sqrt{\\frac{2}{\\pi}}\\times(x+0.044715\\times x^3))$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     6,
     10,
     34
    ]
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Gaussian Error Linear Unit - faster convergence than sigmoid\n",
    "def gelu(x):\n",
    "    return 0.5 * x * (1 + torch.tanh(math.sqrt(2 / math.pi) * (x + 0.044715 * torch.pow(x, 3))))\n",
    "\n",
    "\n",
    "class PositionwiseFeedForward(nn.Module):\n",
    "    \"\"\" A two-layer Feed-Forward-Network with residual layer norm.\n",
    "    Args:\n",
    "        d_model (int): the size of input for the first-layer of the FFN.\n",
    "        d_ff (int): the hidden layer size of the second-layer\n",
    "            of the FNN.\n",
    "        dropout (float): dropout probability in :math:`[0, 1)`.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
    "        super(PositionwiseFeedForward, self).__init__()\n",
    "        self.w_1 = nn.Linear(d_model, d_ff)\n",
    "        self.w_2 = nn.Linear(d_ff, d_model)\n",
    "        self.layer_norm = nn.LayerNorm(d_model, eps=1e-6)\n",
    "        self.actv = gelu\n",
    "        self.dropout_1 = nn.Dropout(dropout)\n",
    "        self.dropout_2 = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        inter = self.dropout_1(self.actv(self.w_1(self.layer_norm(x))))\n",
    "        output = self.dropout_2(self.w_2(inter))\n",
    "        return output + x\n",
    "\n",
    "\n",
    "class MultiHeadedAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Multi-Head Attention module from\n",
    "    \"Attention is All You Need\"\n",
    "    :cite:`DBLP:journals/corr/VaswaniSPUJGKP17`.\n",
    "    Similar to standard `dot` attention but uses\n",
    "    multiple attention distributions simulataneously\n",
    "    to select relevant items.\n",
    "    .. mermaid::\n",
    "       graph BT\n",
    "          A[key]\n",
    "          B[value]\n",
    "          C[query]\n",
    "          O[output]\n",
    "          subgraph Attn\n",
    "            D[Attn 1]\n",
    "            E[Attn 2]\n",
    "            F[Attn N]\n",
    "          end\n",
    "          A --> D\n",
    "          C --> D\n",
    "          A --> E\n",
    "          C --> E\n",
    "          A --> F\n",
    "          C --> F\n",
    "          D --> O\n",
    "          E --> O\n",
    "          F --> O\n",
    "          B --> O\n",
    "    Also includes several additional tricks.\n",
    "    Args:\n",
    "       head_count (int): number of parallel heads\n",
    "       model_dim (int): the dimension of keys/values/queries,\n",
    "           must be divisible by head_count\n",
    "       dropout (float): dropout parameter\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, head_count, model_dim, dropout=0.1, use_final_linear=True):\n",
    "        assert model_dim % head_count == 0\n",
    "        self.dim_per_head = model_dim // head_count\n",
    "        self.model_dim = model_dim\n",
    "\n",
    "        super(MultiHeadedAttention, self).__init__()\n",
    "        self.head_count = head_count\n",
    "\n",
    "        self.linear_keys = nn.Linear(model_dim,\n",
    "                                     head_count * self.dim_per_head)\n",
    "        self.linear_values = nn.Linear(model_dim,\n",
    "                                       head_count * self.dim_per_head)\n",
    "        self.linear_query = nn.Linear(model_dim,\n",
    "                                      head_count * self.dim_per_head)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.use_final_linear = use_final_linear\n",
    "        if (self.use_final_linear):\n",
    "            self.final_linear = nn.Linear(model_dim, model_dim)\n",
    "\n",
    "    def forward(self, key, value, query, mask=None,\n",
    "                layer_cache=None, type=None, predefined_graph_1=None):\n",
    "        \"\"\"\n",
    "        Compute the context vector and the attention vectors.\n",
    "        Args:\n",
    "           key (`FloatTensor`): set of `key_len`\n",
    "                key vectors `[batch, key_len, dim]`\n",
    "           value (`FloatTensor`): set of `key_len`\n",
    "                value vectors `[batch, key_len, dim]`\n",
    "           query (`FloatTensor`): set of `query_len`\n",
    "                 query vectors  `[batch, query_len, dim]`\n",
    "           mask: binary mask indicating which keys have\n",
    "                 non-zero attention `[batch, query_len, key_len]`\n",
    "        Returns:\n",
    "           (`FloatTensor`, `FloatTensor`) :\n",
    "           * output context vectors `[batch, query_len, dim]`\n",
    "           * one of the attention vectors `[batch, query_len, key_len]`\n",
    "        \"\"\"\n",
    "\n",
    "        # CHECKS\n",
    "        # batch, k_len, d = key.size()\n",
    "        # batch_, k_len_, d_ = value.size()\n",
    "        # aeq(batch, batch_)\n",
    "        # aeq(k_len, k_len_)\n",
    "        # aeq(d, d_)\n",
    "        # batch_, q_len, d_ = query.size()\n",
    "        # aeq(batch, batch_)\n",
    "        # aeq(d, d_)\n",
    "        # aeq(self.model_dim % 8, 0)\n",
    "        # if mask is not None:\n",
    "        #    batch_, q_len_, k_len_ = mask.size()\n",
    "        #    aeq(batch_, batch)\n",
    "        #    aeq(k_len_, k_len)\n",
    "        #    aeq(q_len_ == q_len)\n",
    "        # END CHECKS\n",
    "\n",
    "        batch_size = key.size(0)\n",
    "        dim_per_head = self.dim_per_head\n",
    "        head_count = self.head_count\n",
    "        key_len = key.size(1)\n",
    "        query_len = query.size(1)\n",
    "\n",
    "        def shape(x):\n",
    "            \"\"\"  projection \"\"\"\n",
    "            return x.view(batch_size, -1, head_count, dim_per_head) \\\n",
    "                .transpose(1, 2)\n",
    "\n",
    "        def unshape(x):\n",
    "            \"\"\"  compute context \"\"\"\n",
    "            return x.transpose(1, 2).contiguous() \\\n",
    "                .view(batch_size, -1, head_count * dim_per_head)\n",
    "\n",
    "        # 1) Project key, value, and query.\n",
    "        if layer_cache is not None:\n",
    "            if type == \"self\":\n",
    "                query, key, value = self.linear_query(query), \\\n",
    "                                    self.linear_keys(query), \\\n",
    "                                    self.linear_values(query)\n",
    "\n",
    "                key = shape(key)\n",
    "                value = shape(value)\n",
    "\n",
    "                if layer_cache is not None:\n",
    "                    device = key.device\n",
    "                    if layer_cache[\"self_keys\"] is not None:\n",
    "                        key = torch.cat(\n",
    "                            (layer_cache[\"self_keys\"].to(device), key),\n",
    "                            dim=2)\n",
    "                    if layer_cache[\"self_values\"] is not None:\n",
    "                        value = torch.cat(\n",
    "                            (layer_cache[\"self_values\"].to(device), value),\n",
    "                            dim=2)\n",
    "                    layer_cache[\"self_keys\"] = key\n",
    "                    layer_cache[\"self_values\"] = value\n",
    "            elif type == \"context\":\n",
    "                query = self.linear_query(query)\n",
    "                if layer_cache is not None:\n",
    "                    if layer_cache[\"memory_keys\"] is None:\n",
    "                        key, value = self.linear_keys(key), \\\n",
    "                                     self.linear_values(value)\n",
    "                        key = shape(key)\n",
    "                        value = shape(value)\n",
    "                    else:\n",
    "                        key, value = layer_cache[\"memory_keys\"], \\\n",
    "                                     layer_cache[\"memory_values\"]\n",
    "                    layer_cache[\"memory_keys\"] = key\n",
    "                    layer_cache[\"memory_values\"] = value\n",
    "                else:\n",
    "                    key, value = self.linear_keys(key), \\\n",
    "                                 self.linear_values(value)\n",
    "                    key = shape(key)\n",
    "                    value = shape(value)\n",
    "        else:\n",
    "            key = self.linear_keys(key)\n",
    "            value = self.linear_values(value)\n",
    "            query = self.linear_query(query)\n",
    "            key = shape(key)\n",
    "            value = shape(value)\n",
    "\n",
    "        query = shape(query)\n",
    "\n",
    "        key_len = key.size(2)\n",
    "        query_len = query.size(2)\n",
    "\n",
    "        # 2) Calculate and scale scores.\n",
    "        query = query / math.sqrt(dim_per_head)\n",
    "        scores = torch.matmul(query, key.transpose(2, 3))\n",
    "\n",
    "        if mask is not None:\n",
    "            mask = mask.unsqueeze(1).expand_as(scores)\n",
    "            scores = scores.masked_fill(mask, -1e18)\n",
    "\n",
    "        # 3) Apply attention dropout and compute context vectors.\n",
    "\n",
    "        attn = self.softmax(scores)\n",
    "\n",
    "        if (not predefined_graph_1 is None):\n",
    "            attn_masked = attn[:, -1] * predefined_graph_1\n",
    "            attn_masked = attn_masked / (torch.sum(attn_masked, 2).unsqueeze(2) + 1e-9)\n",
    "\n",
    "            attn = torch.cat([attn[:, :-1], attn_masked.unsqueeze(1)], 1)\n",
    "\n",
    "        drop_attn = self.dropout(attn)\n",
    "        if (self.use_final_linear):\n",
    "            context = unshape(torch.matmul(drop_attn, value))\n",
    "            output = self.final_linear(context)\n",
    "            return output\n",
    "        else:\n",
    "            context = torch.matmul(drop_attn, value)\n",
    "            return context\n",
    "\n",
    "        # CHECK\n",
    "        # batch_, q_len_, d_ = output.size()\n",
    "        # aeq(q_len, q_len_)\n",
    "        # aeq(batch, batch_)\n",
    "        # aeq(d, d_)\n",
    "\n",
    "        # Return one attn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0,
     29
    ]
   },
   "outputs": [],
   "source": [
    "class LayerNormLSTMCell(nn.LSTMCell):\n",
    "\n",
    "    def __init__(self, input_size, hidden_size, bias=True):\n",
    "        super().__init__(input_size, hidden_size, bias)\n",
    "\n",
    "        self.ln_ih = nn.LayerNorm(4 * hidden_size)\n",
    "        self.ln_hh = nn.LayerNorm(4 * hidden_size)\n",
    "        self.ln_ho = nn.LayerNorm(hidden_size)\n",
    "\n",
    "    def forward(self, input, hidden=None):\n",
    "        self.check_forward_input(input)\n",
    "        if hidden is None:\n",
    "            hx = input.new_zeros(input.size(0), self.hidden_size, requires_grad=False)\n",
    "            cx = input.new_zeros(input.size(0), self.hidden_size, requires_grad=False)\n",
    "        else:\n",
    "            hx, cx = hidden\n",
    "        self.check_forward_hidden(input, hx, '[0]')\n",
    "        self.check_forward_hidden(input, cx, '[1]')\n",
    "\n",
    "        gates = self.ln_ih(F.linear(input, self.weight_ih, self.bias_ih)) \\\n",
    "                + self.ln_hh(F.linear(hx, self.weight_hh, self.bias_hh))\n",
    "        i, f, o = gates[:, :(3 * self.hidden_size)].sigmoid().chunk(3, 1)\n",
    "        g = gates[:, (3 * self.hidden_size):].tanh()\n",
    "\n",
    "        cy = (f * cx) + (i * g)\n",
    "        hy = o * self.ln_ho(cy).tanh()\n",
    "        return hy, cy\n",
    "\n",
    "\n",
    "class LayerNormLSTM(nn.Module):\n",
    "\n",
    "    def __init__(self, input_size, hidden_size, num_layers=1, bias=True, bidirectional=False):\n",
    "        super().__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.bidirectional = bidirectional\n",
    "\n",
    "        num_directions = 2 if bidirectional else 1\n",
    "        self.hidden0 = nn.ModuleList([\n",
    "            LayerNormLSTMCell(input_size=(input_size if layer == 0 else hidden_size * num_directions),\n",
    "                              hidden_size=hidden_size, bias=bias)\n",
    "            for layer in range(num_layers)\n",
    "        ])\n",
    "\n",
    "        if self.bidirectional:\n",
    "            self.hidden1 = nn.ModuleList([\n",
    "                LayerNormLSTMCell(input_size=(input_size if layer == 0 else hidden_size * num_directions),\n",
    "                                  hidden_size=hidden_size, bias=bias)\n",
    "                for layer in range(num_layers)\n",
    "            ])\n",
    "\n",
    "    def forward(self, input, hidden=None):\n",
    "        seq_len, batch_size, hidden_size = input.size()  # supports TxNxH only\n",
    "        num_directions = 2 if self.bidirectional else 1\n",
    "        if hidden is None:\n",
    "            hx = input.new_zeros(self.num_layers * num_directions, batch_size, self.hidden_size, requires_grad=False)\n",
    "            cx = input.new_zeros(self.num_layers * num_directions, batch_size, self.hidden_size, requires_grad=False)\n",
    "        else:\n",
    "            hx, cx = hidden\n",
    "\n",
    "        ht = [[None, ] * (self.num_layers * num_directions)] * seq_len\n",
    "        ct = [[None, ] * (self.num_layers * num_directions)] * seq_len\n",
    "\n",
    "        if self.bidirectional:\n",
    "            xs = input\n",
    "            for l, (layer0, layer1) in enumerate(zip(self.hidden0, self.hidden1)):\n",
    "                l0, l1 = 2 * l, 2 * l + 1\n",
    "                h0, c0, h1, c1 = hx[l0], cx[l0], hx[l1], cx[l1]\n",
    "                for t, (x0, x1) in enumerate(zip(xs, reversed(xs))):\n",
    "                    ht[t][l0], ct[t][l0] = layer0(x0, (h0, c0))\n",
    "                    h0, c0 = ht[t][l0], ct[t][l0]\n",
    "                    t = seq_len - 1 - t\n",
    "                    ht[t][l1], ct[t][l1] = layer1(x1, (h1, c1))\n",
    "                    h1, c1 = ht[t][l1], ct[t][l1]\n",
    "                xs = [torch.cat((h[l0], h[l1]), dim=1) for h in ht]\n",
    "            y = torch.stack(xs)\n",
    "            hy = torch.stack(ht[-1])\n",
    "            cy = torch.stack(ct[-1])\n",
    "        else:\n",
    "            h, c = hx, cx\n",
    "            for t, x in enumerate(input):\n",
    "                for l, layer in enumerate(self.hidden0):\n",
    "                    ht[t][l], ct[t][l] = layer(x, (h[l], c[l]))\n",
    "                    x = ht[t][l]\n",
    "                h, c = ht[t], ct[t]\n",
    "            y = torch.stack([h[-1] for h in ht])\n",
    "            hy = torch.stack(ht[-1])\n",
    "            cy = torch.stack(ct[-1])\n",
    "\n",
    "        return y, (hy, cy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "# from onmt.utils import use_gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     1,
     8,
     69,
     102
    ]
   },
   "outputs": [],
   "source": [
    "\"\"\" Optimizers class \"\"\"\n",
    "def use_gpu(opt):\n",
    "    \"\"\"\n",
    "    Creates a boolean if gpu used\n",
    "    \"\"\"\n",
    "    return (hasattr(opt, 'gpu_ranks') and len(opt.gpu_ranks) > 0) or \\\n",
    "           (hasattr(opt, 'gpu') and opt.gpu > -1)\n",
    "\n",
    "def build_optim(model, opt, checkpoint):\n",
    "    \"\"\" Build optimizer \"\"\"\n",
    "    saved_optimizer_state_dict = None\n",
    "\n",
    "    if opt.train_from:\n",
    "        optim = checkpoint['optim']\n",
    "        # We need to save a copy of optim.optimizer.state_dict() for setting\n",
    "        # the, optimizer state later on in Stage 2 in this method, since\n",
    "        # the method optim.set_parameters(model.parameters()) will overwrite\n",
    "        # optim.optimizer, and with ith the values stored in\n",
    "        # optim.optimizer.state_dict()\n",
    "        saved_optimizer_state_dict = optim.optimizer.state_dict()\n",
    "    else:\n",
    "        optim = Optimizer(\n",
    "            opt.optim, opt.learning_rate, opt.max_grad_norm,\n",
    "            lr_decay=opt.learning_rate_decay,\n",
    "            start_decay_steps=opt.start_decay_steps,\n",
    "            decay_steps=opt.decay_steps,\n",
    "            beta1=opt.adam_beta1,\n",
    "            beta2=opt.adam_beta2,\n",
    "            adagrad_accum=opt.adagrad_accumulator_init,\n",
    "            decay_method=opt.decay_method,\n",
    "            warmup_steps=opt.warmup_steps)\n",
    "\n",
    "    # Stage 1:\n",
    "    # Essentially optim.set_parameters (re-)creates and optimizer using\n",
    "    # model.paramters() as parameters that will be stored in the\n",
    "    # optim.optimizer.param_groups field of the torch optimizer class.\n",
    "    # Importantly, this method does not yet load the optimizer state, as\n",
    "    # essentially it builds a new optimizer with empty optimizer state and\n",
    "    # parameters from the model.\n",
    "    optim.set_parameters(model.named_parameters())\n",
    "\n",
    "    if opt.train_from:\n",
    "        # Stage 2: In this stage, which is only performed when loading an\n",
    "        # optimizer from a checkpoint, we load the saved_optimizer_state_dict\n",
    "        # into the re-created optimizer, to set the optim.optimizer.state\n",
    "        # field, which was previously empty. For this, we use the optimizer\n",
    "        # state saved in the \"saved_optimizer_state_dict\" variable for\n",
    "        # this purpose.\n",
    "        # See also: https://github.com/pytorch/pytorch/issues/2830\n",
    "        optim.optimizer.load_state_dict(saved_optimizer_state_dict)\n",
    "        # Convert back the state values to cuda type if applicable\n",
    "        if use_gpu(opt):\n",
    "            for state in optim.optimizer.state.values():\n",
    "                for k, v in state.items():\n",
    "                    if torch.is_tensor(v):\n",
    "                        state[k] = v.cuda()\n",
    "\n",
    "        # We want to make sure that indeed we have a non-empty optimizer state\n",
    "        # when we loaded an existing model. This should be at least the case\n",
    "        # for Adam, which saves \"exp_avg\" and \"exp_avg_sq\" state\n",
    "        # (Exponential moving average of gradient and squared gradient values)\n",
    "        if (optim.method == 'adam') and (len(optim.optimizer.state) < 1):\n",
    "            raise RuntimeError(\n",
    "                \"Error: loaded Adam optimizer from existing model\" +\n",
    "                \" but optimizer state is empty\")\n",
    "\n",
    "    return optim\n",
    "\n",
    "\n",
    "class MultipleOptimizer(object):\n",
    "    \"\"\" Implement multiple optimizers needed for sparse adam \"\"\"\n",
    "\n",
    "    def __init__(self, op):\n",
    "        \"\"\" ? \"\"\"\n",
    "        self.optimizers = op\n",
    "\n",
    "    def zero_grad(self):\n",
    "        \"\"\" ? \"\"\"\n",
    "        for op in self.optimizers:\n",
    "            op.zero_grad()\n",
    "\n",
    "    def step(self):\n",
    "        \"\"\" ? \"\"\"\n",
    "        for op in self.optimizers:\n",
    "            op.step()\n",
    "\n",
    "    @property\n",
    "    def state(self):\n",
    "        \"\"\" ? \"\"\"\n",
    "        return {k: v for op in self.optimizers for k, v in op.state.items()}\n",
    "\n",
    "    def state_dict(self):\n",
    "        \"\"\" ? \"\"\"\n",
    "        return [op.state_dict() for op in self.optimizers]\n",
    "\n",
    "    def load_state_dict(self, state_dicts):\n",
    "        \"\"\" ? \"\"\"\n",
    "        assert len(state_dicts) == len(self.optimizers)\n",
    "        for i in range(len(state_dicts)):\n",
    "            self.optimizers[i].load_state_dict(state_dicts[i])\n",
    "\n",
    "\n",
    "class Optimizer(object):\n",
    "    \"\"\"\n",
    "    Controller class for optimization. Mostly a thin\n",
    "    wrapper for `optim`, but also useful for implementing\n",
    "    rate scheduling beyond what is currently available.\n",
    "    Also implements necessary methods for training RNNs such\n",
    "    as grad manipulations.\n",
    "    Args:\n",
    "      method (:obj:`str`): one of [sgd, adagrad, adadelta, adam]\n",
    "      lr (float): learning rate\n",
    "      lr_decay (float, optional): learning rate decay multiplier\n",
    "      start_decay_steps (int, optional): step to start learning rate decay\n",
    "      beta1, beta2 (float, optional): parameters for adam\n",
    "      adagrad_accum (float, optional): initialization parameter for adagrad\n",
    "      decay_method (str, option): custom decay options\n",
    "      warmup_steps (int, option): parameter for `noam` decay\n",
    "    We use the default parameters for Adam that are suggested by\n",
    "    the original paper https://arxiv.org/pdf/1412.6980.pdf\n",
    "    These values are also used by other established implementations,\n",
    "    e.g. https://www.tensorflow.org/api_docs/python/tf/train/AdamOptimizer\n",
    "    https://keras.io/optimizers/\n",
    "    Recently there are slightly different values used in the paper\n",
    "    \"Attention is all you need\"\n",
    "    https://arxiv.org/pdf/1706.03762.pdf, particularly the value beta2=0.98\n",
    "    was used there however, beta2=0.999 is still arguably the more\n",
    "    established value, so we use that here as well\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, method, learning_rate, max_grad_norm,\n",
    "                 lr_decay=1, start_decay_steps=None, decay_steps=None,\n",
    "                 beta1=0.9, beta2=0.999,\n",
    "                 adagrad_accum=0.0,\n",
    "                 decay_method=None,\n",
    "                 warmup_steps=4000\n",
    "                 ):\n",
    "        self.last_ppl = None\n",
    "        self.learning_rate = learning_rate\n",
    "        self.original_lr = learning_rate\n",
    "        self.max_grad_norm = max_grad_norm\n",
    "        self.method = method\n",
    "        self.lr_decay = lr_decay\n",
    "        self.start_decay_steps = start_decay_steps\n",
    "        self.decay_steps = decay_steps\n",
    "        self.start_decay = False\n",
    "        self._step = 0\n",
    "        self.betas = [beta1, beta2]\n",
    "        self.adagrad_accum = adagrad_accum\n",
    "        self.decay_method = decay_method\n",
    "        self.warmup_steps = warmup_steps\n",
    "\n",
    "    def set_parameters(self, params):\n",
    "        \"\"\" ? \"\"\"\n",
    "        self.params = []\n",
    "        self.sparse_params = []\n",
    "        for k, p in params:\n",
    "            if p.requires_grad:\n",
    "                if self.method != 'sparseadam' or \"embed\" not in k:\n",
    "                    self.params.append(p)\n",
    "                else:\n",
    "                    self.sparse_params.append(p)\n",
    "        if self.method == 'sgd':\n",
    "            self.optimizer = optim.SGD(self.params, lr=self.learning_rate)\n",
    "        elif self.method == 'adagrad':\n",
    "            self.optimizer = optim.Adagrad(self.params, lr=self.learning_rate)\n",
    "            for group in self.optimizer.param_groups:\n",
    "                for p in group['params']:\n",
    "                    self.optimizer.state[p]['sum'] = self.optimizer\\\n",
    "                        .state[p]['sum'].fill_(self.adagrad_accum)\n",
    "        elif self.method == 'adadelta':\n",
    "            self.optimizer = optim.Adadelta(self.params, lr=self.learning_rate)\n",
    "        elif self.method == 'adam':\n",
    "            self.optimizer = optim.Adam(self.params, lr=self.learning_rate,\n",
    "                                        betas=self.betas, eps=1e-9)\n",
    "        elif self.method == 'sparseadam':\n",
    "            self.optimizer = MultipleOptimizer(\n",
    "                [optim.Adam(self.params, lr=self.learning_rate,\n",
    "                            betas=self.betas, eps=1e-8),\n",
    "                 optim.SparseAdam(self.sparse_params, lr=self.learning_rate,\n",
    "                                  betas=self.betas, eps=1e-8)])\n",
    "        else:\n",
    "            raise RuntimeError(\"Invalid optim method: \" + self.method)\n",
    "\n",
    "    def _set_rate(self, learning_rate):\n",
    "        self.learning_rate = learning_rate\n",
    "        if self.method != 'sparseadam':\n",
    "            self.optimizer.param_groups[0]['lr'] = self.learning_rate\n",
    "        else:\n",
    "            for op in self.optimizer.optimizers:\n",
    "                op.param_groups[0]['lr'] = self.learning_rate\n",
    "\n",
    "    def step(self):\n",
    "        \"\"\"Update the model parameters based on current gradients.\n",
    "        Optionally, will employ gradient modification or update learning\n",
    "        rate.\n",
    "        \"\"\"\n",
    "        self._step += 1\n",
    "\n",
    "        # Decay method used in tensor2tensor.\n",
    "        if self.decay_method == \"noam\":\n",
    "            self._set_rate(\n",
    "                self.original_lr *\n",
    "\n",
    "                 min(self._step ** (-0.5),\n",
    "                     self._step * self.warmup_steps**(-1.5)))\n",
    "\n",
    "            # self._set_rate(self.original_lr *self.model_size ** (-0.5) *min(1.0, self._step / self.warmup_steps)*max(self._step, self.warmup_steps)**(-0.5))\n",
    "        # Decay based on start_decay_steps every decay_steps\n",
    "        else:\n",
    "            if ((self.start_decay_steps is not None) and (\n",
    "                     self._step >= self.start_decay_steps)):\n",
    "                self.start_decay = True\n",
    "            if self.start_decay:\n",
    "                if ((self._step - self.start_decay_steps)\n",
    "                   % self.decay_steps == 0):\n",
    "                    self.learning_rate = self.learning_rate * self.lr_decay\n",
    "\n",
    "        if self.method != 'sparseadam':\n",
    "            self.optimizer.param_groups[0]['lr'] = self.learning_rate\n",
    "\n",
    "        if self.max_grad_norm:\n",
    "            clip_grad_norm_(self.params, self.max_grad_norm)\n",
    "        self.optimizer.step()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "import distributed\n",
    "# import onmt\n",
    "#from models.reporter import ReportMgr\n",
    "#from models.stats import Statistics\n",
    "#from others.logging import logger\n",
    "#from others.utils import test_rouge, rouge_results_to_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0,
     5,
     49
    ]
   },
   "outputs": [],
   "source": [
    "def _tally_parameters(model):\n",
    "    n_params = sum([p.nelement() for p in model.parameters()])\n",
    "    return n_params\n",
    "\n",
    "\n",
    "def build_trainer(args, device_id, model,\n",
    "                  optim):\n",
    "    \"\"\"\n",
    "    Simplify `Trainer` creation based on user `opt`s*\n",
    "    Args:\n",
    "        opt (:obj:`Namespace`): user options (usually from argument parsing)\n",
    "        model (:obj:`onmt.models.NMTModel`): the model to train\n",
    "        fields (dict): dict of fields\n",
    "        optim (:obj:`onmt.utils.Optimizer`): optimizer used during training\n",
    "        data_type (str): string describing the type of data\n",
    "            e.g. \"text\", \"img\", \"audio\"\n",
    "        model_saver(:obj:`onmt.models.ModelSaverBase`): the utility object\n",
    "            used to save the model\n",
    "    \"\"\"\n",
    "    device = \"cpu\" if args.visible_gpus == '-1' else \"cuda\"\n",
    "\n",
    "\n",
    "    grad_accum_count = args.accum_count\n",
    "    n_gpu = args.world_size\n",
    "\n",
    "    if device_id >= 0:\n",
    "        gpu_rank = int(args.gpu_ranks[device_id])\n",
    "    else:\n",
    "        gpu_rank = 0\n",
    "        n_gpu = 0\n",
    "\n",
    "    print('gpu_rank %d' % gpu_rank)\n",
    "\n",
    "    tensorboard_log_dir = args.model_path\n",
    "\n",
    "    writer = SummaryWriter(tensorboard_log_dir, comment=\"Unmt\")\n",
    "\n",
    "    report_manager = ReportMgr(args.report_every, start_time=-1, tensorboard_writer=writer)\n",
    "\n",
    "    trainer = Trainer(args, model, optim, grad_accum_count, n_gpu, gpu_rank, report_manager)\n",
    "\n",
    "    # print(tr)\n",
    "    if (model):\n",
    "        n_params = _tally_parameters(model)\n",
    "        logger.info('* number of parameters: %d' % n_params)\n",
    "\n",
    "    return trainer\n",
    "\n",
    "\n",
    "class Trainer(object):\n",
    "    \"\"\"\n",
    "    Class that controls the training process.\n",
    "    Args:\n",
    "            model(:py:class:`onmt.models.model.NMTModel`): translation model\n",
    "                to train\n",
    "            train_loss(:obj:`onmt.utils.loss.LossComputeBase`):\n",
    "               training loss computation\n",
    "            valid_loss(:obj:`onmt.utils.loss.LossComputeBase`):\n",
    "               training loss computation\n",
    "            optim(:obj:`onmt.utils.optimizers.Optimizer`):\n",
    "               the optimizer responsible for update\n",
    "            trunc_size(int): length of truncated back propagation through time\n",
    "            shard_size(int): compute loss in shards of this size for efficiency\n",
    "            data_type(string): type of the source input: [text|img|audio]\n",
    "            norm_method(string): normalization methods: [sents|tokens]\n",
    "            grad_accum_count(int): accumulate gradients this many times.\n",
    "            report_manager(:obj:`onmt.utils.ReportMgrBase`):\n",
    "                the object that creates reports, or None\n",
    "            model_saver(:obj:`onmt.models.ModelSaverBase`): the saver is\n",
    "                used to save a checkpoint.\n",
    "                Thus nothing will be saved if this parameter is None\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,  args, model,  optim,\n",
    "                  grad_accum_count=1, n_gpu=1, gpu_rank=1,\n",
    "                  report_manager=None):\n",
    "        # Basic attributes.\n",
    "        self.args = args\n",
    "        self.save_checkpoint_steps = args.save_checkpoint_steps\n",
    "        self.model = model\n",
    "        self.optim = optim\n",
    "        self.grad_accum_count = grad_accum_count\n",
    "        self.n_gpu = n_gpu\n",
    "        self.gpu_rank = gpu_rank\n",
    "        self.report_manager = report_manager\n",
    "\n",
    "        self.loss = torch.nn.BCELoss(reduction='none')\n",
    "        assert grad_accum_count > 0\n",
    "        # Set model in training mode.\n",
    "        if (model):\n",
    "            self.model.train()\n",
    "\n",
    "    def train(self, train_iter_fct, train_steps, valid_iter_fct=None, valid_steps=-1):\n",
    "        \"\"\"\n",
    "        The main training loops.\n",
    "        by iterating over training data (i.e. `train_iter_fct`)\n",
    "        and running validation (i.e. iterating over `valid_iter_fct`\n",
    "        Args:\n",
    "            train_iter_fct(function): a function that returns the train\n",
    "                iterator. e.g. something like\n",
    "                train_iter_fct = lambda: generator(*args, **kwargs)\n",
    "            valid_iter_fct(function): same as train_iter_fct, for valid data\n",
    "            train_steps(int):\n",
    "            valid_steps(int):\n",
    "            save_checkpoint_steps(int):\n",
    "        Return:\n",
    "            None\n",
    "        \"\"\"\n",
    "        logger.info('Start training...')\n",
    "\n",
    "        # step =  self.optim._step + 1\n",
    "        step =  self.optim._step + 1\n",
    "        true_batchs = []\n",
    "        accum = 0\n",
    "        normalization = 0\n",
    "        train_iter = train_iter_fct()\n",
    "\n",
    "        total_stats = Statistics()\n",
    "        report_stats = Statistics()\n",
    "        self._start_report_manager(start_time=total_stats.start_time)\n",
    "\n",
    "        while step <= train_steps:\n",
    "\n",
    "            reduce_counter = 0\n",
    "            for i, batch in enumerate(train_iter):\n",
    "                if self.n_gpu == 0 or (i % self.n_gpu == self.gpu_rank):\n",
    "\n",
    "                    true_batchs.append(batch)\n",
    "                    normalization += batch.batch_size\n",
    "                    accum += 1\n",
    "                    if accum == self.grad_accum_count:\n",
    "                        reduce_counter += 1\n",
    "                        if self.n_gpu > 1:\n",
    "                            normalization = sum(distributed\n",
    "                                                .all_gather_list\n",
    "                                                (normalization))\n",
    "\n",
    "                        self._gradient_accumulation(\n",
    "                            true_batchs, normalization, total_stats,\n",
    "                            report_stats)\n",
    "\n",
    "                        report_stats = self._maybe_report_training(\n",
    "                            step, train_steps,\n",
    "                            self.optim.learning_rate,\n",
    "                            report_stats)\n",
    "\n",
    "                        true_batchs = []\n",
    "                        accum = 0\n",
    "                        normalization = 0\n",
    "                        if (step % self.save_checkpoint_steps == 0 and self.gpu_rank == 0):\n",
    "                            self._save(step)\n",
    "\n",
    "                        step += 1\n",
    "                        if step > train_steps:\n",
    "                            break\n",
    "            train_iter = train_iter_fct()\n",
    "\n",
    "        return total_stats\n",
    "\n",
    "    def validate(self, valid_iter, step=0):\n",
    "        \"\"\" Validate model.\n",
    "            valid_iter: validate data iterator\n",
    "        Returns:\n",
    "            :obj:`nmt.Statistics`: validation loss statistics\n",
    "        \"\"\"\n",
    "        # Set model in validating mode.\n",
    "        self.model.eval()\n",
    "        stats = Statistics()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch in valid_iter:\n",
    "\n",
    "                src = batch.src\n",
    "                labels = batch.labels\n",
    "                segs = batch.segs\n",
    "                clss = batch.clss\n",
    "                mask = batch.mask\n",
    "                mask_cls = batch.mask_cls\n",
    "\n",
    "                sent_scores, mask = self.model(src, segs, clss, mask, mask_cls)\n",
    "\n",
    "\n",
    "                loss = self.loss(sent_scores, labels.float())\n",
    "                loss = (loss * mask.float()).sum()\n",
    "                batch_stats = Statistics(float(loss.cpu().data.numpy()), len(labels))\n",
    "                stats.update(batch_stats)\n",
    "            self._report_step(0, step, valid_stats=stats)\n",
    "            return stats\n",
    "\n",
    "    def test(self, test_iter, step, cal_lead=False, cal_oracle=False):\n",
    "        \"\"\" Validate model.\n",
    "            valid_iter: validate data iterator\n",
    "        Returns:\n",
    "            :obj:`nmt.Statistics`: validation loss statistics\n",
    "        \"\"\"\n",
    "        # Set model in validating mode.\n",
    "        def _get_ngrams(n, text):\n",
    "            ngram_set = set()\n",
    "            text_length = len(text)\n",
    "            max_index_ngram_start = text_length - n\n",
    "            for i in range(max_index_ngram_start + 1):\n",
    "                ngram_set.add(tuple(text[i:i + n]))\n",
    "            return ngram_set\n",
    "\n",
    "        def _block_tri(c, p):\n",
    "            tri_c = _get_ngrams(3, c.split())\n",
    "            for s in p:\n",
    "                tri_s = _get_ngrams(3, s.split())\n",
    "                if len(tri_c.intersection(tri_s))>0:\n",
    "                    return True\n",
    "            return False\n",
    "\n",
    "        if (not cal_lead and not cal_oracle):\n",
    "            self.model.eval()\n",
    "        stats = Statistics()\n",
    "\n",
    "        can_path = '%s_step%d.candidate'%(self.args.result_path,step)\n",
    "        gold_path = '%s_step%d.gold' % (self.args.result_path, step)\n",
    "        with open(can_path, 'w') as save_pred:\n",
    "            with open(gold_path, 'w') as save_gold:\n",
    "                with torch.no_grad():\n",
    "                    for batch in test_iter:\n",
    "                        src = batch.src\n",
    "                        labels = batch.labels\n",
    "                        segs = batch.segs\n",
    "                        clss = batch.clss\n",
    "                        mask = batch.mask\n",
    "                        mask_cls = batch.mask_cls\n",
    "\n",
    "\n",
    "                        gold = []\n",
    "                        pred = []\n",
    "\n",
    "                        if (cal_lead):\n",
    "                            selected_ids = [list(range(batch.clss.size(1)))] * batch.batch_size\n",
    "                        elif (cal_oracle):\n",
    "                            selected_ids = [[j for j in range(batch.clss.size(1)) if labels[i][j] == 1] for i in\n",
    "                                            range(batch.batch_size)]\n",
    "                        else:\n",
    "                            sent_scores, mask = self.model(src, segs, clss, mask, mask_cls)\n",
    "\n",
    "                            loss = self.loss(sent_scores, labels.float())\n",
    "                            loss = (loss * mask.float()).sum()\n",
    "                            batch_stats = Statistics(float(loss.cpu().data.numpy()), len(labels))\n",
    "                            stats.update(batch_stats)\n",
    "\n",
    "                            sent_scores = sent_scores + mask.float()\n",
    "                            sent_scores = sent_scores.cpu().data.numpy()\n",
    "                            selected_ids = np.argsort(-sent_scores, 1)\n",
    "                        # selected_ids = np.sort(selected_ids,1)\n",
    "                        for i, idx in enumerate(selected_ids):\n",
    "                            _pred = []\n",
    "                            if(len(batch.src_str[i])==0):\n",
    "                                continue\n",
    "                            for j in selected_ids[i][:len(batch.src_str[i])]:\n",
    "                                if(j>=len( batch.src_str[i])):\n",
    "                                    continue\n",
    "                                candidate = batch.src_str[i][j].strip()\n",
    "                                if(self.args.block_trigram):\n",
    "                                    if(not _block_tri(candidate,_pred)):\n",
    "                                        _pred.append(candidate)\n",
    "                                else:\n",
    "                                    _pred.append(candidate)\n",
    "\n",
    "                                if ((not cal_oracle) and (not self.args.recall_eval) and len(_pred) == 3):\n",
    "                                    break\n",
    "\n",
    "                            _pred = '<q>'.join(_pred)\n",
    "                            if(self.args.recall_eval):\n",
    "                                _pred = ' '.join(_pred.split()[:len(batch.tgt_str[i].split())])\n",
    "\n",
    "                            pred.append(_pred)\n",
    "                            gold.append(batch.tgt_str[i])\n",
    "\n",
    "                        for i in range(len(gold)):\n",
    "                            save_gold.write(gold[i].strip()+'\\n')\n",
    "                        for i in range(len(pred)):\n",
    "                            save_pred.write(pred[i].strip()+'\\n')\n",
    "        if(step!=-1 and self.args.report_rouge):\n",
    "            rouges = test_rouge(self.args.temp_dir, can_path, gold_path)\n",
    "            logger.info('Rouges at step %d \\n%s' % (step, rouge_results_to_str(rouges)))\n",
    "        self._report_step(0, step, valid_stats=stats)\n",
    "\n",
    "        return stats\n",
    "\n",
    "\n",
    "\n",
    "    def _gradient_accumulation(self, true_batchs, normalization, total_stats,\n",
    "                               report_stats):\n",
    "        if self.grad_accum_count > 1:\n",
    "            self.model.zero_grad()\n",
    "\n",
    "        for batch in true_batchs:\n",
    "            if self.grad_accum_count == 1:\n",
    "                self.model.zero_grad()\n",
    "\n",
    "            src = batch.src\n",
    "            labels = batch.labels\n",
    "            segs = batch.segs\n",
    "            clss = batch.clss\n",
    "            mask = batch.mask\n",
    "            mask_cls = batch.mask_cls\n",
    "\n",
    "            sent_scores, mask = self.model(src, segs, clss, mask, mask_cls)\n",
    "\n",
    "            loss = self.loss(sent_scores, labels.float())\n",
    "            loss = (loss*mask.float()).sum()\n",
    "            (loss/loss.numel()).backward()\n",
    "            # loss.div(float(normalization)).backward()\n",
    "\n",
    "            batch_stats = Statistics(float(loss.cpu().data.numpy()), normalization)\n",
    "\n",
    "\n",
    "            total_stats.update(batch_stats)\n",
    "            report_stats.update(batch_stats)\n",
    "\n",
    "            # 4. Update the parameters and statistics.\n",
    "            if self.grad_accum_count == 1:\n",
    "                # Multi GPU gradient gather\n",
    "                if self.n_gpu > 1:\n",
    "                    grads = [p.grad.data for p in self.model.parameters()\n",
    "                             if p.requires_grad\n",
    "                             and p.grad is not None]\n",
    "                    distributed.all_reduce_and_rescale_tensors(\n",
    "                        grads, float(1))\n",
    "                self.optim.step()\n",
    "\n",
    "        # in case of multi step gradient accumulation,\n",
    "        # update only after accum batches\n",
    "        if self.grad_accum_count > 1:\n",
    "            if self.n_gpu > 1:\n",
    "                grads = [p.grad.data for p in self.model.parameters()\n",
    "                         if p.requires_grad\n",
    "                         and p.grad is not None]\n",
    "                distributed.all_reduce_and_rescale_tensors(\n",
    "                    grads, float(1))\n",
    "            self.optim.step()\n",
    "\n",
    "    def _save(self, step):\n",
    "        real_model = self.model\n",
    "        # real_generator = (self.generator.module\n",
    "        #                   if isinstance(self.generator, torch.nn.DataParallel)\n",
    "        #                   else self.generator)\n",
    "\n",
    "        model_state_dict = real_model.state_dict()\n",
    "        # generator_state_dict = real_generator.state_dict()\n",
    "        checkpoint = {\n",
    "            'model': model_state_dict,\n",
    "            # 'generator': generator_state_dict,\n",
    "            'opt': self.args,\n",
    "            'optim': self.optim,\n",
    "        }\n",
    "        checkpoint_path = os.path.join(self.args.model_path, 'model_step_%d.pt' % step)\n",
    "        logger.info(\"Saving checkpoint %s\" % checkpoint_path)\n",
    "        # checkpoint_path = '%s_step_%d.pt' % (FLAGS.model_path, step)\n",
    "        if (not os.path.exists(checkpoint_path)):\n",
    "            torch.save(checkpoint, checkpoint_path)\n",
    "            return checkpoint, checkpoint_path\n",
    "\n",
    "    def _start_report_manager(self, start_time=None):\n",
    "        \"\"\"\n",
    "        Simple function to start report manager (if any)\n",
    "        \"\"\"\n",
    "        if self.report_manager is not None:\n",
    "            if start_time is None:\n",
    "                self.report_manager.start()\n",
    "            else:\n",
    "                self.report_manager.start_time = start_time\n",
    "\n",
    "    def _maybe_gather_stats(self, stat):\n",
    "        \"\"\"\n",
    "        Gather statistics in multi-processes cases\n",
    "        Args:\n",
    "            stat(:obj:onmt.utils.Statistics): a Statistics object to gather\n",
    "                or None (it returns None in this case)\n",
    "        Returns:\n",
    "            stat: the updated (or unchanged) stat object\n",
    "        \"\"\"\n",
    "        if stat is not None and self.n_gpu > 1:\n",
    "            return Statistics.all_gather_stats(stat)\n",
    "        return stat\n",
    "\n",
    "    def _maybe_report_training(self, step, num_steps, learning_rate,\n",
    "                               report_stats):\n",
    "        \"\"\"\n",
    "        Simple function to report training stats (if report_manager is set)\n",
    "        see `onmt.utils.ReportManagerBase.report_training` for doc\n",
    "        \"\"\"\n",
    "        if self.report_manager is not None:\n",
    "            return self.report_manager.report_training(\n",
    "                step, num_steps, learning_rate, report_stats,\n",
    "                multigpu=self.n_gpu > 1)\n",
    "\n",
    "    def _report_step(self, learning_rate, step, train_stats=None,\n",
    "                     valid_stats=None):\n",
    "        \"\"\"\n",
    "        Simple function to report stats (if report_manager is set)\n",
    "        see `onmt.utils.ReportManagerBase.report_step` for doc\n",
    "        \"\"\"\n",
    "        if self.report_manager is not None:\n",
    "            return self.report_manager.report_step(\n",
    "                learning_rate, step, train_stats=train_stats,\n",
    "                valid_stats=valid_stats)\n",
    "\n",
    "    def _maybe_save(self, step):\n",
    "        \"\"\"\n",
    "        Save the model if a model saver is set\n",
    "        \"\"\"\n",
    "        if self.model_saver is not None:\n",
    "            self.model_saver.maybe_save(step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reporter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "#from models.stats import Statistics\n",
    "#from others.logging import logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     10,
     28,
     100
    ]
   },
   "outputs": [],
   "source": [
    "\"\"\" Report manager utility \"\"\"\n",
    "def build_report_manager(opt):\n",
    "    if opt.tensorboard:\n",
    "        from tensorboardX import SummaryWriter\n",
    "        tensorboard_log_dir = opt.tensorboard_log_dir\n",
    "\n",
    "        if not opt.train_from:\n",
    "            tensorboard_log_dir += datetime.now().strftime(\"/%b-%d_%H-%M-%S\")\n",
    "\n",
    "        writer = SummaryWriter(tensorboard_log_dir,\n",
    "                               comment=\"Unmt\")\n",
    "    else:\n",
    "        writer = None\n",
    "\n",
    "    report_mgr = ReportMgr(opt.report_every, start_time=-1,\n",
    "                           tensorboard_writer=writer)\n",
    "    return report_mgr\n",
    "\n",
    "\n",
    "class ReportMgrBase(object):\n",
    "    \"\"\"\n",
    "    Report Manager Base class\n",
    "    Inherited classes should override:\n",
    "        * `_report_training`\n",
    "        * `_report_step`\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, report_every, start_time=-1.):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            report_every(int): Report status every this many sentences\n",
    "            start_time(float): manually set report start time. Negative values\n",
    "                means that you will need to set it later or use `start()`\n",
    "        \"\"\"\n",
    "        self.report_every = report_every\n",
    "        self.progress_step = 0\n",
    "        self.start_time = start_time\n",
    "\n",
    "    def start(self):\n",
    "        self.start_time = time.time()\n",
    "\n",
    "    def log(self, *args, **kwargs):\n",
    "        logger.info(*args, **kwargs)\n",
    "\n",
    "    def report_training(self, step, num_steps, learning_rate,\n",
    "                        report_stats, multigpu=False):\n",
    "        \"\"\"\n",
    "        This is the user-defined batch-level traing progress\n",
    "        report function.\n",
    "        Args:\n",
    "            step(int): current step count.\n",
    "            num_steps(int): total number of batches.\n",
    "            learning_rate(float): current learning rate.\n",
    "            report_stats(Statistics): old Statistics instance.\n",
    "        Returns:\n",
    "            report_stats(Statistics): updated Statistics instance.\n",
    "        \"\"\"\n",
    "        if self.start_time < 0:\n",
    "            raise ValueError(\"\"\"ReportMgr needs to be started\n",
    "                                (set 'start_time' or use 'start()'\"\"\")\n",
    "\n",
    "        if step % self.report_every == 0:\n",
    "            if multigpu:\n",
    "                report_stats = \\\n",
    "                    Statistics.all_gather_stats(report_stats)\n",
    "            self._report_training(\n",
    "                step, num_steps, learning_rate, report_stats)\n",
    "            self.progress_step += 1\n",
    "            return Statistics()\n",
    "        else:\n",
    "            return report_stats\n",
    "\n",
    "    def _report_training(self, *args, **kwargs):\n",
    "        \"\"\" To be overridden \"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def report_step(self, lr, step, train_stats=None, valid_stats=None):\n",
    "        \"\"\"\n",
    "        Report stats of a step\n",
    "        Args:\n",
    "            train_stats(Statistics): training stats\n",
    "            valid_stats(Statistics): validation stats\n",
    "            lr(float): current learning rate\n",
    "        \"\"\"\n",
    "        self._report_step(\n",
    "            lr, step, train_stats=train_stats, valid_stats=valid_stats)\n",
    "\n",
    "    def _report_step(self, *args, **kwargs):\n",
    "        raise NotImplementedError()\n",
    "\n",
    "\n",
    "class ReportMgr(ReportMgrBase):\n",
    "    def __init__(self, report_every, start_time=-1., tensorboard_writer=None):\n",
    "        \"\"\"\n",
    "        A report manager that writes statistics on standard output as well as\n",
    "        (optionally) TensorBoard\n",
    "        Args:\n",
    "            report_every(int): Report status every this many sentences\n",
    "            tensorboard_writer(:obj:`tensorboard.SummaryWriter`):\n",
    "                The TensorBoard Summary writer to use or None\n",
    "        \"\"\"\n",
    "        super(ReportMgr, self).__init__(report_every, start_time)\n",
    "        self.tensorboard_writer = tensorboard_writer\n",
    "\n",
    "    def maybe_log_tensorboard(self, stats, prefix, learning_rate, step):\n",
    "        if self.tensorboard_writer is not None:\n",
    "            stats.log_tensorboard(\n",
    "                prefix, self.tensorboard_writer, learning_rate, step)\n",
    "\n",
    "    def _report_training(self, step, num_steps, learning_rate,\n",
    "                         report_stats):\n",
    "        \"\"\"\n",
    "        See base class method `ReportMgrBase.report_training`.\n",
    "        \"\"\"\n",
    "        report_stats.output(step, num_steps,\n",
    "                            learning_rate, self.start_time)\n",
    "\n",
    "        # Log the progress using the number of batches on the x-axis.\n",
    "        self.maybe_log_tensorboard(report_stats,\n",
    "                                   \"progress\",\n",
    "                                   learning_rate,\n",
    "                                   self.progress_step)\n",
    "        report_stats = Statistics()\n",
    "\n",
    "        return report_stats\n",
    "\n",
    "    def _report_step(self, lr, step, train_stats=None, valid_stats=None):\n",
    "        \"\"\"\n",
    "        See base class method `ReportMgrBase.report_step`.\n",
    "        \"\"\"\n",
    "        if train_stats is not None:\n",
    "            self.log('Train xent: %g' % train_stats.xent())\n",
    "\n",
    "            self.maybe_log_tensorboard(train_stats,\n",
    "                                       \"train\",\n",
    "                                       lr,\n",
    "                                       step)\n",
    "\n",
    "        if valid_stats is not None:\n",
    "            self.log('Validation xent: %g at step %d' % (valid_stats.xent(), step))\n",
    "\n",
    "            self.maybe_log_tensorboard(valid_stats,\n",
    "                                       \"valid\",\n",
    "                                       lr,\n",
    "                                       step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "\n",
    "import sys\n",
    "import time\n",
    "\n",
    "#from others.logging import logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     9
    ]
   },
   "outputs": [],
   "source": [
    "\"\"\" Statistics calculation utility \"\"\"\n",
    "\n",
    "\n",
    "\n",
    "class Statistics(object):\n",
    "    \"\"\"\n",
    "    Accumulator for loss statistics.\n",
    "    Currently calculates:\n",
    "    * accuracy\n",
    "    * perplexity\n",
    "    * elapsed time\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, loss=0, n_docs=0, n_correct=0):\n",
    "        self.loss = loss\n",
    "        self.n_docs = n_docs\n",
    "        self.start_time = time.time()\n",
    "\n",
    "    @staticmethod\n",
    "    def all_gather_stats(stat, max_size=4096):\n",
    "        \"\"\"\n",
    "        Gather a `Statistics` object accross multiple process/nodes\n",
    "        Args:\n",
    "            stat(:obj:Statistics): the statistics object to gather\n",
    "                accross all processes/nodes\n",
    "            max_size(int): max buffer size to use\n",
    "        Returns:\n",
    "            `Statistics`, the update stats object\n",
    "        \"\"\"\n",
    "        stats = Statistics.all_gather_stats_list([stat], max_size=max_size)\n",
    "        return stats[0]\n",
    "\n",
    "    @staticmethod\n",
    "    def all_gather_stats_list(stat_list, max_size=4096):\n",
    "        \"\"\"\n",
    "        Gather a `Statistics` list accross all processes/nodes\n",
    "        Args:\n",
    "            stat_list(list([`Statistics`])): list of statistics objects to\n",
    "                gather accross all processes/nodes\n",
    "            max_size(int): max buffer size to use\n",
    "        Returns:\n",
    "            our_stats(list([`Statistics`])): list of updated stats\n",
    "        \"\"\"\n",
    "        from torch.distributed import get_rank\n",
    "        from distributed import all_gather_list\n",
    "\n",
    "        # Get a list of world_size lists with len(stat_list) Statistics objects\n",
    "        all_stats = all_gather_list(stat_list, max_size=max_size)\n",
    "\n",
    "        our_rank = get_rank()\n",
    "        our_stats = all_stats[our_rank]\n",
    "        for other_rank, stats in enumerate(all_stats):\n",
    "            if other_rank == our_rank:\n",
    "                continue\n",
    "            for i, stat in enumerate(stats):\n",
    "                our_stats[i].update(stat, update_n_src_words=True)\n",
    "        return our_stats\n",
    "\n",
    "    def update(self, stat, update_n_src_words=False):\n",
    "        \"\"\"\n",
    "        Update statistics by suming values with another `Statistics` object\n",
    "        Args:\n",
    "            stat: another statistic object\n",
    "            update_n_src_words(bool): whether to update (sum) `n_src_words`\n",
    "                or not\n",
    "        \"\"\"\n",
    "        self.loss += stat.loss\n",
    "\n",
    "        self.n_docs += stat.n_docs\n",
    "\n",
    "    def xent(self):\n",
    "        \"\"\" compute cross entropy \"\"\"\n",
    "        if(self.n_docs==0):\n",
    "            return 0\n",
    "        return self.loss/self.n_docs\n",
    "\n",
    "\n",
    "    def elapsed_time(self):\n",
    "        \"\"\" compute elapsed time \"\"\"\n",
    "        return time.time() - self.start_time\n",
    "\n",
    "    def output(self, step, num_steps, learning_rate, start):\n",
    "        \"\"\"Write out statistics to stdout.\n",
    "        Args:\n",
    "           step (int): current step\n",
    "           n_batch (int): total batches\n",
    "           start (int): start time of step.\n",
    "        \"\"\"\n",
    "        t = self.elapsed_time()\n",
    "        step_fmt = \"%2d\" % step\n",
    "        if num_steps > 0:\n",
    "            step_fmt = \"%s/%5d\" % (step_fmt, num_steps)\n",
    "        logger.info(\n",
    "            (\"Step %s; xent: %4.2f; \" +\n",
    "             \"lr: %7.7f; %3.0f docs/s; %6.0f sec\")\n",
    "            % (step_fmt,\n",
    "               self.xent(),\n",
    "               learning_rate,\n",
    "               self.n_docs / (t + 1e-5),\n",
    "               time.time() - start))\n",
    "        sys.stdout.flush()\n",
    "\n",
    "    def log_tensorboard(self, prefix, writer, learning_rate, step):\n",
    "        \"\"\" display statistics to tensorboard \"\"\"\n",
    "        t = self.elapsed_time()\n",
    "        writer.add_scalar(prefix + \"/xent\", self.xent(), step)\n",
    "        writer.add_scalar(prefix + \"/lr\", learning_rate, step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Builder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from pytorch_pretrained_bert import BertModel, BertConfig\n",
    "from torch.nn.init import xavier_uniform_\n",
    "\n",
    "#from models.encoder import TransformerInterEncoder, Classifier, RNNEncoder\n",
    "#from models.optimizers import Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0,
     32,
     45
    ]
   },
   "outputs": [],
   "source": [
    "def build_optim(args, model, checkpoint):\n",
    "    \"\"\" Build optimizer \"\"\"\n",
    "    saved_optimizer_state_dict = None\n",
    "\n",
    "    if args.train_from != '':\n",
    "        optim = checkpoint['optim']\n",
    "        saved_optimizer_state_dict = optim.optimizer.state_dict()\n",
    "    else:\n",
    "        optim = Optimizer(\n",
    "            args.optim, args.lr, args.max_grad_norm,\n",
    "            beta1=args.beta1, beta2=args.beta2,\n",
    "            decay_method=args.decay_method,\n",
    "            warmup_steps=args.warmup_steps)\n",
    "\n",
    "    optim.set_parameters(list(model.named_parameters()))\n",
    "\n",
    "    if args.train_from != '':\n",
    "        optim.optimizer.load_state_dict(saved_optimizer_state_dict)\n",
    "        if args.visible_gpus != '-1':\n",
    "            for state in optim.optimizer.state.values():\n",
    "                for k, v in state.items():\n",
    "                    if torch.is_tensor(v):\n",
    "                        state[k] = v.cuda()\n",
    "\n",
    "        if (optim.method == 'adam') and (len(optim.optimizer.state) < 1):\n",
    "            raise RuntimeError(\n",
    "                \"Error: loaded Adam optimizer from existing model\" +\n",
    "                \" but optimizer state is empty\")\n",
    "\n",
    "    return optim\n",
    "\n",
    "\n",
    "class Bert(nn.Module):\n",
    "    def __init__(self, temp_dir, load_pretrained_bert, bert_config):\n",
    "        super(Bert, self).__init__()\n",
    "        if(load_pretrained_bert):\n",
    "            self.model = BertModel.from_pretrained('bert-base-uncased', cache_dir=temp_dir)\n",
    "        else:\n",
    "            self.model = BertModel(bert_config)\n",
    "\n",
    "    def forward(self, x, segs, mask):\n",
    "        encoded_layers, _ = self.model(x, segs, attention_mask =mask)\n",
    "        top_vec = encoded_layers[-1]\n",
    "        return top_vec\n",
    "\n",
    "class Summarizer(nn.Module):\n",
    "    def __init__(self, args, device, load_pretrained_bert = False, bert_config = None):\n",
    "        super(Summarizer, self).__init__()\n",
    "        self.args = args\n",
    "        self.device = device\n",
    "        self.bert = Bert(args.temp_dir, load_pretrained_bert, bert_config)\n",
    "        if (args.encoder == 'classifier'):\n",
    "            self.encoder = Classifier(self.bert.model.config.hidden_size)\n",
    "        elif(args.encoder=='transformer'):\n",
    "            self.encoder = TransformerInterEncoder(self.bert.model.config.hidden_size, args.ff_size, args.heads,\n",
    "                                                   args.dropout, args.inter_layers)\n",
    "        elif(args.encoder=='rnn'):\n",
    "            self.encoder = RNNEncoder(bidirectional=True, num_layers=1,\n",
    "                                      input_size=self.bert.model.config.hidden_size, hidden_size=args.rnn_size,\n",
    "                                      dropout=args.dropout)\n",
    "        elif (args.encoder == 'baseline'):\n",
    "            bert_config = BertConfig(self.bert.model.config.vocab_size, hidden_size=args.hidden_size,\n",
    "                                     num_hidden_layers=6, num_attention_heads=8, intermediate_size=args.ff_size)\n",
    "            self.bert.model = BertModel(bert_config)\n",
    "            self.encoder = Classifier(self.bert.model.config.hidden_size)\n",
    "\n",
    "        if args.param_init != 0.0:\n",
    "            for p in self.encoder.parameters():\n",
    "                p.data.uniform_(-args.param_init, args.param_init)\n",
    "        if args.param_init_glorot:\n",
    "            for p in self.encoder.parameters():\n",
    "                if p.dim() > 1:\n",
    "                    xavier_uniform_(p)\n",
    "\n",
    "        self.to(device)\n",
    "    def load_cp(self, pt):\n",
    "        self.load_state_dict(pt['model'], strict=True)\n",
    "\n",
    "    def forward(self, x, segs, clss, mask, mask_cls, sentence_range=None):\n",
    "\n",
    "        top_vec = self.bert(x, segs, mask)\n",
    "        sents_vec = top_vec[torch.arange(top_vec.size(0)).unsqueeze(1), clss]\n",
    "        sents_vec = sents_vec * mask_cls[:, :, None].float()\n",
    "        sent_scores = self.encoder(sents_vec, mask_cls).squeeze(-1)\n",
    "        return sent_scores, mask_cls"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
