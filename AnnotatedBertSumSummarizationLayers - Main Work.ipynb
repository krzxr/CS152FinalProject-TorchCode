{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-24T23:02:44.244350Z",
     "start_time": "2020-11-24T23:02:36.026158Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "P9TduVJGW9LH",
    "outputId": "160772ce-7d5f-41ac-f276-76e8e0f29517"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/krong/.local/bin/pip\", line 6, in <module>\n",
      "    from pip._internal.main import main\n",
      "  File \"/usr/lib/python3/dist-packages/pip/__init__.py\", line 22, in <module>\n",
      "    from pip._vendor.requests.packages.urllib3.exceptions import DependencyWarning\n",
      "  File \"/usr/lib/python3/dist-packages/pip/_vendor/__init__.py\", line 82, in <module>\n",
      "    vendored(\"requests.packages.urllib3._collections\")\n",
      "  File \"/usr/lib/python3/dist-packages/pip/_vendor/__init__.py\", line 33, in vendored\n",
      "    __import__(vendored_name, globals(), locals(), level=0)\n",
      "  File \"<frozen importlib._bootstrap>\", line 983, in _find_and_load\n",
      "  File \"<frozen importlib._bootstrap>\", line 967, in _find_and_load_unlocked\n",
      "  File \"<frozen importlib._bootstrap>\", line 668, in _load_unlocked\n",
      "  File \"<frozen importlib._bootstrap>\", line 638, in _load_backward_compatible\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 1459, in _fix_up_module\n",
      "KeyboardInterrupt\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/krong/.local/bin/pip\", line 6, in <module>\n",
      "    from pip._internal.main import main\n",
      "ModuleNotFoundError: No module named 'pip._internal'\n",
      "Error in sys.excepthook:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3/dist-packages/apport_python_hook.py\", line 63, in apport_excepthook\n",
      "    from apport.fileutils import likely_packaged, get_recent_crashes\n",
      "  File \"/usr/lib/python3/dist-packages/apport/__init__.py\", line 5, in <module>\n",
      "    from apport.report import Report\n",
      "  File \"/usr/lib/python3/dist-packages/apport/report.py\", line 30, in <module>\n",
      "    import apport.fileutils\n",
      "  File \"/usr/lib/python3/dist-packages/apport/fileutils.py\", line 23, in <module>\n",
      "    from apport.packaging_impl import impl as packaging\n",
      "  File \"/usr/lib/python3/dist-packages/apport/packaging_impl.py\", line 24, in <module>\n",
      "    import apt\n",
      "  File \"/usr/lib/python3/dist-packages/apt/__init__.py\", line 23, in <module>\n",
      "    import apt_pkg\n",
      "ModuleNotFoundError: No module named 'apt_pkg'\n",
      "\n",
      "Original exception was:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/krong/.local/bin/pip\", line 6, in <module>\n",
      "    from pip._internal.main import main\n",
      "ModuleNotFoundError: No module named 'pip._internal'\n"
     ]
    }
   ],
   "source": [
    "!pip install pytorch_pretrained_bert\n",
    "!pip install pyrouge\n",
    "# !pip install tensorboardX\n",
    "# !pip install multiprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-11T11:08:00.475946Z",
     "start_time": "2020-11-11T11:07:59.377780Z"
    },
    "id": "1RAytaZu7BSF"
   },
   "outputs": [],
   "source": [
    "import pytorch_pretrained_bert\n",
    "import math\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import os\n",
    "#from tensorboardX import SummaryWriter\n",
    "\n",
    "import time\n",
    "#import distributed\n",
    "\n",
    "import gc\n",
    "import glob\n",
    "#import hashlib\n",
    "#import itertools\n",
    "import json\n",
    "import re\n",
    "import subprocess\n",
    "import time\n",
    "from os.path import join as pjoin\n",
    "\n",
    "#from multiprocess import Pool\n",
    "from pytorch_pretrained_bert import BertTokenizer\n",
    "import numpy as np\n",
    "import random \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "wdO4grV_EfBX"
   },
   "outputs": [],
   "source": [
    "!mkdir bert_data\n",
    "!mkdir results\n",
    "!mkdir tmp\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "adCrJIQJ7BSL"
   },
   "source": [
    "# Encoder -  Built from Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-12T03:26:20.879250Z",
     "start_time": "2020-11-12T03:26:20.854223Z"
    },
    "code_folding": [
     11
    ],
    "id": "hb9ItP0k7BSM"
   },
   "outputs": [],
   "source": [
    "# function is our implementation\n",
    "# function2 is the offical implementation\n",
    "'''\n",
    "# verified\n",
    "torch.seed()\n",
    "pe = PositionalEncoding(4,.8,2)\n",
    "a=torch.tensor([[1,0,0,1],[1,1,0,0]])\n",
    "pe(a),pe.getPositonalEncoding(a)\n",
    "torch.seed()\n",
    "pe = PositionalEncoding2(.8,4,2)\n",
    "a=torch.tensor([[1,0,0,1],[1,1,0,0]])\n",
    "pe(a),pe.get_emb(a)\n",
    "'''\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self,embeddingDim,dropout,maxWordLen=5000):\n",
    "        positionalEncoding = torch.zeros(maxWordLen,embeddingDim)\n",
    "        # keep dim 1 array a singleton\n",
    "        position = torch.arange(0,maxWordLen).unsqueeze(1)\n",
    "        # positional encoding is defined to be \n",
    "        # PE(pos,2i)=sin(pos/1e4^(2i/embedding_dim))\n",
    "        # PE(pos,2i+1)=cos(pos/1e4^(2i/embedding_dim))\n",
    "        exponentTerm = torch.arange(0,embeddingDim,2, dtype=torch.float)\\\n",
    "                *(-math.log(1e4*1.0)/embeddingDim)\n",
    "        divisionTerm = torch.exp(exponentTerm)\n",
    "        # all even indices\n",
    "        positionalEncoding[:,0::2] = torch.sin(position.float()*divisionTerm)\n",
    "        # all odd indices\n",
    "        positionalEncoding[:,1::2] = torch.cos(position.float()*divisionTerm)\n",
    "        # keep dim 0 array size 1 --> a single array\n",
    "        positionalEncoding = positionalEncoding.unsqueeze(0)\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.register_buffer('positionalEncoding', positionalEncoding)\n",
    "        #dropout is probability of dropout\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        self.embeddingDim = embeddingDim\n",
    "        \n",
    "    def forward(self,embedding):\n",
    "        # optional: add step -- not sure what it does\n",
    "        # for dropout?\n",
    "        embedding = embedding * math.sqrt(self.embeddingDim)\n",
    "        embedding = embedding + self.positionalEncoding[:, :embedding.size(1)]\n",
    "        embedding = self.dropout(embedding)\n",
    "        return embedding\n",
    "    def getPositonalEncoding(self,embedding):\n",
    "        return self.positionalEncoding[:, :embedding.size(1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-12T03:26:23.328770Z",
     "start_time": "2020-11-12T03:26:23.302825Z"
    },
    "code_folding": [
     11
    ],
    "id": "dTQCccgF7BSP"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "# correctness confirmed\n",
    "torch.random.manual_seed(42)\n",
    "pff=PositionwiseFeedForward2(4,2,.1)\n",
    "a=torch.tensor([[1.0,0,0,1],[1.0,1,0,0]])\n",
    "pff(a)\n",
    "torch.random.manual_seed(42)\n",
    "pff=PositionwiseFeedForward(4,2,.1)\n",
    "a=torch.tensor([[1.0,0,0,1],[1.0,1,0,0]])\n",
    "pff(a)\n",
    "'''\n",
    "class PositionwiseFeedForward(nn.Module):\n",
    "    '''\n",
    "    A two-layer Feed-Forward-Network with residual layer norm.\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, modelDim, feedforwardDim, dropout=0.1):\n",
    "        super(PositionwiseFeedForward, self).__init__()\n",
    "        self.linear1 = nn.Linear(modelDim, feedforwardDim)\n",
    "        self.linear2 = nn.Linear(feedforwardDim, modelDim)\n",
    "        self.layerNorm = nn.LayerNorm(modelDim, eps=1e-6)\n",
    "        # activation function\n",
    "        self.gelu = lambda x: \\\n",
    "                0.5 * x * (1 + torch.tanh(math.sqrt(2 / math.pi) * (x + 0.044715 * torch.pow(x, 3))))\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        layerNorm = self.layerNorm(x)\n",
    "        linear1 = self.linear1(layerNorm)\n",
    "        gelu = self.gelu(linear1)\n",
    "        hidden = self.dropout1(gelu)\n",
    "        #hidden = self.dropout1(self.gelu(self.linear1(self.layerNorm(x))))\n",
    "        output = self.dropout2(self.linear2(hidden))\n",
    "        return output + x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-12T03:32:48.277785Z",
     "start_time": "2020-11-12T03:32:48.196248Z"
    },
    "code_folding": [
     1,
     88
    ],
    "id": "C4ltuECc7BSW"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "def exampleOfMultiHeadedAttention():\n",
    "    torch.random.manual_seed(42)\n",
    "    m=MultiHeadedAttention(2,4)\n",
    "    a=torch.tensor([[1.0,2,3,1]])\n",
    "    b=torch.tensor([[1.0,0.5,0.3,1]])\n",
    "    c=torch.tensor([[1.0,0.5,0.7,1]])\n",
    "    d={'selfKeys': torch.tensor([[[[ 1.7056, -0.2705]],\\\n",
    "    [[ 0.8711,  1.2850]]]]), 'selfValues': torch.tensor([[[[ 0.0890, -0.4489]],\\\n",
    "    [[-0.4343,  0.5302]]]])}\n",
    "    #m(a,b,c,layerCache={\"memoryKeys\":None},types=\"context\")\n",
    "    m(a,b,c,layerCache=d,types=\"self\")\n",
    "    torch.random.manual_seed(42)\n",
    "    m=MultiHeadedAttention2(2,4)\n",
    "    a=torch.tensor([[1.0,2,3,1]])\n",
    "    b=torch.tensor([[1.0,0.5,0.3,1]])\n",
    "    c=torch.tensor([[1.0,0.5,0.7,1]])\n",
    "    d={'self_keys': torch.tensor([[[[ 1.7056, -0.2705]],\\\n",
    "    [[ 0.8711,  1.2850]]]]), 'self_values': torch.tensor([[[[ 0.0890, -0.4489]],\\\n",
    "    [[-0.4343,  0.5302]]]])}\n",
    "    m(a,b,c,layer_cache=d,type=\"self\")\n",
    "\n",
    "\n",
    "    \n",
    "    torch.random.manual_seed(42)\n",
    "    m=MultiHeadedAttention(2,4)\n",
    "    a=torch.tensor([[1.0,2,3,1]])\n",
    "    b=torch.tensor([[1.0,0.5,0.3,1]])\n",
    "    c=torch.tensor([[1.0,0.5,0.7,1]])\n",
    "    d={'memoryKeys': torch.tensor([[[[ 1.7056, -0.2705]],\\\n",
    "    [[ 0.8711,  1.2850]]]]), 'memoryValues': torch.tensor([[[[ 0.0890, -0.4489]],\\\n",
    "    [[-0.4343,  0.5302]]]])}\n",
    "    #m(a,b,c,layerCache={\"memoryKeys\":None},types=\"context\")\n",
    "    m(a,b,c,layerCache=d,types=\"context\")\n",
    "    torch.random.manual_seed(42)\n",
    "    m=MultiHeadedAttention2(2,4)\n",
    "    a=torch.tensor([[1.0,2,3,1]])\n",
    "    b=torch.tensor([[1.0,0.5,0.3,1]])\n",
    "    c=torch.tensor([[1.0,0.5,0.7,1]])\n",
    "    d={'memory_keys': torch.tensor([[[[ 1.7056, -0.2705]],\\\n",
    "    [[ 0.8711,  1.2850]]]]), 'memory_values': torch.tensor([[[[ 0.0890, -0.4489]],\\\n",
    "    [[-0.4343,  0.5302]]]])}\n",
    "    m(a,b,c,layer_cache=d,type=\"context\")\n",
    "    \n",
    "    \n",
    "    torch.random.manual_seed(42)\n",
    "    m=MultiHeadedAttention2(2,4)\n",
    "    a=torch.tensor([[1.0,2,3,1]])\n",
    "    b=torch.tensor([[1.0,0.5,0.3,1]])\n",
    "    c=torch.tensor([[1.0,0.5,0.7,1]])\n",
    "    d={'memory_keys': None, 'memory_values': None}\n",
    "    m(a,b,c,layer_cache=d,type=\"context\")\n",
    "    torch.random.manual_seed(42)\n",
    "    m=MultiHeadedAttention(2,4)\n",
    "    a=torch.tensor([[1.0,2,3,1]])\n",
    "    b=torch.tensor([[1.0,0.5,0.3,1]])\n",
    "    c=torch.tensor([[1.0,0.5,0.7,1]])\n",
    "    d={'memoryKeys': None, 'memoryValues': None}\n",
    "    m(a,b,c,layerCache=d,types=\"context\")\n",
    "    \n",
    "    \n",
    "    torch.random.manual_seed(42)\n",
    "    m=MultiHeadedAttention2(2,4)\n",
    "    a=torch.tensor([[1.0,2,3,1]])\n",
    "    b=torch.tensor([[1.0,0.5,0.3,1]])\n",
    "    c=torch.tensor([[1.0,0.5,0.7,1]])\n",
    "    d={'self_keys': None, 'self_values': None}\n",
    "    m(a,b,c,layer_cache=d,type=\"self\")\n",
    "    torch.random.manual_seed(42)\n",
    "    m=MultiHeadedAttention(2,4)\n",
    "    a=torch.tensor([[1.0,2,3,1]])\n",
    "    b=torch.tensor([[1.0,0.5,0.3,1]])\n",
    "    c=torch.tensor([[1.0,0.5,0.7,1]])\n",
    "    d={'selfKeys': None, 'selfValues': None}\n",
    "    m(a,b,c,layerCache=d,types=\"self\")\n",
    "    \n",
    "    torch.random.manual_seed(42)\n",
    "    m=MultiHeadedAttention2(2,4)\n",
    "    a=torch.tensor([[1.0,2,3,1]])\n",
    "    b=torch.tensor([[1.0,0.5,0.3,1]])\n",
    "    c=torch.tensor([[1.0,0.5,0.7,1]])\n",
    "    d={'self_keys': None, 'self_values': None}\n",
    "    torch.random.manual_seed(42)\n",
    "    m=MultiHeadedAttention(2,4)\n",
    "    a=torch.tensor([[1.0,2,3,1]])\n",
    "    b=torch.tensor([[1.0,0.5,0.3,1]])\n",
    "    c=torch.tensor([[1.0,0.5,0.7,1]])\n",
    "'''\n",
    "class MultiHeadedAttention(nn.Module):\n",
    "    def __init__(self,numHeads, modelDim, dropout=0.1, isFinalLinear=True):\n",
    "        assert modelDim%numHeads == 0\n",
    "        self.dimPerHead =modelDim//numHeads \n",
    "        self.modelDim = modelDim\n",
    "        \n",
    "        super(MultiHeadedAttention,self).__init__()\n",
    "        self.numHeads = numHeads\n",
    "        self.linearKeys = nn.Linear(modelDim,numHeads*self.dimPerHead)\n",
    "        self.linearValues = nn.Linear(modelDim,numHeads*self.dimPerHead)\n",
    "        self.linearQuery = nn.Linear(modelDim,numHeads*self.dimPerHead)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.isFinalLinear = isFinalLinear\n",
    "        if self.isFinalLinear:\n",
    "            self.finalLayer = nn.Linear(modelDim,modelDim)\n",
    "    def forward(self,key,value,query,mask = None,\n",
    "               layerCache = None, types = None):\n",
    "\n",
    "        batchSize = key.size(0) # size(0) is batch size for all three vectors\n",
    "        dimPerHead = self.dimPerHead\n",
    "        numHeads = self.numHeads\n",
    "        \n",
    "        shape = lambda x: x.view(batchSize,-1,numHeads,dimPerHead).transpose(1,2)\n",
    "        # why contiguous: https://discuss.pytorch.org/t/when-and-why-do-we-use-contiguous/47588\n",
    "        # we might not need it\n",
    "        # apparently in the old version, transpose only changes the view of data, but not data itself\n",
    "        # so to force it to change the data, use contiguous \n",
    "        unshape = lambda x: x.transpose(1,2).contiguous().view(batchSize,-1,numHeads*dimPerHead)\n",
    "        \n",
    "        # get key, value, and query\n",
    "        if layerCache is None:\n",
    "     \n",
    "            key = self.linearKeys(key)\n",
    "            value = self.linearValues(value)\n",
    "            query = self.linearQuery(query)\n",
    "            key = shape(key)\n",
    "            value = shape(value)\n",
    "            \n",
    "        else:\n",
    "  \n",
    "            # Note: this is different from the original code. the original code has \n",
    "            # if statement that is already tested, and else statements that will \n",
    "            # never get use\n",
    "            \n",
    "            # concatenate to variable key\" and \"value\" to their respective caches. \n",
    "            if types == \"self\":\n",
    "        \n",
    "                # all query??\n",
    "                key = self.linearKeys(query)\n",
    "                value = self.linearValues(query)\n",
    "                query = self.linearQuery(query)\n",
    "                key = shape(key)\n",
    "                value = shape(value)\n",
    "                device = key.device\n",
    "                \n",
    "                itemPairsToUpdate = [[key,\"selfKeys\"],[value,\"selfValues\"]]\n",
    "                \n",
    "                \n",
    "                for i in range(2):\n",
    "                    variable, variableName = itemPairsToUpdate[i]\n",
    "                    if layerCache[variableName] is not None:\n",
    "                        itemPairsToUpdate[i][0] = torch.cat((layerCache[variableName].to(device),variable), dim=2)\n",
    "                    layerCache[variableName] = itemPairsToUpdate[i][0]\n",
    "                key, value = itemPairsToUpdate[0][0],itemPairsToUpdate[1][0]\n",
    "                \n",
    "            elif types == \"context\":\n",
    "               \n",
    "                # if no cache, create the cache, \n",
    "                # else copy the cache to the variables \n",
    "                query = self.linearQuery(query)\n",
    "                if layerCache[\"memoryKeys\"] is None:\n",
    "                    # checked!\n",
    "                    key = self.linearKeys(key)\n",
    "                    value = self.linearValues(value)\n",
    "                    key = shape(key)\n",
    "                    value = shape(value)\n",
    "\n",
    "                else:\n",
    "                    key, value = layerCache[\"memoryKeys\"], layerCache[\"memoryValues\"]\n",
    "                layerCache[\"memoryKeys\"] = key\n",
    "                layerCache[\"memoryValues\"] = value\n",
    "\n",
    "        query = shape(query)\n",
    "        \n",
    "        '''\n",
    "        # possibly for debugging purpose\n",
    "        keyLength = key.size(2)\n",
    "        queryLength = query.size(2)\n",
    "        '''\n",
    "        \n",
    "        # compute and scale the scores\n",
    "        \n",
    "        # sqrt to deal with dropout\n",
    "    \n",
    "        query = query / math.sqrt(dimPerHead)\n",
    "    \n",
    "        scores = torch.matmul(query, key.transpose(2, 3))\n",
    "        \n",
    "        if mask is not None:\n",
    "            mask = mask.unsqueeze(1).expand_as(scores)\n",
    "            scores = scores.masked_fill(mask,-1e18) # negative infinity \n",
    "            \n",
    "        # apply attention dropout and compute context vectors \n",
    "        attention = self.softmax(scores)\n",
    "        attentionDropout = self.dropout(attention)\n",
    "        \n",
    "        if self.isFinalLinear:\n",
    "            context = unshape(torch.matmul(attentionDropout,value))\n",
    "            output = self.finalLayer(context)\n",
    "            return output\n",
    "        else:\n",
    "            context = torch.matmul(attentionDropout,value)\n",
    "            return context\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-12T03:21:13.342757Z",
     "start_time": "2020-11-12T03:21:13.324267Z"
    },
    "code_folding": [
     16
    ],
    "id": "fcWb4Cxs7BSg"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "torch.random.manual_seed(42)\n",
    "t = TransformerEncoderLayer2(4,2,2,.1)\n",
    "a=torch.tensor([[1.0,2,3,1],[1.0,2,3,1]])\n",
    "b=torch.tensor([[1.0,0.5,0.3,1],[1.0,0.5,0.3,1]])\n",
    "c=torch.tensor([[1.0,0.5,0.7,1]])\n",
    "d=torch.tensor([[True]])\n",
    "t(0,a,b,d)\n",
    "torch.random.manual_seed(42)\n",
    "t = TransformerEncoderLayer(2,4,2,.1)\n",
    "a=torch.tensor([[1.0,2,3,1],[1.0,2,3,1]])\n",
    "b=torch.tensor([[1.0,0.5,0.3,1],[1.0,0.5,0.3,1]])\n",
    "c=torch.tensor([[1.0,0.5,0.7,1]])\n",
    "d=torch.tensor([[True]])\n",
    "t(0,a,b,d)\n",
    "'''\n",
    "class TransformerEncoderLayer(nn.Module):\n",
    "    def __init__(self, numHeads, modelDim, feedForwardDim, dropout):\n",
    "        super(TransformerEncoderLayer, self).__init__()\n",
    "        self.selfAttention = MultiHeadedAttention(numHeads,modelDim,dropout=dropout)\n",
    "        self.feedForward = PositionwiseFeedForward(modelDim,feedForwardDim,dropout)\n",
    "        # output = (input - Expectation[input])/sqrt(variance(input)+eps)*gamma+beta\n",
    "        # gamma and beta learnable \n",
    "        # normalizing over the last dim\n",
    "        self.layerNorm = nn.LayerNorm(modelDim,eps=1e-6)\n",
    "        #dropout is probability of dropout\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self,iteration,query,inputs,mask):\n",
    "        # if it is not the first iteration, then normalize the layer \n",
    "        inputsNorm = self.layerNorm(inputs) if iteration != 0 else inputs \n",
    "        # keep dim 1 a singleton\n",
    "        mask=mask.unsqueeze(1)\n",
    "        # why?\n",
    "        contextEncoding = self.selfAttention(inputsNorm,inputsNorm,inputsNorm,mask=mask)\n",
    "        # why do we add input back?\n",
    "        contextEncodingWDropout = self.dropout(contextEncoding) + inputs \n",
    "        return self.feedForward(contextEncodingWDropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-12T03:34:34.456069Z",
     "start_time": "2020-11-12T03:34:34.384751Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Tvqeo4oG7BSo",
    "outputId": "11ead097-0950-4e2f-ed6b-dcf26980b7a0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-1.3448,  0.3613,  1.3798, -0.3963],\n",
      "         [-1.2437, -0.6751,  1.2393,  0.6795]]],\n",
      "       grad_fn=<NativeLayerNormBackward>) tensor([[[ 0.4179,  0.2501],\n",
      "         [ 0.4903, -0.0544]]], grad_fn=<AddBackward0>) tensor([[[ 0.2767,  0.1498],\n",
      "         [ 0.3374, -0.0260]]], grad_fn=<MulBackward0>) tensor([[[ 0.3074,  0.1664],\n",
      "         [ 0.3749, -0.0289]]], grad_fn=<MulBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0.4031, 0.3847]], grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 115,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.random.manual_seed(42)\n",
    "t = InterSentencesTransformerEncoderLayer(4,2,2,.1,1)\n",
    "a=torch.tensor([[[1.0,2,3,1],[1.0,2,3,1]]])\n",
    "d=torch.tensor([[True]])\n",
    "t(a,d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-12T03:34:50.740868Z",
     "start_time": "2020-11-12T03:34:50.704484Z"
    },
    "code_folding": [
     11
    ],
    "id": "YN-DRvaO7BSt"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "torch.random.manual_seed(42)\n",
    "t = InterSentencesTransformerEncoderLayer(4,2,2,.1,1)\n",
    "a=torch.tensor([[[1.0,2,3,1],[1.0,2,3,1]]])\n",
    "d=torch.tensor([[True]])\n",
    "t(a,d)\n",
    "'''\n",
    "#inter-sentence transformer focuses on learning relationship between sentences to produce a document level summary\n",
    "# the input to this transformer is output of Bert, which can be viewed as contextual encoding. \n",
    "# modelDim is the output of the Bert's hidden size, or Bert's transformer's model dimension size \n",
    "# (model dimension size is the word used in the self hidden is all you need)\n",
    "class InterSentencesTransformerEncoderLayer(nn.Module):\n",
    "    def __init__(self, modelDim, feedforwardDim, numHeads, dropout,numInterSentencesLayers=0):\n",
    "        super(InterSentencesTransformerEncoderLayer, self).__init__()\n",
    "        self.modelDim = modelDim\n",
    "        self.numInterSentencesLayers =  numInterSentencesLayers\n",
    "        self.positionalEmbedding = PositionalEncoding(modelDim,dropout)\n",
    "        self.interSentencesTransformers = nn.ModuleList(\n",
    "            [TransformerEncoderLayer(numHeads, modelDim, feedforwardDim, dropout)\n",
    "             for _ in range(numInterSentencesLayers)])\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.layerNorm = nn.LayerNorm(modelDim, eps=1e-6)\n",
    "        self.linearLayer = nn.Linear(modelDim, 1, bias=True)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    # top_vector in the original code: top vectors? topic vectors?\n",
    "    # topicVectors is the output of Bert, some sort of contextual embedding\n",
    "    # we will use contextualEncoding instead of top_vector\n",
    "    def forward(self,contextualEncoding,mask):\n",
    "        batchSize, nSentences = contextualEncoding.size(0), contextualEncoding.size(1)\n",
    "        positionalEmbedding = self.positionalEmbedding.positionalEncoding[:,:nSentences]\n",
    "        # mask takes [:,:,None] to account for batches?\n",
    "        # x will be the contextualEncoding undergoing transformer operations\n",
    "        x = contextualEncoding * mask[:,:,None].float() + positionalEmbedding\n",
    "        for i in range(self.numInterSentencesLayers):\n",
    "            x = self.interSentencesTransformers[i](i,x,x,~mask)\n",
    "        x = self.layerNorm(x)\n",
    "        sentencesScores = self.sigmoid(self.linearLayer(x))\n",
    "        sentencesScores = sentencesScores.squeeze(-1)*mask.float()\n",
    "        return sentencesScores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-12T03:36:00.375607Z",
     "start_time": "2020-11-12T03:36:00.354611Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Mq3EJYzz7BSw",
    "outputId": "e3a79e45-a113-466c-b448-674d4a05e5c7",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "InterSentencesTransformerEncoderLayer(\n",
      "  (positionalEmbedding): PositionalEncoding(\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (interSentencesTransformers): ModuleList(\n",
      "    (0): TransformerEncoderLayer(\n",
      "      (selfAttention): MultiHeadedAttention(\n",
      "        (linearKeys): Linear(in_features=2, out_features=2, bias=True)\n",
      "        (linearValues): Linear(in_features=2, out_features=2, bias=True)\n",
      "        (linearQuery): Linear(in_features=2, out_features=2, bias=True)\n",
      "        (softmax): Softmax(dim=-1)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (finalLayer): Linear(in_features=2, out_features=2, bias=True)\n",
      "      )\n",
      "      (feedForward): PositionwiseFeedForward(\n",
      "        (linear1): Linear(in_features=2, out_features=4, bias=True)\n",
      "        (linear2): Linear(in_features=4, out_features=2, bias=True)\n",
      "        (layerNorm): LayerNorm((2,), eps=1e-06, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0.1, inplace=False)\n",
      "        (dropout2): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (layerNorm): LayerNorm((2,), eps=1e-06, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (layerNorm): LayerNorm((2,), eps=1e-06, elementwise_affine=True)\n",
      "  (linearLayer): Linear(in_features=2, out_features=1, bias=True)\n",
      "  (sigmoid): Sigmoid()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "t=InterSentencesTransformerEncoderLayer(2,4,2,.1,1)\n",
    "print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "code_folding": [],
    "id": "S4cqr6Eg7BTG"
   },
   "outputs": [],
   "source": [
    "from pytorch_pretrained_bert import BertModel, BertConfig\n",
    "# load out of box bert\n",
    "class Bert(nn.Module):\n",
    "    def __init__(self, temp_dir, load_pretrained_bert, bert_config):\n",
    "        super(Bert, self).__init__()\n",
    "        if(load_pretrained_bert):\n",
    "            self.model = BertModel.from_pretrained('bert-base-uncased', cache_dir=temp_dir)\n",
    "        else:\n",
    "            self.model = BertModel(bert_config)\n",
    "\n",
    "    def forward(self, x, segs, mask):\n",
    "        encoded_layers, _ = self.model(x, segs, attention_mask =mask)\n",
    "        contextualEncoding = encoded_layers[-1]\n",
    "        return contextualEncoding\n",
    "# summarizer \n",
    "class Summarizer(nn.Module):\n",
    "    def __init__(self, args, device, load_pretrained_bert = False, bert_config = None):\n",
    "        super(Summarizer, self).__init__()\n",
    "        self.args = args\n",
    "        self.device = device\n",
    "        self.bert = Bert(args[\"temp_dir\"], load_pretrained_bert, bert_config)\n",
    "        self.encoder = InterSentencesTransformerEncoderLayer(self.bert.model.config.hidden_size, args[\"ff_size\"], args[\"heads\"],\n",
    "                                                   args[\"dropout\"], args[\"inter_layers\"])\n",
    "        if args[\"param_init\"] != 0.0:\n",
    "            for p in self.encoder.parameters():\n",
    "                p.data.uniform_(-args[\"param_init\"], args[\"param_init\"])\n",
    "        if args[\"param_init_glorot\"]:\n",
    "            for p in self.encoder.parameters():\n",
    "                if p.dim() > 1:\n",
    "                    xavier_uniform_(p)\n",
    "\n",
    "        self.to(device)\n",
    "    def load_cp(self, pt):\n",
    "        self.load_state_dict(pt['model'], strict=True)\n",
    "\n",
    "    def forward(self, x, segs, clss, mask, maskCls, sentence_range=None):\n",
    "\n",
    "        contextualEncoding = self.bert(x, segs, mask)\n",
    "        contextualVector = contextualEncoding[torch.arange(contextualEncoding.size(0)).unsqueeze(1), clss]\n",
    "        contextualVector = contextualVector * maskCls[:, :, None].float()\n",
    "        contextualScores = self.encoder(contextualVector, maskCls).squeeze(-1)\n",
    "        return contextualScores, maskCls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4nwbBhuh7BS6"
   },
   "source": [
    "# Training - Modified from Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "code_folding": [
     0
    ],
    "id": "hHMovbIF7BTD"
   },
   "outputs": [],
   "source": [
    "def build_optim(args, model, checkpoint):\n",
    "    \"\"\" Build optimizer \"\"\"\n",
    "    saved_optimizer_state_dict = None\n",
    "    optim = None\n",
    "\n",
    "    if args[\"train_from\"] != '':\n",
    "        print(\"We made a checkpoint\")\n",
    "        optim = checkpoint['optim']\n",
    "        saved_optimizer_state_dict = optim.optimizer.state_dict()\n",
    "    else:\n",
    "        print(\"we created an optimizer\")\n",
    "        optim = Optimizer(\n",
    "            args[\"optim\"], args[\"lr\"], args[\"max_grad_norm\"],\n",
    "            beta1=args[\"beta1\"], beta2=args[\"beta2\"],\n",
    "            decay_method=args[\"decay_method\"],\n",
    "            warmup_steps=args[\"warmup_steps\"])\n",
    "\n",
    "    optim.set_parameters(list(model.named_parameters()))\n",
    "\n",
    "    if args[\"train_from\"] != '':\n",
    "        optim.optimizer.load_state_dict(saved_optimizer_state_dict)\n",
    "        if args[\"visible_gpus\"] != '-1':\n",
    "            for state in optim.optimizer.state.values():\n",
    "                for k, v in state.items():\n",
    "                    if torch.is_tensor(v):\n",
    "                        state[k] = v.cuda()\n",
    "\n",
    "        if (optim.method == 'adam') and (len(optim.optimizer.state) < 1):\n",
    "            raise RuntimeError(\n",
    "                \"Error: loaded Adam optimizer from existing model\" +\n",
    "                \" but optimizer state is empty\")\n",
    "\n",
    "    return optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aq4XplGA7BTM"
   },
   "source": [
    "# Calculation of Rouge - Copied"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "code_folding": [
     0,
     20,
     58
    ],
    "id": "m2EAJVQK7BTN"
   },
   "outputs": [],
   "source": [
    "def cal_rouge(evaluated_ngrams, reference_ngrams):\n",
    "    reference_count = len(reference_ngrams)\n",
    "    evaluated_count = len(evaluated_ngrams)\n",
    "\n",
    "    overlapping_ngrams = evaluated_ngrams.intersection(reference_ngrams)\n",
    "    overlapping_count = len(overlapping_ngrams)\n",
    "\n",
    "    if evaluated_count == 0:\n",
    "        precision = 0.0\n",
    "    else:\n",
    "        precision = overlapping_count / evaluated_count\n",
    "\n",
    "    if reference_count == 0:\n",
    "        recall = 0.0\n",
    "    else:\n",
    "        recall = overlapping_count / reference_count\n",
    "\n",
    "    f1_score = 2.0 * ((precision * recall) / (precision + recall + 1e-8))\n",
    "    return {\"f\": f1_score, \"p\": precision, \"r\": recall}\n",
    "\n",
    "def greedy_selection(doc_sent_list, abstract_sent_list, summary_size):\n",
    "    def _rouge_clean(s):\n",
    "        return re.sub(r'[^a-zA-Z0-9 ]', '', s)\n",
    "\n",
    "    max_rouge = 0.0\n",
    "    abstract = sum(abstract_sent_list, [])\n",
    "    abstract = _rouge_clean(' '.join(abstract)).split()\n",
    "    sents = [_rouge_clean(' '.join(s)).split() for s in doc_sent_list]\n",
    "    evaluated_1grams = [_get_word_ngrams(1, [sent]) for sent in sents]\n",
    "    reference_1grams = _get_word_ngrams(1, [abstract])\n",
    "    evaluated_2grams = [_get_word_ngrams(2, [sent]) for sent in sents]\n",
    "    reference_2grams = _get_word_ngrams(2, [abstract])\n",
    "\n",
    "    selected = []\n",
    "    for s in range(summary_size):\n",
    "        cur_max_rouge = max_rouge\n",
    "        cur_id = -1\n",
    "        for i in range(len(sents)):\n",
    "            if (i in selected):\n",
    "                continue\n",
    "            c = selected + [i]\n",
    "            candidates_1 = [evaluated_1grams[idx] for idx in c]\n",
    "            candidates_1 = set.union(*map(set, candidates_1))\n",
    "            candidates_2 = [evaluated_2grams[idx] for idx in c]\n",
    "            candidates_2 = set.union(*map(set, candidates_2))\n",
    "            rouge_1 = cal_rouge(candidates_1, reference_1grams)['f']\n",
    "            rouge_2 = cal_rouge(candidates_2, reference_2grams)['f']\n",
    "            rouge_score = rouge_1 + rouge_2\n",
    "            if rouge_score > cur_max_rouge:\n",
    "                cur_max_rouge = rouge_score\n",
    "                cur_id = i\n",
    "        if (cur_id == -1):\n",
    "            return selected\n",
    "        selected.append(cur_id)\n",
    "        max_rouge = cur_max_rouge\n",
    "\n",
    "    return sorted(selected)\n",
    "\n",
    "def combination_selection(doc_sent_list, abstract_sent_list, summary_size):\n",
    "    def _rouge_clean(s):\n",
    "        return re.sub(r'[^a-zA-Z0-9 ]', '', s)\n",
    "\n",
    "    max_rouge = 0.0\n",
    "    max_idx = (0, 0)\n",
    "    abstract = sum(abstract_sent_list, [])\n",
    "    abstract = _rouge_clean(' '.join(abstract)).split()\n",
    "    sents = [_rouge_clean(' '.join(s)).split() for s in doc_sent_list]\n",
    "    evaluated_1grams = [_get_word_ngrams(1, [sent]) for sent in sents]\n",
    "    reference_1grams = _get_word_ngrams(1, [abstract])\n",
    "    evaluated_2grams = [_get_word_ngrams(2, [sent]) for sent in sents]\n",
    "    reference_2grams = _get_word_ngrams(2, [abstract])\n",
    "\n",
    "    impossible_sents = []\n",
    "    for s in range(summary_size + 1):\n",
    "        combinations = itertools.combinations([i for i in range(len(sents)) if i not in impossible_sents], s + 1)\n",
    "        for c in combinations:\n",
    "            candidates_1 = [evaluated_1grams[idx] for idx in c]\n",
    "            candidates_1 = set.union(*map(set, candidates_1))\n",
    "            candidates_2 = [evaluated_2grams[idx] for idx in c]\n",
    "            candidates_2 = set.union(*map(set, candidates_2))\n",
    "            rouge_1 = cal_rouge(candidates_1, reference_1grams)['f']\n",
    "            rouge_2 = cal_rouge(candidates_2, reference_2grams)['f']\n",
    "\n",
    "            rouge_score = rouge_1 + rouge_2\n",
    "            if (s == 0 and rouge_score == 0):\n",
    "                impossible_sents.append(c[0])\n",
    "            if rouge_score > max_rouge:\n",
    "                max_idx = c\n",
    "                max_rouge = rouge_score\n",
    "    return sorted(list(max_idx))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gioY0r7w7BTQ"
   },
   "source": [
    "# Data Loader / BATCH - Modified from Source\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "code_folding": [
     1,
     8,
     47,
     63,
     74,
     96
    ],
    "id": "X51AdEJG7BTQ"
   },
   "outputs": [],
   "source": [
    "\n",
    "class Batch(object):\n",
    "    def _pad(self, data, pad_id, width=-1):\n",
    "        if (width == -1):\n",
    "            width = max(len(d) for d in data)\n",
    "        rtn_data = [d + [pad_id] * (width - len(d)) for d in data]\n",
    "        return rtn_data\n",
    "\n",
    "    def __init__(self, data=None, device=None,  is_test=False):\n",
    "        \"\"\"Create a Batch from a list of examples.\"\"\"\n",
    "        if data is not None and data != []:\n",
    "            self.bad_batch = False\n",
    "            self.batch_size = len(data)\n",
    "            pre_src = [x[0] for x in data]\n",
    "            pre_labels = [x[1] for x in data]\n",
    "            pre_segs = [x[2] for x in data]\n",
    "            pre_clss = [x[3] for x in data]\n",
    "\n",
    "            src = torch.tensor(self._pad(pre_src, 0))\n",
    "\n",
    "            labels = torch.tensor(self._pad(pre_labels, 0))\n",
    "            segs = torch.tensor(self._pad(pre_segs, 0))\n",
    "            mask = ~(src == 0)\n",
    "\n",
    "            clss = torch.tensor(self._pad(pre_clss, -1))\n",
    "            mask_cls = ~(clss == -1)\n",
    "            clss[clss == -1] = 0\n",
    "\n",
    "            setattr(self, 'clss', clss.to(device))\n",
    "            setattr(self, 'mask_cls', mask_cls.to(device))\n",
    "            setattr(self, 'src', src.to(device))\n",
    "            setattr(self, 'labels', labels.to(device))\n",
    "            setattr(self, 'segs', segs.to(device))\n",
    "            setattr(self, 'mask', mask.to(device))\n",
    "\n",
    "            if (is_test):\n",
    "                src_str = [x[-2] for x in data]\n",
    "                setattr(self, 'src_str', src_str)\n",
    "                tgt_str = [x[-1] for x in data]\n",
    "                setattr(self, 'tgt_str', tgt_str)\n",
    "        else:\n",
    "          self.bad_batch = True\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.batch_size\n",
    "\n",
    "\n",
    "def batch(data, batch_size):\n",
    "    \"\"\"Yield elements from data in chunks of batch_size.\"\"\"\n",
    "    minibatch, size_so_far = [], 0\n",
    "    for ex in data:\n",
    "        minibatch.append(ex)\n",
    "        size_so_far = simple_batch_size_fn(ex, len(minibatch))\n",
    "        if size_so_far == batch_size:\n",
    "            yield minibatch\n",
    "            minibatch, size_so_far = [], 0\n",
    "        elif size_so_far > batch_size:\n",
    "            yield minibatch[:-1]\n",
    "            minibatch, size_so_far = minibatch[-1:], simple_batch_size_fn(ex, 1)\n",
    "    if minibatch:\n",
    "        yield minibatch\n",
    "\n",
    "\n",
    "def load_dataset(args, corpus_type, shuffle, bert_data_path=\"bert_data/\", pre=\"cnndm\"):\n",
    "    \"\"\"\n",
    "    Dataset generator. Don't do extra stuff here, like printing,\n",
    "    because they will be postponed to the first loading time.\n",
    "    Args:\n",
    "        corpus_type: 'train' or 'valid'\n",
    "    Returns:\n",
    "        A list of dataset, the dataset(s) are lazily loaded.\n",
    "    \"\"\"\n",
    "    assert corpus_type in [\"train\", \"valid\", \"test\"]\n",
    "\n",
    "    def _lazy_dataset_loader(pt_file, corpus_type):\n",
    "        dataset = torch.load(pt_file)\n",
    "        print('Loading %s dataset from %s, number of examples: %d' %\n",
    "                    (corpus_type, pt_file, len(dataset)))\n",
    "        return dataset\n",
    "\n",
    "    # Sort the glob output by file name (by increasing indexes).\n",
    "    allFiles = os.listdir(bert_data_path)\n",
    "    pts = [bert_data_path+file for file in allFiles if file[-3:] == '.pt']\n",
    "    if pts:\n",
    "        if (shuffle):\n",
    "            random.shuffle(pts)\n",
    "\n",
    "        for pt in pts:\n",
    "            print(pt)\n",
    "            yield _lazy_dataset_loader(pt, corpus_type)\n",
    "    else:\n",
    "        # Only one inputters.*Dataset, simple!\n",
    "        pt = bert_data_path + pre + '.' + corpus_type + '.pt'\n",
    "        yield _lazy_dataset_loader(pt, corpus_type)\n",
    "\n",
    "\n",
    "def simple_batch_size_fn(new, count):\n",
    "    src, labels = new[0], new[1]\n",
    "    global max_n_sents, max_n_tokens, max_size\n",
    "    if count == 1:\n",
    "        max_size = 0\n",
    "        max_n_sents=0\n",
    "        max_n_tokens=0\n",
    "    max_n_sents = max(max_n_sents, len(src))\n",
    "    max_size = max(max_size, max_n_sents)\n",
    "    src_elements = count * max_size\n",
    "    return src_elements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LEqHX-IQ7BTT"
   },
   "source": [
    "# Data Loader Class + Iterators - Modified from Source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "code_folding": [
     0,
     39
    ],
    "id": "TyAjWcqd7BTT"
   },
   "outputs": [],
   "source": [
    "class Dataloader(object):\n",
    "    def __init__(self, args, datasets,  batch_size,\n",
    "                 device, shuffle, is_test):\n",
    "        self.args = args\n",
    "        self.datasets = datasets\n",
    "        self.batch_size = batch_size\n",
    "        self.device = device\n",
    "        self.shuffle = shuffle\n",
    "        self.is_test = is_test\n",
    "        self.cur_iter = self._next_dataset_iterator(datasets)\n",
    "\n",
    "        assert self.cur_iter is not None\n",
    "\n",
    "    def __iter__(self):\n",
    "        dataset_iter = (d for d in self.datasets)\n",
    "        while self.cur_iter is not None:\n",
    "            for batch in self.cur_iter:\n",
    "                yield batch\n",
    "            self.cur_iter = self._next_dataset_iterator(dataset_iter)\n",
    "\n",
    "\n",
    "    def _next_dataset_iterator(self, dataset_iter):\n",
    "        try:\n",
    "            # Drop the current dataset for decreasing memory\n",
    "            if hasattr(self, \"cur_dataset\"):\n",
    "                self.cur_dataset = None\n",
    "                gc.collect()\n",
    "                del self.cur_dataset\n",
    "                gc.collect()\n",
    "\n",
    "            self.cur_dataset = next(dataset_iter)\n",
    "        except StopIteration:\n",
    "            return None\n",
    "\n",
    "        return DataIterator(args = self.args,\n",
    "            dataset=self.cur_dataset,  batch_size=self.batch_size,\n",
    "            device=self.device, shuffle=self.shuffle, is_test=self.is_test)\n",
    "\n",
    "\n",
    "class DataIterator(object):\n",
    "    def __init__(self, args, dataset,  batch_size,  device=None, is_test=False,\n",
    "                 shuffle=True):\n",
    "        self.args = args\n",
    "        self.batch_size, self.is_test, self.dataset = batch_size, is_test, dataset\n",
    "        self.iterations = 0\n",
    "        self.device = device\n",
    "        self.shuffle = shuffle\n",
    "\n",
    "        self.sort_key = lambda x: len(x[1])\n",
    "\n",
    "        self._iterations_this_epoch = 0\n",
    "\n",
    "    def data(self):\n",
    "        if self.shuffle:\n",
    "            random.shuffle(self.dataset)\n",
    "        xs = self.dataset\n",
    "        return xs\n",
    "\n",
    "\n",
    "    def preprocess(self, ex, is_test):\n",
    "        src = ex['src']\n",
    "        if('labels' in ex):\n",
    "            labels = ex['labels']\n",
    "        else:\n",
    "            labels = ex['src_sent_labels']\n",
    "\n",
    "        segs = ex['segs']\n",
    "        if(not self.args[\"use_interval\"]):\n",
    "            segs=[0]*len(segs)\n",
    "        clss = ex['clss']\n",
    "        src_txt = ex['src_txt']\n",
    "        tgt_txt = ex['tgt_txt']\n",
    "\n",
    "        if(is_test):\n",
    "            return src,labels,segs, clss, src_txt, tgt_txt\n",
    "        else:\n",
    "            return src,labels,segs, clss\n",
    "\n",
    "    def batch_buffer(self, data, batch_size):\n",
    "        minibatch, size_so_far = [], 0\n",
    "        for ex in data:\n",
    "            if(len(ex['src'])==0):\n",
    "                continue\n",
    "            ex = self.preprocess(ex, self.is_test)\n",
    "            if(ex is None):\n",
    "                continue\n",
    "            minibatch.append(ex)\n",
    "            size_so_far = simple_batch_size_fn(ex, len(minibatch))\n",
    "            if size_so_far == batch_size:\n",
    "                yield minibatch\n",
    "                minibatch, size_so_far = [], 0\n",
    "            elif size_so_far > batch_size:\n",
    "                yield minibatch[:-1]\n",
    "                minibatch, size_so_far = minibatch[-1:], simple_batch_size_fn(ex, 1)\n",
    "        if minibatch:\n",
    "            yield minibatch\n",
    "\n",
    "    def create_batches(self):\n",
    "        \"\"\" Create batches \"\"\"\n",
    "        data = self.data()\n",
    "        for buffer in self.batch_buffer(data, self.batch_size * 50):\n",
    "\n",
    "            p_batch = sorted(buffer, key=lambda x: len(x[3]))\n",
    "            p_batch = batch(p_batch, self.batch_size)\n",
    "\n",
    "            p_batch = list(p_batch)\n",
    "            if (self.shuffle):\n",
    "                random.shuffle(p_batch)\n",
    "            for b in p_batch:\n",
    "                yield b\n",
    "\n",
    "    def __iter__(self):\n",
    "        while True:\n",
    "            self.batches = self.create_batches()\n",
    "            for idx, minibatch in enumerate(self.batches):\n",
    "                # fast-forward if loaded from state\n",
    "                if self._iterations_this_epoch > idx:\n",
    "                    continue\n",
    "                self.iterations += 1\n",
    "                self._iterations_this_epoch += 1\n",
    "                batch = Batch(minibatch, self.device, self.is_test)\n",
    "\n",
    "                yield batch\n",
    "            return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O4lAWRGL7BTW"
   },
   "source": [
    "# Optimizer - Copied"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "code_folding": [
     4,
     11,
     72,
     105
    ],
    "id": "tKe2tnfV7BTX"
   },
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "\n",
    "\"\"\" Optimizers class \"\"\"\n",
    "def use_gpu(opt):\n",
    "    \"\"\"\n",
    "    Creates a boolean if gpu used\n",
    "    \"\"\"\n",
    "    return (hasattr(opt, 'gpu_ranks') and len(opt.gpu_ranks) > 0) or \\\n",
    "           (hasattr(opt, 'gpu') and opt.gpu > -1)\n",
    "\n",
    "def build_optim2(model, opt, checkpoint):\n",
    "    \"\"\" Build optimizer \"\"\"\n",
    "    saved_optimizer_state_dict = None\n",
    "\n",
    "    if opt.train_from:\n",
    "        optim = checkpoint['optim']\n",
    "        # We need to save a copy of optim.optimizer.state_dict() for setting\n",
    "        # the, optimizer state later on in Stage 2 in this method, since\n",
    "        # the method optim.set_parameters(model.parameters()) will overwrite\n",
    "        # optim.optimizer, and with ith the values stored in\n",
    "        # optim.optimizer.state_dict()\n",
    "        saved_optimizer_state_dict = optim.optimizer.state_dict()\n",
    "    else:\n",
    "        optim = Optimizer(\n",
    "            opt.optim, opt.learning_rate, opt.max_grad_norm,\n",
    "            lr_decay=opt.learning_rate_decay,\n",
    "            start_decay_steps=opt.start_decay_steps,\n",
    "            decay_steps=opt.decay_steps,\n",
    "            beta1=opt.adam_beta1,\n",
    "            beta2=opt.adam_beta2,\n",
    "            adagrad_accum=opt.adagrad_accumulator_init,\n",
    "            decay_method=opt.decay_method,\n",
    "            warmup_steps=opt.warmup_steps)\n",
    "\n",
    "    # Stage 1:\n",
    "    # Essentially optim.set_parameters (re-)creates and optimizer using\n",
    "    # model.paramters() as parameters that will be stored in the\n",
    "    # optim.optimizer.param_groups field of the torch optimizer class.\n",
    "    # Importantly, this method does not yet load the optimizer state, as\n",
    "    # essentially it builds a new optimizer with empty optimizer state and\n",
    "    # parameters from the model.\n",
    "    optim.set_parameters(model.named_parameters())\n",
    "\n",
    "    if opt.train_from:\n",
    "        # Stage 2: In this stage, which is only performed when loading an\n",
    "        # optimizer from a checkpoint, we load the saved_optimizer_state_dict\n",
    "        # into the re-created optimizer, to set the optim.optimizer.state\n",
    "        # field, which was previously empty. For this, we use the optimizer\n",
    "        # state saved in the \"saved_optimizer_state_dict\" variable for\n",
    "        # this purpose.\n",
    "        # See also: https://github.com/pytorch/pytorch/issues/2830\n",
    "        optim.optimizer.load_state_dict(saved_optimizer_state_dict)\n",
    "        # Convert back the state values to cuda type if applicable\n",
    "        if use_gpu(opt):\n",
    "            for state in optim.optimizer.state.values():\n",
    "                for k, v in state.items():\n",
    "                    if torch.is_tensor(v):\n",
    "                        state[k] = v.cuda()\n",
    "\n",
    "        # We want to make sure that indeed we have a non-empty optimizer state\n",
    "        # when we loaded an existing model. This should be at least the case\n",
    "        # for Adam, which saves \"exp_avg\" and \"exp_avg_sq\" state\n",
    "        # (Exponential moving average of gradient and squared gradient values)\n",
    "        if (optim.method == 'adam') and (len(optim.optimizer.state) < 1):\n",
    "            raise RuntimeError(\n",
    "                \"Error: loaded Adam optimizer from existing model\" +\n",
    "                \" but optimizer state is empty\")\n",
    "\n",
    "    return optim\n",
    "\n",
    "\n",
    "class MultipleOptimizer(object):\n",
    "    \"\"\" Implement multiple optimizers needed for sparse adam \"\"\"\n",
    "\n",
    "    def __init__(self, op):\n",
    "        \"\"\" ? \"\"\"\n",
    "        self.optimizers = op\n",
    "\n",
    "    def zero_grad(self):\n",
    "        \"\"\" ? \"\"\"\n",
    "        for op in self.optimizers:\n",
    "            op.zero_grad()\n",
    "\n",
    "    def step(self):\n",
    "        \"\"\" ? \"\"\"\n",
    "        for op in self.optimizers:\n",
    "            op.step()\n",
    "\n",
    "    @property\n",
    "    def state(self):\n",
    "        \"\"\" ? \"\"\"\n",
    "        return {k: v for op in self.optimizers for k, v in op.state.items()}\n",
    "\n",
    "    def state_dict(self):\n",
    "        \"\"\" ? \"\"\"\n",
    "        return [op.state_dict() for op in self.optimizers]\n",
    "\n",
    "    def load_state_dict(self, state_dicts):\n",
    "        \"\"\" ? \"\"\"\n",
    "        assert len(state_dicts) == len(self.optimizers)\n",
    "        for i in range(len(state_dicts)):\n",
    "            self.optimizers[i].load_state_dict(state_dicts[i])\n",
    "\n",
    "\n",
    "class Optimizer(object):\n",
    "    \"\"\"\n",
    "    Controller class for optimization. Mostly a thin\n",
    "    wrapper for `optim`, but also useful for implementing\n",
    "    rate scheduling beyond what is currently available.\n",
    "    Also implements necessary methods for training RNNs such\n",
    "    as grad manipulations.\n",
    "    Args:\n",
    "      method (:obj:`str`): one of [sgd, adagrad, adadelta, adam]\n",
    "      lr (float): learning rate\n",
    "      lr_decay (float, optional): learning rate decay multiplier\n",
    "      start_decay_steps (int, optional): step to start learning rate decay\n",
    "      beta1, beta2 (float, optional): parameters for adam\n",
    "      adagrad_accum (float, optional): initialization parameter for adagrad\n",
    "      decay_method (str, option): custom decay options\n",
    "      warmup_steps (int, option): parameter for `noam` decay\n",
    "    We use the default parameters for Adam that are suggested by\n",
    "    the original paper https://arxiv.org/pdf/1412.6980.pdf\n",
    "    These values are also used by other established implementations,\n",
    "    e.g. https://www.tensorflow.org/api_docs/python/tf/train/AdamOptimizer\n",
    "    https://keras.io/optimizers/\n",
    "    Recently there are slightly different values used in the paper\n",
    "    \"Attention is all you need\"\n",
    "    https://arxiv.org/pdf/1706.03762.pdf, particularly the value beta2=0.98\n",
    "    was used there however, beta2=0.999 is still arguably the more\n",
    "    established value, so we use that here as well\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, method, learning_rate, max_grad_norm,\n",
    "                 lr_decay=1, start_decay_steps=None, decay_steps=None,\n",
    "                 beta1=0.9, beta2=0.999,\n",
    "                 adagrad_accum=0.0,\n",
    "                 decay_method=None,\n",
    "                 warmup_steps=4000\n",
    "                 ):\n",
    "        self.last_ppl = None\n",
    "        self.learning_rate = learning_rate\n",
    "        self.original_lr = learning_rate\n",
    "        self.max_grad_norm = max_grad_norm\n",
    "        self.method = method\n",
    "        self.lr_decay = lr_decay\n",
    "        self.start_decay_steps = start_decay_steps\n",
    "        self.decay_steps = decay_steps\n",
    "        self.start_decay = False\n",
    "        self._step = 0\n",
    "        self.betas = [beta1, beta2]\n",
    "        self.adagrad_accum = adagrad_accum\n",
    "        self.decay_method = decay_method\n",
    "        self.warmup_steps = warmup_steps\n",
    "\n",
    "    def set_parameters(self, params):\n",
    "        \"\"\" ? \"\"\"\n",
    "        self.params = []\n",
    "        self.sparse_params = []\n",
    "        for k, p in params:\n",
    "            if p.requires_grad:\n",
    "                if self.method != 'sparseadam' or \"embed\" not in k:\n",
    "                    self.params.append(p)\n",
    "                else:\n",
    "                    self.sparse_params.append(p)\n",
    "        if self.method == 'sgd':\n",
    "            self.optimizer = optim.SGD(self.params, lr=self.learning_rate)\n",
    "        elif self.method == 'adagrad':\n",
    "            self.optimizer = optim.Adagrad(self.params, lr=self.learning_rate)\n",
    "            for group in self.optimizer.param_groups:\n",
    "                for p in group['params']:\n",
    "                    self.optimizer.state[p]['sum'] = self.optimizer\\\n",
    "                        .state[p]['sum'].fill_(self.adagrad_accum)\n",
    "        elif self.method == 'adadelta':\n",
    "            self.optimizer = optim.Adadelta(self.params, lr=self.learning_rate)\n",
    "        elif self.method == 'adam':\n",
    "            self.optimizer = optim.Adam(self.params, lr=self.learning_rate,\n",
    "                                        betas=self.betas, eps=1e-9)\n",
    "        elif self.method == 'sparseadam':\n",
    "            self.optimizer = MultipleOptimizer(\n",
    "                [optim.Adam(self.params, lr=self.learning_rate,\n",
    "                            betas=self.betas, eps=1e-8),\n",
    "                 optim.SparseAdam(self.sparse_params, lr=self.learning_rate,\n",
    "                                  betas=self.betas, eps=1e-8)])\n",
    "        else:\n",
    "            raise RuntimeError(\"Invalid optim method: \" + self.method)\n",
    "\n",
    "    def _set_rate(self, learning_rate):\n",
    "        self.learning_rate = learning_rate\n",
    "        if self.method != 'sparseadam':\n",
    "            self.optimizer.param_groups[0]['lr'] = self.learning_rate\n",
    "        else:\n",
    "            for op in self.optimizer.optimizers:\n",
    "                op.param_groups[0]['lr'] = self.learning_rate\n",
    "\n",
    "    def step(self):\n",
    "        \"\"\"Update the model parameters based on current gradients.\n",
    "        Optionally, will employ gradient modification or update learning\n",
    "        rate.\n",
    "        \"\"\"\n",
    "        self._step += 1\n",
    "\n",
    "        # Decay method used in tensor2tensor.\n",
    "        if self.decay_method == \"noam\":\n",
    "            self._set_rate(\n",
    "                self.original_lr *\n",
    "\n",
    "                 min(self._step ** (-0.5),\n",
    "                     self._step * self.warmup_steps**(-1.5)))\n",
    "\n",
    "            # self._set_rate(self.original_lr *self.model_size ** (-0.5) *min(1.0, self._step / self.warmup_steps)*max(self._step, self.warmup_steps)**(-0.5))\n",
    "        # Decay based on start_decay_steps every decay_steps\n",
    "        else:\n",
    "            if ((self.start_decay_steps is not None) and (\n",
    "                     self._step >= self.start_decay_steps)):\n",
    "                self.start_decay = True\n",
    "            if self.start_decay:\n",
    "                if ((self._step - self.start_decay_steps)\n",
    "                   % self.decay_steps == 0):\n",
    "                    self.learning_rate = self.learning_rate * self.lr_decay\n",
    "\n",
    "        if self.method != 'sparseadam':\n",
    "            self.optimizer.param_groups[0]['lr'] = self.learning_rate\n",
    "\n",
    "        if self.max_grad_norm:\n",
    "            clip_grad_norm_(self.params, self.max_grad_norm)\n",
    "        self.optimizer.step()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WTqMBREs7BTa"
   },
   "source": [
    "# Eval - Copied"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "code_folding": [
     0,
     184,
     400
    ],
    "id": "IFkD2rB67BTa"
   },
   "outputs": [],
   "source": [
    "class Rouge155(object):\n",
    "    \"\"\"\n",
    "    This is a wrapper for the ROUGE 1.5.5 summary evaluation package.\n",
    "    This class is designed to simplify the evaluation process by:\n",
    "        1) Converting summaries into a format ROUGE understands.\n",
    "        2) Generating the ROUGE configuration file automatically based\n",
    "            on filename patterns.\n",
    "    This class can be used within Python like this:\n",
    "    rouge = Rouge155()\n",
    "    rouge.system_dir = 'test/systems'\n",
    "    rouge.model_dir = 'test/models'\n",
    "    # The system filename pattern should contain one group that\n",
    "    # matches the document ID.\n",
    "    rouge.system_filename_pattern = 'SL.P.10.R.11.SL062003-(\\d+).html'\n",
    "    # The model filename pattern has '#ID#' as a placeholder for the\n",
    "    # document ID. If there are multiple model summaries, pyrouge\n",
    "    # will use the provided regex to automatically match them with\n",
    "    # the corresponding system summary. Here, [A-Z] matches\n",
    "    # multiple model summaries for a given #ID#.\n",
    "    rouge.model_filename_pattern = 'SL.P.10.R.[A-Z].SL062003-#ID#.html'\n",
    "    rouge_output = rouge.evaluate()\n",
    "    print(rouge_output)\n",
    "    output_dict = rouge.output_to_dict(rouge_ouput)\n",
    "    print(output_dict)\n",
    "    ->    {'rouge_1_f_score': 0.95652,\n",
    "         'rouge_1_f_score_cb': 0.95652,\n",
    "         'rouge_1_f_score_ce': 0.95652,\n",
    "         'rouge_1_precision': 0.95652,\n",
    "        [...]\n",
    "    To evaluate multiple systems:\n",
    "        rouge = Rouge155()\n",
    "        rouge.system_dir = '/PATH/TO/systems'\n",
    "        rouge.model_dir = 'PATH/TO/models'\n",
    "        for system_id in ['id1', 'id2', 'id3']:\n",
    "            rouge.system_filename_pattern = \\\n",
    "                'SL.P/.10.R.{}.SL062003-(\\d+).html'.format(system_id)\n",
    "            rouge.model_filename_pattern = \\\n",
    "                'SL.P.10.R.[A-Z].SL062003-#ID#.html'\n",
    "            rouge_output = rouge.evaluate(system_id)\n",
    "            print(rouge_output)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, rouge_dir=None, rouge_args=None, temp_dir = None):\n",
    "        \"\"\"\n",
    "        Create a Rouge155 object.\n",
    "            rouge_dir:  Directory containing Rouge-1.5.5.pl\n",
    "            rouge_args: Arguments to pass through to ROUGE if you\n",
    "                        don't want to use the default pyrouge\n",
    "                        arguments.\n",
    "        \"\"\"\n",
    "        self.temp_dir=temp_dir\n",
    "        self.log = log.get_global_console_logger()\n",
    "        self.__set_dir_properties()\n",
    "        self._config_file = None\n",
    "        self._settings_file = self.__get_config_path()\n",
    "        self.__set_rouge_dir(rouge_dir)\n",
    "        self.args = self.__clean_rouge_args(rouge_args)\n",
    "        self._system_filename_pattern = None\n",
    "        self._model_filename_pattern = None\n",
    "\n",
    "    def save_home_dir(self):\n",
    "        config = ConfigParser()\n",
    "        section = 'pyrouge settings'\n",
    "        config.add_section(section)\n",
    "        config.set(section, 'home_dir', self._home_dir)\n",
    "        with open(self._settings_file, 'w') as f:\n",
    "            config.write(f)\n",
    "        self.log.info(\"Set ROUGE home directory to {}.\".format(self._home_dir))\n",
    "\n",
    "    @property\n",
    "    def settings_file(self):\n",
    "        \"\"\"\n",
    "        Path of the setttings file, which stores the ROUGE home dir.\n",
    "        \"\"\"\n",
    "        return self._settings_file\n",
    "\n",
    "    @property\n",
    "    def bin_path(self):\n",
    "        \"\"\"\n",
    "        The full path of the ROUGE binary (although it's technically\n",
    "        a script), i.e. rouge_home_dir/ROUGE-1.5.5.pl\n",
    "        \"\"\"\n",
    "        if self._bin_path is None:\n",
    "            raise Exception(\n",
    "                \"ROUGE path not set. Please set the ROUGE home directory \"\n",
    "                \"and ensure that ROUGE-1.5.5.pl exists in it.\")\n",
    "        return self._bin_path\n",
    "\n",
    "    @property\n",
    "    def system_filename_pattern(self):\n",
    "        \"\"\"\n",
    "        The regular expression pattern for matching system summary\n",
    "        filenames. The regex string.\n",
    "        E.g. \"SL.P.10.R.11.SL062003-(\\d+).html\" will match the system\n",
    "        filenames in the SPL2003/system folder of the ROUGE SPL example\n",
    "        in the \"sample-test\" folder.\n",
    "        Currently, there is no support for multiple systems.\n",
    "        \"\"\"\n",
    "        return self._system_filename_pattern\n",
    "\n",
    "    @system_filename_pattern.setter\n",
    "    def system_filename_pattern(self, pattern):\n",
    "        self._system_filename_pattern = pattern\n",
    "\n",
    "    @property\n",
    "    def model_filename_pattern(self):\n",
    "        \"\"\"\n",
    "        The regular expression pattern for matching model summary\n",
    "        filenames. The pattern needs to contain the string \"#ID#\",\n",
    "        which is a placeholder for the document ID.\n",
    "        E.g. \"SL.P.10.R.[A-Z].SL062003-#ID#.html\" will match the model\n",
    "        filenames in the SPL2003/system folder of the ROUGE SPL\n",
    "        example in the \"sample-test\" folder.\n",
    "        \"#ID#\" is a placeholder for the document ID which has been\n",
    "        matched by the \"(\\d+)\" part of the system filename pattern.\n",
    "        The different model summaries for a given document ID are\n",
    "        matched by the \"[A-Z]\" part.\n",
    "        \"\"\"\n",
    "        return self._model_filename_pattern\n",
    "\n",
    "    @model_filename_pattern.setter\n",
    "    def model_filename_pattern(self, pattern):\n",
    "        self._model_filename_pattern = pattern\n",
    "\n",
    "    @property\n",
    "    def config_file(self):\n",
    "        return self._config_file\n",
    "\n",
    "    @config_file.setter\n",
    "    def config_file(self, path):\n",
    "        config_dir, _ = os.path.split(path)\n",
    "        verify_dir(config_dir, \"configuration file\")\n",
    "        self._config_file = path\n",
    "\n",
    "    def split_sentences(self):\n",
    "        \"\"\"\n",
    "        ROUGE requires texts split into sentences. In case the texts\n",
    "        are not already split, this method can be used.\n",
    "        \"\"\"\n",
    "        from pyrouge.utils.sentence_splitter import PunktSentenceSplitter\n",
    "        self.log.info(\"Splitting sentences.\")\n",
    "        ss = PunktSentenceSplitter()\n",
    "        sent_split_to_string = lambda s: \"\\n\".join(ss.split(s))\n",
    "        process_func = partial(\n",
    "            DirectoryProcessor.process, function=sent_split_to_string)\n",
    "        self.__process_summaries(process_func)\n",
    "\n",
    "    @staticmethod\n",
    "    def convert_summaries_to_rouge_format(input_dir, output_dir):\n",
    "        \"\"\"\n",
    "        Convert all files in input_dir into a format ROUGE understands\n",
    "        and saves the files to output_dir. The input files are assumed\n",
    "        to be plain text with one sentence per line.\n",
    "            input_dir:  Path of directory containing the input files.\n",
    "            output_dir: Path of directory in which the converted files\n",
    "                        will be saved.\n",
    "        \"\"\"\n",
    "        DirectoryProcessor.process(\n",
    "            input_dir, output_dir, Rouge155.convert_text_to_rouge_format)\n",
    "\n",
    "    @staticmethod\n",
    "    def convert_text_to_rouge_format(text, title=\"dummy title\"):\n",
    "        \"\"\"\n",
    "        Convert a text to a format ROUGE understands. The text is\n",
    "        assumed to contain one sentence per line.\n",
    "            text:   The text to convert, containg one sentence per line.\n",
    "            title:  Optional title for the text. The title will appear\n",
    "                    in the converted file, but doesn't seem to have\n",
    "                    any other relevance.\n",
    "        Returns: The converted text as string.\n",
    "        \"\"\"\n",
    "        # sentences = text.split(\"\\n\")\n",
    "        sentences = text.split(\"<q>\")\n",
    "        sent_elems = [\n",
    "            \"<a name=\\\"{i}\\\">[{i}]</a> <a href=\\\"#{i}\\\" id={i}>\"\n",
    "            \"{text}</a>\".format(i=i, text=sent)\n",
    "            for i, sent in enumerate(sentences, start=1)]\n",
    "        html = \"\"\"<html>\n",
    "<head>\n",
    "<title>{title}</title>\n",
    "</head>\n",
    "<body bgcolor=\"white\">\n",
    "{elems}\n",
    "</body>\n",
    "</html>\"\"\".format(title=title, elems=\"\\n\".join(sent_elems))\n",
    "\n",
    "        return html\n",
    "\n",
    "    @staticmethod\n",
    "    def write_config_static(system_dir, system_filename_pattern,\n",
    "                            model_dir, model_filename_pattern,\n",
    "                            config_file_path, system_id=None):\n",
    "        \"\"\"\n",
    "        Write the ROUGE configuration file, which is basically a list\n",
    "        of system summary files and their corresponding model summary\n",
    "        files.\n",
    "        pyrouge uses regular expressions to automatically find the\n",
    "        matching model summary files for a given system summary file\n",
    "        (cf. docstrings for system_filename_pattern and\n",
    "        model_filename_pattern).\n",
    "            system_dir:                 Path of directory containing\n",
    "                                        system summaries.\n",
    "            system_filename_pattern:    Regex string for matching\n",
    "                                        system summary filenames.\n",
    "            model_dir:                  Path of directory containing\n",
    "                                        model summaries.\n",
    "            model_filename_pattern:     Regex string for matching model\n",
    "                                        summary filenames.\n",
    "            config_file_path:           Path of the configuration file.\n",
    "            system_id:                  Optional system ID string which\n",
    "                                        will appear in the ROUGE output.\n",
    "        \"\"\"\n",
    "        system_filenames = [f for f in os.listdir(system_dir)]\n",
    "        system_models_tuples = []\n",
    "\n",
    "        system_filename_pattern = re.compile(system_filename_pattern)\n",
    "        for system_filename in sorted(system_filenames):\n",
    "            match = system_filename_pattern.match(system_filename)\n",
    "            if match:\n",
    "                id = match.groups(0)[0]\n",
    "                model_filenames = [model_filename_pattern.replace('#ID#',id)]\n",
    "                # model_filenames = Rouge155.__get_model_filenames_for_id(\n",
    "                #     id, model_dir, model_filename_pattern)\n",
    "                system_models_tuples.append(\n",
    "                    (system_filename, sorted(model_filenames)))\n",
    "        if not system_models_tuples:\n",
    "            raise Exception(\n",
    "                \"Did not find any files matching the pattern {} \"\n",
    "                \"in the system summaries directory {}.\".format(\n",
    "                    system_filename_pattern.pattern, system_dir))\n",
    "\n",
    "        with codecs.open(config_file_path, 'w', encoding='utf-8') as f:\n",
    "            f.write('<ROUGE-EVAL version=\"1.55\">')\n",
    "            for task_id, (system_filename, model_filenames) in enumerate(\n",
    "                    system_models_tuples, start=1):\n",
    "\n",
    "                eval_string = Rouge155.__get_eval_string(\n",
    "                    task_id, system_id,\n",
    "                    system_dir, system_filename,\n",
    "                    model_dir, model_filenames)\n",
    "                f.write(eval_string)\n",
    "            f.write(\"</ROUGE-EVAL>\")\n",
    "\n",
    "    def write_config(self, config_file_path=None, system_id=None):\n",
    "        \"\"\"\n",
    "        Write the ROUGE configuration file, which is basically a list\n",
    "        of system summary files and their matching model summary files.\n",
    "        This is a non-static version of write_config_file_static().\n",
    "            config_file_path:   Path of the configuration file.\n",
    "            system_id:          Optional system ID string which will\n",
    "                                appear in the ROUGE output.\n",
    "        \"\"\"\n",
    "        if not system_id:\n",
    "            system_id = 1\n",
    "        if (not config_file_path) or (not self._config_dir):\n",
    "            self._config_dir = mkdtemp(dir=self.temp_dir)\n",
    "            config_filename = \"rouge_conf.xml\"\n",
    "        else:\n",
    "            config_dir, config_filename = os.path.split(config_file_path)\n",
    "            verify_dir(config_dir, \"configuration file\")\n",
    "        self._config_file = os.path.join(self._config_dir, config_filename)\n",
    "        Rouge155.write_config_static(\n",
    "            self._system_dir, self._system_filename_pattern,\n",
    "            self._model_dir, self._model_filename_pattern,\n",
    "            self._config_file, system_id)\n",
    "        self.log.info(\n",
    "            \"Written ROUGE configuration to {}\".format(self._config_file))\n",
    "\n",
    "    def evaluate(self, system_id=1, rouge_args=None):\n",
    "        \"\"\"\n",
    "        Run ROUGE to evaluate the system summaries in system_dir against\n",
    "        the model summaries in model_dir. The summaries are assumed to\n",
    "        be in the one-sentence-per-line HTML format ROUGE understands.\n",
    "            system_id:  Optional system ID which will be printed in\n",
    "                        ROUGE's output.\n",
    "        Returns: Rouge output as string.\n",
    "        \"\"\"\n",
    "        self.write_config(system_id=system_id)\n",
    "        options = self.__get_options(rouge_args)\n",
    "        command = [self._bin_path] + options\n",
    "        self.log.info(\n",
    "            \"Running ROUGE with command {}\".format(\" \".join(command)))\n",
    "        rouge_output = check_output(command).decode(\"UTF-8\")\n",
    "        return rouge_output\n",
    "\n",
    "    def convert_and_evaluate(self, system_id=1,\n",
    "                             split_sentences=False, rouge_args=None):\n",
    "        \"\"\"\n",
    "        Convert plain text summaries to ROUGE format and run ROUGE to\n",
    "        evaluate the system summaries in system_dir against the model\n",
    "        summaries in model_dir. Optionally split texts into sentences\n",
    "        in case they aren't already.\n",
    "        This is just a convenience method combining\n",
    "        convert_summaries_to_rouge_format() and evaluate().\n",
    "            split_sentences:    Optional argument specifying if\n",
    "                                sentences should be split.\n",
    "            system_id:          Optional system ID which will be printed\n",
    "                                in ROUGE's output.\n",
    "        Returns: ROUGE output as string.\n",
    "        \"\"\"\n",
    "        if split_sentences:\n",
    "            self.split_sentences()\n",
    "        self.__write_summaries()\n",
    "        rouge_output = self.evaluate(system_id, rouge_args)\n",
    "        return rouge_output\n",
    "\n",
    "    def output_to_dict(self, output):\n",
    "        \"\"\"\n",
    "        Convert the ROUGE output into python dictionary for further\n",
    "        processing.\n",
    "        \"\"\"\n",
    "        #0 ROUGE-1 Average_R: 0.02632 (95%-conf.int. 0.02632 - 0.02632)\n",
    "        pattern = re.compile(\n",
    "            r\"(\\d+) (ROUGE-\\S+) (Average_\\w): (\\d.\\d+) \"\n",
    "            r\"\\(95%-conf.int. (\\d.\\d+) - (\\d.\\d+)\\)\")\n",
    "        results = {}\n",
    "        for line in output.split(\"\\n\"):\n",
    "            match = pattern.match(line)\n",
    "            if match:\n",
    "                sys_id, rouge_type, measure, result, conf_begin, conf_end = \\\n",
    "                    match.groups()\n",
    "                measure = {\n",
    "                    'Average_R': 'recall',\n",
    "                    'Average_P': 'precision',\n",
    "                    'Average_F': 'f_score'\n",
    "                    }[measure]\n",
    "                rouge_type = rouge_type.lower().replace(\"-\", '_')\n",
    "                key = \"{}_{}\".format(rouge_type, measure)\n",
    "                results[key] = float(result)\n",
    "                results[\"{}_cb\".format(key)] = float(conf_begin)\n",
    "                results[\"{}_ce\".format(key)] = float(conf_end)\n",
    "        return results\n",
    "\n",
    "    ###################################################################\n",
    "    # Private methods\n",
    "\n",
    "    def __set_rouge_dir(self, home_dir=None):\n",
    "        \"\"\"\n",
    "        Verfify presence of ROUGE-1.5.5.pl and data folder, and set\n",
    "        those paths.\n",
    "        \"\"\"\n",
    "        if not home_dir:\n",
    "            self._home_dir = self.__get_rouge_home_dir_from_settings()\n",
    "        else:\n",
    "            self._home_dir = home_dir\n",
    "            self.save_home_dir()\n",
    "        self._bin_path = os.path.join(self._home_dir, 'ROUGE-1.5.5.pl')\n",
    "        self.data_dir = os.path.join(self._home_dir, 'data')\n",
    "        if not os.path.exists(self._bin_path):\n",
    "            raise Exception(\n",
    "                \"ROUGE binary not found at {}. Please set the \"\n",
    "                \"correct path by running pyrouge_set_rouge_path \"\n",
    "                \"/path/to/rouge/home.\".format(self._bin_path))\n",
    "\n",
    "    def __get_rouge_home_dir_from_settings(self):\n",
    "        config = ConfigParser()\n",
    "        with open(self._settings_file) as f:\n",
    "            if hasattr(config, \"read_file\"):\n",
    "                config.read_file(f)\n",
    "            else:\n",
    "                # use deprecated python 2.x method\n",
    "                config.readfp(f)\n",
    "        rouge_home_dir = config.get('pyrouge settings', 'home_dir')\n",
    "        return rouge_home_dir\n",
    "\n",
    "    @staticmethod\n",
    "    def __get_eval_string(\n",
    "            task_id, system_id,\n",
    "            system_dir, system_filename,\n",
    "            model_dir, model_filenames):\n",
    "        \"\"\"\n",
    "        ROUGE can evaluate several system summaries for a given text\n",
    "        against several model summaries, i.e. there is an m-to-n\n",
    "        relation between system and model summaries. The system\n",
    "        summaries are listed in the <PEERS> tag and the model summaries\n",
    "        in the <MODELS> tag. pyrouge currently only supports one system\n",
    "        summary per text, i.e. it assumes a 1-to-n relation between\n",
    "        system and model summaries.\n",
    "        \"\"\"\n",
    "        peer_elems = \"<P ID=\\\"{id}\\\">{name}</P>\".format(\n",
    "            id=system_id, name=system_filename)\n",
    "\n",
    "        model_elems = [\"<M ID=\\\"{id}\\\">{name}</M>\".format(\n",
    "            id=chr(65 + i), name=name)\n",
    "            for i, name in enumerate(model_filenames)]\n",
    "\n",
    "        model_elems = \"\\n\\t\\t\\t\".join(model_elems)\n",
    "        eval_string = \"\"\"\n",
    "    <EVAL ID=\"{task_id}\">\n",
    "        <MODEL-ROOT>{model_root}</MODEL-ROOT>\n",
    "        <PEER-ROOT>{peer_root}</PEER-ROOT>\n",
    "        <INPUT-FORMAT TYPE=\"SEE\">\n",
    "        </INPUT-FORMAT>\n",
    "        <PEERS>\n",
    "            {peer_elems}\n",
    "        </PEERS>\n",
    "        <MODELS>\n",
    "            {model_elems}\n",
    "        </MODELS>\n",
    "    </EVAL>\n",
    "\"\"\".format(\n",
    "            task_id=task_id,\n",
    "            model_root=model_dir, model_elems=model_elems,\n",
    "            peer_root=system_dir, peer_elems=peer_elems)\n",
    "        return eval_string\n",
    "\n",
    "    def __process_summaries(self, process_func):\n",
    "        \"\"\"\n",
    "        Helper method that applies process_func to the files in the\n",
    "        system and model folders and saves the resulting files to new\n",
    "        system and model folders.\n",
    "        \"\"\"\n",
    "        temp_dir = mkdtemp(dir=self.temp_dir)\n",
    "        new_system_dir = os.path.join(temp_dir, \"system\")\n",
    "        os.mkdir(new_system_dir)\n",
    "        new_model_dir = os.path.join(temp_dir, \"model\")\n",
    "        os.mkdir(new_model_dir)\n",
    "        self.log.info(\n",
    "            \"Processing summaries. Saving system files to {} and \"\n",
    "            \"model files to {}.\".format(new_system_dir, new_model_dir))\n",
    "        process_func(self._system_dir, new_system_dir)\n",
    "        process_func(self._model_dir, new_model_dir)\n",
    "        self._system_dir = new_system_dir\n",
    "        self._model_dir = new_model_dir\n",
    "\n",
    "    def __write_summaries(self):\n",
    "        self.log.info(\"Writing summaries.\")\n",
    "        self.__process_summaries(self.convert_summaries_to_rouge_format)\n",
    "\n",
    "    @staticmethod\n",
    "    def __get_model_filenames_for_id(id, model_dir, model_filenames_pattern):\n",
    "        pattern = re.compile(model_filenames_pattern.replace('#ID#', id))\n",
    "        model_filenames = [\n",
    "            f for f in os.listdir(model_dir) if pattern.match(f)]\n",
    "        if not model_filenames:\n",
    "            raise Exception(\n",
    "                \"Could not find any model summaries for the system\"\n",
    "                \" summary with ID {}. Specified model filename pattern was: \"\n",
    "                \"{}\".format(id, model_filenames_pattern))\n",
    "        return model_filenames\n",
    "\n",
    "    def __get_options(self, rouge_args=None):\n",
    "        \"\"\"\n",
    "        Get supplied command line arguments for ROUGE or use default\n",
    "        ones.\n",
    "        \"\"\"\n",
    "        if self.args:\n",
    "            options = self.args.split()\n",
    "        elif rouge_args:\n",
    "            options = rouge_args.split()\n",
    "        else:\n",
    "            options = [\n",
    "                '-e', self._data_dir,\n",
    "                '-c', 95,\n",
    "                # '-2',\n",
    "                # '-1',\n",
    "                # '-U',\n",
    "                '-m',\n",
    "                # '-v',\n",
    "                '-r', 1000,\n",
    "                '-n', 2,\n",
    "                # '-w', 1.2,\n",
    "                '-a',\n",
    "                ]\n",
    "            options = list(map(str, options))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        options = self.__add_config_option(options)\n",
    "        return options\n",
    "\n",
    "    def __create_dir_property(self, dir_name, docstring):\n",
    "        \"\"\"\n",
    "        Generate getter and setter for a directory property.\n",
    "        \"\"\"\n",
    "        property_name = \"{}_dir\".format(dir_name)\n",
    "        private_name = \"_\" + property_name\n",
    "        setattr(self, private_name, None)\n",
    "\n",
    "        def fget(self):\n",
    "            return getattr(self, private_name)\n",
    "\n",
    "        def fset(self, path):\n",
    "            verify_dir(path, dir_name)\n",
    "            setattr(self, private_name, path)\n",
    "\n",
    "        p = property(fget=fget, fset=fset, doc=docstring)\n",
    "        setattr(self.__class__, property_name, p)\n",
    "\n",
    "    def __set_dir_properties(self):\n",
    "        \"\"\"\n",
    "        Automatically generate the properties for directories.\n",
    "        \"\"\"\n",
    "        directories = [\n",
    "            (\"home\", \"The ROUGE home directory.\"),\n",
    "            (\"data\", \"The path of the ROUGE 'data' directory.\"),\n",
    "            (\"system\", \"Path of the directory containing system summaries.\"),\n",
    "            (\"model\", \"Path of the directory containing model summaries.\"),\n",
    "            ]\n",
    "        for (dirname, docstring) in directories:\n",
    "            self.__create_dir_property(dirname, docstring)\n",
    "\n",
    "    def __clean_rouge_args(self, rouge_args):\n",
    "        \"\"\"\n",
    "        Remove enclosing quotation marks, if any.\n",
    "        \"\"\"\n",
    "        if not rouge_args:\n",
    "            return\n",
    "        quot_mark_pattern = re.compile('\"(.+)\"')\n",
    "        match = quot_mark_pattern.match(rouge_args)\n",
    "        if match:\n",
    "            cleaned_args = match.group(1)\n",
    "            return cleaned_args\n",
    "        else:\n",
    "            return rouge_args\n",
    "\n",
    "    def __add_config_option(self, options):\n",
    "        return options + [self._config_file]\n",
    "\n",
    "    def __get_config_path(self):\n",
    "        if platform.system() == \"Windows\":\n",
    "            parent_dir = os.getenv(\"APPDATA\")\n",
    "            config_dir_name = \"pyrouge\"\n",
    "        elif os.name == \"posix\":\n",
    "            parent_dir = os.path.expanduser(\"~\")\n",
    "            config_dir_name = \".pyrouge\"\n",
    "        else:\n",
    "            parent_dir = os.path.dirname(__file__)\n",
    "            config_dir_name = \"\"\n",
    "        config_dir = os.path.join(parent_dir, config_dir_name)\n",
    "        if not os.path.exists(config_dir):\n",
    "            os.makedirs(config_dir)\n",
    "        return os.path.join(config_dir, 'settings.ini')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "K1-5e40z-MRn"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8lz0YY97WzwF"
   },
   "source": [
    "# Training - Modified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "Rz7qzH4kYihY"
   },
   "outputs": [],
   "source": [
    "def IterFunc(corpus_type='train'):\n",
    "        return Dataloader(args, load_dataset(args, corpus_type, shuffle=True), args[\"batch_size\"], device,\n",
    "                                                 shuffle=True, is_test=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "code_folding": [
     3
    ],
    "id": "5E8CD-1m7BS7"
   },
   "outputs": [],
   "source": [
    "loss = torch.nn.BCELoss(reduction='none')\n",
    "\n",
    "# def train(model, optim, trainSteps, trainIterFunc, validIterFunc, savePath, seed=42):\n",
    "def train(args, model, savePath, seed=42):\n",
    "    '''\n",
    "    optim : optimizer\n",
    "    savePath: where the model of the last will be saved.\n",
    "    '''\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    def trainIterFunc():\n",
    "        tmp = load_dataset(args, corpus_type='train', shuffle=True)\n",
    "        return Dataloader(args, tmp, args[\"batch_size\"], device,\n",
    "                                                 shuffle=True, is_test=False)\n",
    "    def validIterFunc():\n",
    "        tmp = load_dataset(args, corpus_type='valid', shuffle=True)\n",
    "        return Dataloader(args, tmp, args[\"batch_size\"], device,\n",
    "                                                 shuffle=True, is_test=False)\n",
    "        \n",
    "    torch.manual_seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    \n",
    "\n",
    "    \n",
    "    model = Summarizer(args, device, load_pretrained_bert=True)\n",
    "    n_gpu = torch.cuda.current_device()\n",
    "    \n",
    "    \n",
    "    if args[\"train_from\"] != '':\n",
    "        print('Loading checkpoint from %s' % args[\"train_from\"])\n",
    "        checkpoint = torch.load(args[\"train_from\"],\n",
    "                                map_location=lambda storage, loc: storage)\n",
    "        opt = vars(checkpoint['opt'])\n",
    "        for k in opt.keys():\n",
    "            if (k in model_flags):\n",
    "                setattr(args, k, opt[k])\n",
    "        model.load_cp(checkpoint)\n",
    "        optim = build_optim(args, model, checkpoint)\n",
    "    else:\n",
    "        optim = build_optim(args, model, None)\n",
    "    \n",
    "    \n",
    "    for step in range(optim._step + 1, args[\"train_steps\"]):\n",
    "        print(\"step\", step)\n",
    "        trainIter = trainIterFunc()\n",
    "        # optim.zero_grad()\n",
    "        trainLoss, trainBatchSize = 0, 0\n",
    "        validLoss, validBatchSize = 0, 0\n",
    "        timeStart = time.time()\n",
    "        \n",
    "        # training \n",
    "        model.train()\n",
    "        for i, batch in enumerate(trainIter):\n",
    "            if batch.bad_batch:\n",
    "              print(\"This is a bad batch\")\n",
    "              continue\n",
    "            print(\"i (batch step)\", i)\n",
    "            labels = batch.labels\n",
    "            segs = batch.segs\n",
    "            cls = batch.clss\n",
    "            mask = batch.mask\n",
    "            mask_cls = batch.mask_cls\n",
    "            src = batch.src\n",
    "\n",
    "\n",
    "            sentenceScores, mask = model(src, segs, cls, mask, mask_cls)\n",
    "            lossVec = loss(sentenceScores, labels.float())\n",
    "            lossMag = (lossVec*mask.float()).sum()\n",
    "            (lossMag/lossMag.numel()).backward()\n",
    "            optim.step()\n",
    "            lossMag.detach()\n",
    "            \n",
    "            trainLoss += lossMag\n",
    "            trainBatchSize += batch.batch_size\n",
    "        \n",
    "        timeEnded_T = time.time()\n",
    "        \n",
    "        # validating \n",
    "        model.eval()\n",
    "        print(\"starting to validate\")\n",
    "        with torch.no_grad():\n",
    "            validIter = validIterFunc() \n",
    "            for batch in validIter:\n",
    "                if batch.bad_batch:\n",
    "                  print(\"This is a bad batch\")\n",
    "                  continue\n",
    "                labels = batch.labels\n",
    "                segs = batch.segs\n",
    "                cls = batch.clss\n",
    "                mask = batch.mask\n",
    "                mask_cls = batch.mask_cls\n",
    "                src = batch.src\n",
    "                \n",
    "                sentenceScores, mask = model(src, segs, cls, mask, mask_cls)\n",
    "                lossVec = loss(sentenceScores, labels.float())\n",
    "                lossMag = (lossVec*mask.float()).sum()\n",
    "                validLoss += lossMag\n",
    "                validBatchSize += batch.batch_size\n",
    "        \n",
    "        timeEnded_V = time.time()\n",
    "        trainTime = timeStart   - timeEnded_T\n",
    "        validTime = timeEnded_T - timeEnded_V\n",
    "        trainLoss /= trainBatchSize\n",
    "        validLoss /= validBatchSize\n",
    "        print(\"Step %s; lr: %7.7f; training loss: %4.2f;\" +\n",
    "             \" trained %6.0f sec; valid loss: %4.2f; validated %6.0f sec\", step, \n",
    "              step, optim.learning_rate, trainLoss, trainTime, validLoss, \n",
    "             validTime)\n",
    "    # saving the model \n",
    "    modelStateDict  = model.state_dict()\n",
    "    # don't have args, check if it's okay when loaded\n",
    "    checkpoint = {\n",
    "        'model' : modelStateDict,\n",
    "        'optim' : optim\n",
    "    }\n",
    "    \n",
    "    checkpointPath = os.path.join(savePath, 'model_step_last.pt')\n",
    "    print(\"Saving checkpoint at\", checkpointPath)\n",
    "    torch.save(checkpoint, checkpointPath)\n",
    "    return checkpoint, checkpointPath\n",
    "    \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0Rfu4zWoS9sv",
    "outputId": "56158f3d-1edc-4ba3-80e1-82d03f61c6d0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('active.all.allocated', 0), ('active.all.current', 0), ('active.all.freed', 0), ('active.all.peak', 0), ('active.large_pool.allocated', 0), ('active.large_pool.current', 0), ('active.large_pool.freed', 0), ('active.large_pool.peak', 0), ('active.small_pool.allocated', 0), ('active.small_pool.current', 0), ('active.small_pool.freed', 0), ('active.small_pool.peak', 0), ('active_bytes.all.allocated', 0), ('active_bytes.all.current', 0), ('active_bytes.all.freed', 0), ('active_bytes.all.peak', 0), ('active_bytes.large_pool.allocated', 0), ('active_bytes.large_pool.current', 0), ('active_bytes.large_pool.freed', 0), ('active_bytes.large_pool.peak', 0), ('active_bytes.small_pool.allocated', 0), ('active_bytes.small_pool.current', 0), ('active_bytes.small_pool.freed', 0), ('active_bytes.small_pool.peak', 0), ('allocated_bytes.all.allocated', 0), ('allocated_bytes.all.current', 0), ('allocated_bytes.all.freed', 0), ('allocated_bytes.all.peak', 0), ('allocated_bytes.large_pool.allocated', 0), ('allocated_bytes.large_pool.current', 0), ('allocated_bytes.large_pool.freed', 0), ('allocated_bytes.large_pool.peak', 0), ('allocated_bytes.small_pool.allocated', 0), ('allocated_bytes.small_pool.current', 0), ('allocated_bytes.small_pool.freed', 0), ('allocated_bytes.small_pool.peak', 0), ('allocation.all.allocated', 0), ('allocation.all.current', 0), ('allocation.all.freed', 0), ('allocation.all.peak', 0), ('allocation.large_pool.allocated', 0), ('allocation.large_pool.current', 0), ('allocation.large_pool.freed', 0), ('allocation.large_pool.peak', 0), ('allocation.small_pool.allocated', 0), ('allocation.small_pool.current', 0), ('allocation.small_pool.freed', 0), ('allocation.small_pool.peak', 0), ('inactive_split.all.allocated', 0), ('inactive_split.all.current', 0), ('inactive_split.all.freed', 0), ('inactive_split.all.peak', 0), ('inactive_split.large_pool.allocated', 0), ('inactive_split.large_pool.current', 0), ('inactive_split.large_pool.freed', 0), ('inactive_split.large_pool.peak', 0), ('inactive_split.small_pool.allocated', 0), ('inactive_split.small_pool.current', 0), ('inactive_split.small_pool.freed', 0), ('inactive_split.small_pool.peak', 0), ('inactive_split_bytes.all.allocated', 0), ('inactive_split_bytes.all.current', 0), ('inactive_split_bytes.all.freed', 0), ('inactive_split_bytes.all.peak', 0), ('inactive_split_bytes.large_pool.allocated', 0), ('inactive_split_bytes.large_pool.current', 0), ('inactive_split_bytes.large_pool.freed', 0), ('inactive_split_bytes.large_pool.peak', 0), ('inactive_split_bytes.small_pool.allocated', 0), ('inactive_split_bytes.small_pool.current', 0), ('inactive_split_bytes.small_pool.freed', 0), ('inactive_split_bytes.small_pool.peak', 0), ('num_alloc_retries', 0), ('num_ooms', 0), ('reserved_bytes.all.allocated', 0), ('reserved_bytes.all.current', 0), ('reserved_bytes.all.freed', 0), ('reserved_bytes.all.peak', 0), ('reserved_bytes.large_pool.allocated', 0), ('reserved_bytes.large_pool.current', 0), ('reserved_bytes.large_pool.freed', 0), ('reserved_bytes.large_pool.peak', 0), ('reserved_bytes.small_pool.allocated', 0), ('reserved_bytes.small_pool.current', 0), ('reserved_bytes.small_pool.freed', 0), ('reserved_bytes.small_pool.peak', 0), ('segment.all.allocated', 0), ('segment.all.current', 0), ('segment.all.freed', 0), ('segment.all.peak', 0), ('segment.large_pool.allocated', 0), ('segment.large_pool.current', 0), ('segment.large_pool.freed', 0), ('segment.large_pool.peak', 0), ('segment.small_pool.allocated', 0), ('segment.small_pool.current', 0), ('segment.small_pool.freed', 0), ('segment.small_pool.peak', 0)])\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "import torch.cuda as cutorch\n",
    "\n",
    "print(torch.cuda.memory_stats())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sQ85GZ5a7BS-",
    "outputId": "5e39c8e3-f523-41ed-f0fd-ee62be78ec2f",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "we created an optimizer\n",
      "Saving checkpoint at results/model_step_last.pt\n"
     ]
    }
   ],
   "source": [
    "# python train.py -mode train -encoder classifier -dropout 0.1 \n",
    "# -bert_data_path ../bert_data/cnndm -model_path ../models/bert_classifier -lr 2e-3\n",
    "# -visible_gpus 0,1,2  -gpu_ranks 0,1,2 -world_size 3 -report_every 50 -save_checkpoint_steps 1000 \n",
    "# -batch_size 3000 -decay_method noam -train_steps 50000 -accum_count 2 -log_file ../logs/bert_classifier -use_interval true -warmup_steps 10000\n",
    "\n",
    "# torch.random.manual_seed(42)\n",
    "# t = TransformerInterEncoder2(4,2,2,.1,1)\n",
    "# a=torch.tensor([[[1.0,2,3,1],[1.0,2,3,1]]])\n",
    "# d=torch.tensor([[True]])\n",
    "# t(a,d)\n",
    "\n",
    "\n",
    "argDict = {\n",
    "    \"temp_dir\": \"tmp\",\n",
    "    \"ff_size\": 2,\n",
    "    \"heads\": 1,\n",
    "    \"dropout\": 0.1,\n",
    "    \"inter_layers\": 1,\n",
    "    \"param_init\": 0,\n",
    "    \"param_init_glorot\": False,\n",
    "    \"train_from\": \"\",\n",
    "    \"train_steps\":1, \n",
    "    \"use_interval\": True, \n",
    "    \"batch_size\": 2500,\n",
    "    \"lr\":1,\n",
    "    \"beta1\":.9,\n",
    "    \"beta2\":.999,\n",
    "    \"max_grad_norm\": 0,\n",
    "    \"visible_gpus\": -1, \n",
    "    \"warmup_steps\": 10,\n",
    "    \"decay_method\":'',\n",
    "    \"optim\": 'adam'\n",
    "}\n",
    "device = \"cpu\" \n",
    "savePath = \"results/\"\n",
    "model = Summarizer(argDict, device, load_pretrained_bert=True)\n",
    "blah = train(argDict, model,savePath)\n",
    "# optim = build_optim(argDict, model, None)\n",
    "# config = BertConfig.from_json_file(\"testBert.json\")\n",
    "# summ = Summarizer(argDict, device, bert_config=config)\n",
    "# model = Summarizer(argDict, device, load_pretrained_bert=True)\n",
    "\n",
    "\n",
    "# train(model, optim, trainSteps, trainIterFunc, validIterFunc, savePath)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Input Analysis - From Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "RMXFIu5DXb4v"
   },
   "outputs": [],
   "source": [
    "src=torch.tensor([[  101, 23920,  9530,  2386,  1005,  1055,  2684, 11113, 11283,  1010,\n",
    "          2260,  1010,  2001,  2741,  4424,  5084,  7696,  2011,  2019,  3784,\n",
    "          2566, 16874,  2006,  9130,   102,   101,  2048, 10756,  2031,  7549,\n",
    "          2610,  2005,  7989,  2000,  2552,  2044,  2037,  5727,  2020,  9416,\n",
    "          2011,  2019,  3784, 15267,  1012,   102,   101,  2119, 23920,  9530,\n",
    "          2386,  1998, 11260,  2665,  1005,  1055,  2402,  5727,  2020,  2741,\n",
    "          4424,  5084,  7696,  2011,  2019,  3784,  2566, 16874,  2006,  9130,\n",
    "          1011,  2021,  2610,  4188,  2000,  8556,  2004,  2053, 15226,  2018,\n",
    "          2042,  5462,  1012,   102,   101,  5796,  9530,  2386,  1005,  1055,\n",
    "          2684, 11113, 11283,  1010,  2260,  1010,  2001,  2741,  1037,  4471,\n",
    "          2011,  1996,  2158,  2044, 10564,  1037,  2767,  5227,  2013,  2619,\n",
    "         12097,  2000,  2022,  1037,  2459,  1011,  2095,  1011,  2214,  2879,\n",
    "          1012,   102,   101,  2044,  1996, 14603,  2388,  2387,  1996, 26422,\n",
    "          7696,  1010,  2016,  8315,  1996,  2158,  2841,  1010, 12097,  2000,\n",
    "          2022, 11113, 11283,  1012,   102,   101,  5796,  9530,  2386,  1010,\n",
    "          4090,  1010,  2013,  6738,  1010,  2056,  1024,  1036,  2045,  2020,\n",
    "          2176,  2030,  2274,  7696,  2008,  2002,  2741,  1010,  2077,  2002,\n",
    "          2941,  8534,  2033,  1012,   102,   101,  1045,  2228,  1045,  6015,\n",
    "          2032,  1012,   102,   101,  1036,  2017,  2064,  2069,  2424,  2026,\n",
    "          2684,  2006,  9130,  2065,  2017,  2031,  1037,  8203,  2767,  1998,\n",
    "          2023,  3124,  2001,  2814,  2007,  2531,  1997,  2014,  2082,  2814,\n",
    "          1010,  2061,  2016,  2245,  2002,  2001,  2013,  2082,  1012,  1005,\n",
    "           102,   101,  2044, 26997,  2989,  1996,  2566, 16874,  1010, 23920,\n",
    "          6866,  1996,  7696,  2006,  2014,  2219,  4070,  2000, 11582,  2060,\n",
    "          3008,  1997,  1996,  4795, 15267,  1012,   102,   101,  2016,  2056,\n",
    "          1024,  1005,  1045,  4207,  2009,  2138,  1045,  2052,  2360,  3938,\n",
    "          2566,  9358,  1997,  2026,  2684,  1005,  1055,  2082,  2020,  2006,\n",
    "          2010,  6337,  1998,  2002,  2001,  3331,  2000,  2014,  2814,  1998,\n",
    "          2010,  6337,  2018,  2055,  3998,  1011,  4278,  2814,  1998,  2027,\n",
    "          2020,  2035,  2104,  2321,  1012,  1005,   102,   101,  1999,  1996,\n",
    "          2553,  1997,  5796,  2665,  1005,  1055,  2684,  6253,  1010,  2410,\n",
    "          1010,  1996,  5305,  2158,  5561,  2000,  2695, 13216,  4620,  2006,\n",
    "          2014,  9130,  2813,  1011,  2029,  2035,  1997,  2014,  2814,  2052,\n",
    "          2031,  2059,  2464,  1011,  2065,  2016,  2106,  2025,  2377,  2247,\n",
    "          2007,  2010,  2061, 17080,  2094,  7696,  1012,   102,   101,  1996,\n",
    "          4620,  2020,  2025,  1997,  6253,  1010,  2021,  2009,  2003,  2245,\n",
    "          2008,  2002,  3740,  2000,  2695,  2068,  3784,  2004,  2065,  2027,\n",
    "          2020,  2014,  1012,   102,   101,  5796,  2665,  1010,  2013,  2167,\n",
    "          2047, 10264,  2094,  1010,  2264,  7018,  1010,  2056,  1024,  1036,\n",
    "          2043,  1045,  2034,  2387,  1996,  7696,  1045,  9357,  2041,  1010,\n",
    "          2021,  1045,  2106,  1050,  1005,  1056,  2000,  2377,  1996,  2535,\n",
    "          1997,  1037,  6643, 26010, 24862,  4477,  1998,  2599,  2032,  2006,\n",
    "          2138,  1045,  2106,  1050,  1005,  1056,  2215,  2032,  2000,  2228,\n",
    "          2008,  2026,  2684,  2001,  4699,  1999,  2032,  1999,  4312,  2012,\n",
    "          2035,  1012,  1005,   102,   101,  5796,  9530,  2386,  8315,  1996,\n",
    "          2158,  1006, 15885,  2187,  1998,  3015,  1999,  4462,  1007,  2043,\n",
    "          2016,  2179,  1996,  7696,  2006, 11113, 11283,  1005,  1055,  9130,\n",
    "          1006, 11113, 11283,  1005,  1055,  7696,  1999,  2630,  1007,   102,\n",
    "           101,  2044,  1996, 14603,  2388,  2387,  1996, 26422,  7696,  1010,\n",
    "          2016,  8315,  1006,  1999,  2630,  1007,  1996,  2158,  1006,  3015,\n",
    "          1999,  4462,  1007,  2841,  1010, 12097,  2000,  2022, 11113, 11283,\n",
    "           102,   102]], device='cuda:0')\n",
    "label=torch.tensor([[0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], device='cuda:0')\n",
    "seg=torch.tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
    "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
    "         1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
    "         1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
    "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
    "         1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
    "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
    "         1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
    "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
    "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 1]], device='cuda:0')\n",
    "cls=torch.tensor([[  0,  25,  46,  84, 122, 145, 175, 183, 221, 247, 297, 348, 374, 444,\n",
    "         480]], device='cuda:0')\n",
    "mask=torch.tensor([[True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "         True, True, True, True, True, True, True, True]], device='cuda:0')\n",
    "mask_cls=torch.tensor([[True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "         True, True, True]], device='cuda:0')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pytorch_pretrained_bert\n",
    "from pytorch_pretrained_bert import BertTokenizer, BertModel, BertForMaskedLM\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "def runModel(model, inputText, device):\n",
    "  tokenized_text = tokenizer.tokenize(inputText)\n",
    "  indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "  condition = [i == 101 for i in indexed_tokens]\n",
    "  cls = torch.tensor(np.arange(len(indexed_tokens))[condition])\n",
    "  segs = [0 for _ in range(len(indexed_tokens))]\n",
    "  zero = True\n",
    "  for i in range(1,len(cls)):\n",
    "    begin, end = cls[i-1],cls[i]\n",
    "    segs[begin:end] = 0 if zero else 1\n",
    "    zero=not zero\n",
    "  mask = torch.tensor([[True for _ in indexed_tokens]]).to(device)\n",
    "  mask_cls = torch.tensor([[True for _ in cls]]).to(device)\n",
    "  outputText = model(indexed_tokens, segs, cls, mask, mask_cls)\n",
    "  return tokenizer.convert_ids_to_tokens(outputText)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "AnnotatedBertSumSummarizationLayers.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
