{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-07T22:38:42.332362Z",
     "start_time": "2020-11-07T22:38:40.712207Z"
    }
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self,embeddingDim,dropout,maxWordLen=5000):\n",
    "        positionalEncoding = torch.zeros(maxWordLen,embeddingDim)\n",
    "        # keep dim 1 array a singleton\n",
    "        position = torch.arange(0,maxWordLen).unsqueeze(1)\n",
    "        # positional encoding is defined to be \n",
    "        # PE(pos,2i)=sin(pos/1e4^(2i/embedding_dim))\n",
    "        # PE(pos,2i+1)=cos(pos/1e4^(2i/embedding_dim))\n",
    "        exponentTerm = torch.arange(0,dim,2, dtype=torch.float)\\\n",
    "                *(-math.log(1e4*1.0)/embeddingDim)\n",
    "        divisionTerm = torch.exp(exponentTerm)\n",
    "        # all even indices\n",
    "        positionalEncoding[:,0::2] = torch.sin(position.float()*divisionTerm)\n",
    "        # all odd indices\n",
    "        positionalEncoding[:,1::2] = torch.cos(position.float()*divisionTerm)\n",
    "        # keep dim 0 array size 1 --> a single array\n",
    "        positionalEncoding = positionalEncoding.unsqueeze(0)\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.register_buffer('positionalEncoding', positionalEncoding)\n",
    "        #dropout is probability of dropout\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        self.embeddingDim = embeddingDim\n",
    "        \n",
    "    def forward(self,embedding):\n",
    "        # optional: add step -- not sure what it does\n",
    "        # for dropout?\n",
    "        embedding = embedding * math.sqrt(self.embeddingDim)\n",
    "        embedding = embedding + self.positionalEncoding[:,:embeddingDim.size(1)]\n",
    "        embedding = self.dropout(embedding)\n",
    "        return embedding\n",
    "    def getPositonalEncoding(self,embedding):\n",
    "        return self.positionalEncoding[:,:embedding.size(1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-07T22:56:20.592419Z",
     "start_time": "2020-11-07T22:56:20.563764Z"
    }
   },
   "outputs": [],
   "source": [
    "class TransformerEncoderLayer(nn.Module):\n",
    "    def __init__(self, numHeads, modelDim, feedForwardDim, dropout):\n",
    "        super(TransformerEncoderLayer, self).__init__()\n",
    "        self.selfAttention = MultiHeadedAttention(numHeads,modelDim,dropout=dropout)\n",
    "        self.feedForward = PositionwiseFeedForward(modelDim,feedForwardDim,dropout)\n",
    "        # output = (input - Expectation[input])/sqrt(variance(input)+eps)*gamma+beta\n",
    "        # gamma and beta learnable \n",
    "        # normalizing over the last dim\n",
    "        self.layerNorm = nn.LayerNorm(modelDim,eps=1e-6)\n",
    "        #dropout is probability of dropout\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self,iteration,query,inputs,mask):\n",
    "        # if it is not the first iteration, then normalize the layer \n",
    "        inputsNorm = self.layerNorm(inputs) if iteration != 0 else inputs \n",
    "        # keep dim 1 a singleton\n",
    "        mask=mask.unsqueeze(1)\n",
    "        # why?\n",
    "        contextEncoding = self.selfAttention(inputsNorm,inputsNorm,inputsNorm,mask=mask)\n",
    "        # why do we add input back?\n",
    "        contextEncodingWDropout = self.dropout(contextEncoding) + inputs \n",
    "        return self.feedForward(contextEncodingWDropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-08T03:26:30.544088Z",
     "start_time": "2020-11-08T03:26:30.514518Z"
    }
   },
   "outputs": [],
   "source": [
    "#inter-sentence transformer focuses on learning relationship between sentences to produce a document level summary\n",
    "# the input to this transformer is output of Bert, which can be viewed as contextual encoding. \n",
    "# modelDim is the output of the Bert's hidden size, or Bert's transformer's model dimension size \n",
    "# (model dimension size is the word used in the self hidden is all you need)\n",
    "class InterSentencesTransformerEncoderLayer(nn.Module):\n",
    "    def __init__(self,numHeads, modelDim, feedforwardDim, dropout,numInterSentencesLayers=0):\n",
    "        super(InterSentencesTransformerEncoderLayer, self).__init__()\n",
    "        self.modelDim = modelDim\n",
    "        self.numInterSentencesLayers =  numInterSentencesLayers\n",
    "        self.positionalEmbedding = PositionalEncoding(dimModel,dropout)\n",
    "        self.interSentencesTransformers = nn.ModuleList(\n",
    "            [TransformerEncoderLayer(numHeads, modelDim, feedForwardDim, dropout)\n",
    "             for _ in range(numTransformers)])\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.layerNorm = nn.LayerNorm(modelDim, eps=1e-6)\n",
    "        self.linearLayer = nn.Linear(modelDim, 1, bias=True)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    # top_vector in the original code: top vectors? topic vectors?\n",
    "    # topicVectors is the output of Bert, some sort of contextual embedding\n",
    "    # we will use contextualEncoding instead of top_vector\n",
    "    def forward(self,contextualEncoding,mask):\n",
    "        batchSize, nSentences = contextualEncoding.size(0), contextualEncoding.size(1)\n",
    "        positionalEmbedding = self.positionalEmbedding.positionalEncoding[:,:nSentences]\n",
    "        # mask takes [:,:,None] to account for batches?\n",
    "        # x will be the contextualEncoding undergoing transformer operations\n",
    "        x = contextualEncoding * mask[:,:,None].float() + positionalEmbedding\n",
    "        for iteration in range(self.numInterSentencesLayers):\n",
    "            x = self.interSentencesTransformers[i](i,x,x,1-mask)\n",
    "        x = self.layerNorm(x)\n",
    "        sentencesScores = self.sigmoid(self.linearLayer(x))\n",
    "        sentencesScores = sentencesScores.squeeze(-1)*mask.float()\n",
    "        return sentencesScores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-08T04:35:52.193845Z",
     "start_time": "2020-11-08T04:35:52.141262Z"
    }
   },
   "outputs": [],
   "source": [
    "class MultiHeadedAttention(nn.Module):\n",
    "    def __init__(self,numHeads, modelDim, dropout=0.1, isFinalLinear=True):\n",
    "        assert modelDim%numHeads == 0\n",
    "        self.dimPerHead = model.dim//numHeads \n",
    "        self.modelDim = modelDim\n",
    "        \n",
    "        super(MultiHeadedAttention,self).__init__()\n",
    "        self.numHeads = numHeads\n",
    "        self.linearKeys = nn.Linear(modelDim,numHeads*self.dimPerHead)\n",
    "        self.linearValues = nn.Linear(modelDim,numHeads*self.dimPerHead)\n",
    "        self.linearQuery = nn.Linear(modelDim,numHeads*self.dimPerHead)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.isFinalLinear = isFinalLinear\n",
    "        if self.isFinalLinear:\n",
    "            self.finalLayer = nn.Linear(modelDim,modelDim)\n",
    "    def forward(self,key,value,query,mask = None,\n",
    "               layerCache = None, types = None):\n",
    "        batchSize = key.size(0) # size(0) is batch size for all three vectors\n",
    "        dimPerHead = self.dimPerHead\n",
    "        numHeads = self.numHeads\n",
    "\n",
    "        \n",
    "        shape = lambda x: x.view(batchSize,-1,numHeads,dimPerHead).transpose(1,2)\n",
    "        # why contiguous: https://discuss.pytorch.org/t/when-and-why-do-we-use-contiguous/47588\n",
    "        # we might not need it\n",
    "        # apparently in the old version, transpose only changes the view of data, but not data itself\n",
    "        # so to force it to change the data, use contiguous \n",
    "        unshape = lambda x: x.transpose(1,2).contiguous().view(batchSize,-1,numHeads*dimPerHead)\n",
    "        \n",
    "        # get key, value, and query\n",
    "        if layerCache is None:\n",
    "            key = self.linearKeys(key)\n",
    "            value = self.linearValues(value)\n",
    "            query = self.linearQuery(query)\n",
    "            key = shape(key)\n",
    "            value = shape(value)\n",
    "            \n",
    "        else:\n",
    "            # Note: this is different from the original code. the original code has \n",
    "            # if statement that is already tested, and else statements that will \n",
    "            # never get use\n",
    "            \n",
    "            # concatenate to variable key\" and \"value\" to their respective caches. \n",
    "            if type == \"self\":\n",
    "                key = self.linearKeys(key)\n",
    "                value = self.linearValues(value)\n",
    "                query = self.linearQuery(query)\n",
    "                key = shape(key)\n",
    "                value = shape(value)\n",
    "                \n",
    "                device = key.device\n",
    "                \n",
    "                itemPairsToUpdate = [(key,\"selfKeys\"),(value,\"selfValues\")]\n",
    "                \n",
    "                \n",
    "                for variable, variableName in itemPairsToUpdate:\n",
    "                    if layerCache[variableName] is not None:\n",
    "                        variable = torch.cat((layer_cache[variableName].to(device),variable), dim=2)\n",
    "                    layerCache[variableName] = variable\n",
    "                \n",
    "            elif type == \"context\":\n",
    "                # if no cache, create the cache, \n",
    "                # else copy the cache to the variables \n",
    "                query = self.linearQuery(query)\n",
    "                if layerCache[\"memoryKeys\"] is None:\n",
    "                    key = self.linearKeys(key)\n",
    "                    value = self.linearValues(value)\n",
    "                    key = shape(key)\n",
    "                    value = shape(value)\n",
    "                    layerCache[\"memoryKeys\"] = key\n",
    "                    layerCache[\"memoryValues\"] = value\n",
    "                else:\n",
    "                    key, value = layerCache[\"memoryKeys\"], layerCache[\"memoryValues\"]\n",
    "        query = shape(query)\n",
    "        \n",
    "        '''\n",
    "        # possibly for debugging purpose\n",
    "        keyLength = key.size(2)\n",
    "        queryLength = query.size(2)\n",
    "        '''\n",
    "        \n",
    "        # compute and scale the scores\n",
    "        \n",
    "        # why sqrt?\n",
    "        query = query / math.sqrt(dimPerHead)\n",
    "        scores = torch.matmul(query,key.transpose(2,3))\n",
    "        \n",
    "        if mask is not None:\n",
    "            mask = mask.unsqueeze(1).expand_as(scores)\n",
    "            scores = scores.masked_fill(mask,-1e18) # negative infinity \n",
    "            \n",
    "        # apply attention dropout and compute context vectors \n",
    "        attention = self.softmax(scores)\n",
    "        attentionDropout = self.dropout(attention)\n",
    "        if self.isFinalLinear:\n",
    "            context = unshape(torch.matmult(attentionDropout,value))\n",
    "            output = self.finalLayer(context)\n",
    "            return output\n",
    "        else:\n",
    "            context = torch.matmul(attentionDropout,value)\n",
    "            return context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-08T04:43:36.553135Z",
     "start_time": "2020-11-08T04:43:36.527706Z"
    }
   },
   "outputs": [],
   "source": [
    "class PositionwiseFeedForward(nn.Module):\n",
    "    '''\n",
    "    A two-layer Feed-Forward-Network with residual layer norm.\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
    "        super(PositionwiseFeedForward, self).__init__()\n",
    "        self.linear1 = nn.Linear(modelDim, feedforwardDim)\n",
    "        self.linear2 = nn.Linear(feedforwardDim, modelDim)\n",
    "        self.layerNorm = nn.LayerNorm(modelDim, eps=1e-6)\n",
    "        # activation function\n",
    "        self.gelu = lambda x: \\\n",
    "                0.5 * x * (1 + torch.tanh(math.sqrt(2 / math.pi) * (x + 0.044715 * torch.pow(x, 3))))\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        hidden = self.dropout1(self.gelu(self.linear1(self.layerNorm(x))))\n",
    "        output = self.dropout2(self.linear12(hidden))\n",
    "        return output + x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
