{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import os\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "import time\n",
    "import distributed\n",
    "\n",
    "import gc\n",
    "import glob\n",
    "import hashlib\n",
    "import itertools\n",
    "import json\n",
    "import re\n",
    "import subprocess\n",
    "import time\n",
    "from os.path import join as pjoin\n",
    "\n",
    "from multiprocess import Pool\n",
    "from pytorch_pretrained_bert import BertTokenizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-07T22:38:42.332362Z",
     "start_time": "2020-11-07T22:38:40.712207Z"
    }
   },
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self,embeddingDim,dropout,maxWordLen=5000):\n",
    "        positionalEncoding = torch.zeros(maxWordLen,embeddingDim)\n",
    "        # keep dim 1 array a singleton\n",
    "        position = torch.arange(0,maxWordLen).unsqueeze(1)\n",
    "        # positional encoding is defined to be \n",
    "        # PE(pos,2i)=sin(pos/1e4^(2i/embedding_dim))\n",
    "        # PE(pos,2i+1)=cos(pos/1e4^(2i/embedding_dim))\n",
    "        exponentTerm = torch.arange(0,dim,2, dtype=torch.float)\\\n",
    "                *(-math.log(1e4*1.0)/embeddingDim)\n",
    "        divisionTerm = torch.exp(exponentTerm)\n",
    "        # all even indices\n",
    "        positionalEncoding[:,0::2] = torch.sin(position.float()*divisionTerm)\n",
    "        # all odd indices\n",
    "        positionalEncoding[:,1::2] = torch.cos(position.float()*divisionTerm)\n",
    "        # keep dim 0 array size 1 --> a single array\n",
    "        positionalEncoding = positionalEncoding.unsqueeze(0)\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.register_buffer('positionalEncoding', positionalEncoding)\n",
    "        #dropout is probability of dropout\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        self.embeddingDim = embeddingDim\n",
    "        \n",
    "    def forward(self,embedding):\n",
    "        # optional: add step -- not sure what it does\n",
    "        # for dropout?\n",
    "        embedding = embedding * math.sqrt(self.embeddingDim)\n",
    "        embedding = embedding + self.positionalEncoding[:,:embeddingDim.size(1)]\n",
    "        embedding = self.dropout(embedding)\n",
    "        return embedding\n",
    "    def getPositonalEncoding(self,embedding):\n",
    "        return self.positionalEncoding[:,:embedding.size(1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-07T22:56:20.592419Z",
     "start_time": "2020-11-07T22:56:20.563764Z"
    }
   },
   "outputs": [],
   "source": [
    "class TransformerEncoderLayer(nn.Module):\n",
    "    def __init__(self, numHeads, modelDim, feedForwardDim, dropout):\n",
    "        super(TransformerEncoderLayer, self).__init__()\n",
    "        self.selfAttention = MultiHeadedAttention(numHeads,modelDim,dropout=dropout)\n",
    "        self.feedForward = PositionwiseFeedForward(modelDim,feedForwardDim,dropout)\n",
    "        # output = (input - Expectation[input])/sqrt(variance(input)+eps)*gamma+beta\n",
    "        # gamma and beta learnable \n",
    "        # normalizing over the last dim\n",
    "        self.layerNorm = nn.LayerNorm(modelDim,eps=1e-6)\n",
    "        #dropout is probability of dropout\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self,iteration,query,inputs,mask):\n",
    "        # if it is not the first iteration, then normalize the layer \n",
    "        inputsNorm = self.layerNorm(inputs) if iteration != 0 else inputs \n",
    "        # keep dim 1 a singleton\n",
    "        mask=mask.unsqueeze(1)\n",
    "        # why?\n",
    "        contextEncoding = self.selfAttention(inputsNorm,inputsNorm,inputsNorm,mask=mask)\n",
    "        # why do we add input back?\n",
    "        contextEncodingWDropout = self.dropout(contextEncoding) + inputs \n",
    "        return self.feedForward(contextEncodingWDropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-08T03:26:30.544088Z",
     "start_time": "2020-11-08T03:26:30.514518Z"
    }
   },
   "outputs": [],
   "source": [
    "#inter-sentence transformer focuses on learning relationship between sentences to produce a document level summary\n",
    "# the input to this transformer is output of Bert, which can be viewed as contextual encoding. \n",
    "# modelDim is the output of the Bert's hidden size, or Bert's transformer's model dimension size \n",
    "# (model dimension size is the word used in the self hidden is all you need)\n",
    "class InterSentencesTransformerEncoderLayer(nn.Module):\n",
    "    def __init__(self,numHeads, modelDim, feedforwardDim, dropout,numInterSentencesLayers=0):\n",
    "        super(InterSentencesTransformerEncoderLayer, self).__init__()\n",
    "        self.modelDim = modelDim\n",
    "        self.numInterSentencesLayers =  numInterSentencesLayers\n",
    "        self.positionalEmbedding = PositionalEncoding(dimModel,dropout)\n",
    "        self.interSentencesTransformers = nn.ModuleList(\n",
    "            [TransformerEncoderLayer(numHeads, modelDim, feedForwardDim, dropout)\n",
    "             for _ in range(numTransformers)])\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.layerNorm = nn.LayerNorm(modelDim, eps=1e-6)\n",
    "        self.linearLayer = nn.Linear(modelDim, 1, bias=True)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    # top_vector in the original code: top vectors? topic vectors?\n",
    "    # topicVectors is the output of Bert, some sort of contextual embedding\n",
    "    # we will use contextualEncoding instead of top_vector\n",
    "    def forward(self,contextualEncoding,mask):\n",
    "        batchSize, nSentences = contextualEncoding.size(0), contextualEncoding.size(1)\n",
    "        positionalEmbedding = self.positionalEmbedding.positionalEncoding[:,:nSentences]\n",
    "        # mask takes [:,:,None] to account for batches?\n",
    "        # x will be the contextualEncoding undergoing transformer operations\n",
    "        x = contextualEncoding * mask[:,:,None].float() + positionalEmbedding\n",
    "        for iteration in range(self.numInterSentencesLayers):\n",
    "            x = self.interSentencesTransformers[i](i,x,x,~mask)\n",
    "        x = self.layerNorm(x)\n",
    "        sentencesScores = self.sigmoid(self.linearLayer(x))\n",
    "        sentencesScores = sentencesScores.squeeze(-1)*mask.float()\n",
    "        return sentencesScores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-08T04:35:52.193845Z",
     "start_time": "2020-11-08T04:35:52.141262Z"
    }
   },
   "outputs": [],
   "source": [
    "class MultiHeadedAttention(nn.Module):\n",
    "    def __init__(self,numHeads, modelDim, dropout=0.1, isFinalLinear=True):\n",
    "        assert modelDim%numHeads == 0\n",
    "        self.dimPerHead = model.dim//numHeads \n",
    "        self.modelDim = modelDim\n",
    "        \n",
    "        super(MultiHeadedAttention,self).__init__()\n",
    "        self.numHeads = numHeads\n",
    "        self.linearKeys = nn.Linear(modelDim,numHeads*self.dimPerHead)\n",
    "        self.linearValues = nn.Linear(modelDim,numHeads*self.dimPerHead)\n",
    "        self.linearQuery = nn.Linear(modelDim,numHeads*self.dimPerHead)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.isFinalLinear = isFinalLinear\n",
    "        if self.isFinalLinear:\n",
    "            self.finalLayer = nn.Linear(modelDim,modelDim)\n",
    "    def forward(self,key,value,query,mask = None,\n",
    "               layerCache = None, types = None):\n",
    "        batchSize = key.size(0) # size(0) is batch size for all three vectors\n",
    "        dimPerHead = self.dimPerHead\n",
    "        numHeads = self.numHeads\n",
    "\n",
    "        \n",
    "        shape = lambda x: x.view(batchSize,-1,numHeads,dimPerHead).transpose(1,2)\n",
    "        # why contiguous: https://discuss.pytorch.org/t/when-and-why-do-we-use-contiguous/47588\n",
    "        # we might not need it\n",
    "        # apparently in the old version, transpose only changes the view of data, but not data itself\n",
    "        # so to force it to change the data, use contiguous \n",
    "        unshape = lambda x: x.transpose(1,2).contiguous().view(batchSize,-1,numHeads*dimPerHead)\n",
    "        \n",
    "        # get key, value, and query\n",
    "        if layerCache is None:\n",
    "            key = self.linearKeys(key)\n",
    "            value = self.linearValues(value)\n",
    "            query = self.linearQuery(query)\n",
    "            key = shape(key)\n",
    "            value = shape(value)\n",
    "            \n",
    "        else:\n",
    "            # Note: this is different from the original code. the original code has \n",
    "            # if statement that is already tested, and else statements that will \n",
    "            # never get use\n",
    "            \n",
    "            # concatenate to variable key\" and \"value\" to their respective caches. \n",
    "            if type == \"self\":\n",
    "                key = self.linearKeys(key)\n",
    "                value = self.linearValues(value)\n",
    "                query = self.linearQuery(query)\n",
    "                key = shape(key)\n",
    "                value = shape(value)\n",
    "                \n",
    "                device = key.device\n",
    "                \n",
    "                itemPairsToUpdate = [(key,\"selfKeys\"),(value,\"selfValues\")]\n",
    "                \n",
    "                \n",
    "                for variable, variableName in itemPairsToUpdate:\n",
    "                    if layerCache[variableName] is not None:\n",
    "                        variable = torch.cat((layer_cache[variableName].to(device),variable), dim=2)\n",
    "                    layerCache[variableName] = variable\n",
    "                \n",
    "            elif type == \"context\":\n",
    "                # if no cache, create the cache, \n",
    "                # else copy the cache to the variables \n",
    "                query = self.linearQuery(query)\n",
    "                if layerCache[\"memoryKeys\"] is None:\n",
    "                    key = self.linearKeys(key)\n",
    "                    value = self.linearValues(value)\n",
    "                    key = shape(key)\n",
    "                    value = shape(value)\n",
    "                    layerCache[\"memoryKeys\"] = key\n",
    "                    layerCache[\"memoryValues\"] = value\n",
    "                else:\n",
    "                    key, value = layerCache[\"memoryKeys\"], layerCache[\"memoryValues\"]\n",
    "        query = shape(query)\n",
    "        \n",
    "        '''\n",
    "        # possibly for debugging purpose\n",
    "        keyLength = key.size(2)\n",
    "        queryLength = query.size(2)\n",
    "        '''\n",
    "        \n",
    "        # compute and scale the scores\n",
    "        \n",
    "        # why sqrt?\n",
    "        query = query / math.sqrt(dimPerHead)\n",
    "        scores = torch.matmul(query,key.transpose(2,3))\n",
    "        \n",
    "        if mask is not None:\n",
    "            mask = mask.unsqueeze(1).expand_as(scores)\n",
    "            scores = scores.masked_fill(mask,-1e18) # negative infinity \n",
    "            \n",
    "        # apply attention dropout and compute context vectors \n",
    "        attention = self.softmax(scores)\n",
    "        attentionDropout = self.dropout(attention)\n",
    "        if self.isFinalLinear:\n",
    "            context = unshape(torch.matmult(attentionDropout,value))\n",
    "            output = self.finalLayer(context)\n",
    "            return output\n",
    "        else:\n",
    "            context = torch.matmul(attentionDropout,value)\n",
    "            return context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-08T04:43:36.553135Z",
     "start_time": "2020-11-08T04:43:36.527706Z"
    }
   },
   "outputs": [],
   "source": [
    "class PositionwiseFeedForward(nn.Module):\n",
    "    '''\n",
    "    A two-layer Feed-Forward-Network with residual layer norm.\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
    "        super(PositionwiseFeedForward, self).__init__()\n",
    "        self.linear1 = nn.Linear(modelDim, feedforwardDim)\n",
    "        self.linear2 = nn.Linear(feedforwardDim, modelDim)\n",
    "        self.layerNorm = nn.LayerNorm(modelDim, eps=1e-6)\n",
    "        # activation function\n",
    "        self.gelu = lambda x: \\\n",
    "                0.5 * x * (1 + torch.tanh(math.sqrt(2 / math.pi) * (x + 0.044715 * torch.pow(x, 3))))\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        hidden = self.dropout1(self.gelu(self.linear1(self.layerNorm(x))))\n",
    "        output = self.dropout2(self.linear12(hidden))\n",
    "        return output + x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = torch.nn.BCELoss(reduction='none')\n",
    "\n",
    "def train(model, optim, trainSteps, trainIterFunc, validIterFunc, savePath):\n",
    "    '''\n",
    "    optim : optimizer\n",
    "    savePath: where the model of the last will be saved.\n",
    "    '''\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(device)\n",
    "    n_gpu = torch.cuda.current_device()\n",
    "    \n",
    "    for step in range(optim._step + 1, trainSteps):\n",
    "        trainIter = trainIterFunc()\n",
    "        optim.zero_grad()\n",
    "        trainLoss, trainBatchSize = 0, 0\n",
    "        validLoss, validBatchSize = 0, 0\n",
    "        timeStart = time.time()\n",
    "        \n",
    "        # training \n",
    "        for i, batch in enumerate(trainIter):\n",
    "            src = batch.src\n",
    "            labels = batch.labels\n",
    "            segs = batch.segs\n",
    "            cls = batch.clss\n",
    "            mask = batch.mask\n",
    "            mask_cls = batch.mask_cls\n",
    "                \n",
    "            sentenceScores, mask = model(src, seg, cls, mask, mask_cls)\n",
    "            lossVec = loss(sentenceScores, labels.float())\n",
    "            lossMag = (lossVec*mask.float()).sum()\n",
    "            (lossMag/lossMag.numel()).backward()\n",
    "            optim.step()\n",
    "            lossMag.detach()\n",
    "            \n",
    "            trainLoss += lossMag\n",
    "            trainBatchSize += batch.batch_size\n",
    "        \n",
    "        timeEnded_T = time.time()\n",
    "        # validating \n",
    "        self.model.eval()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            validIter = validIterFunc() \n",
    "            for batch in validIter:\n",
    "                src = batch.src\n",
    "                labels = batch.labels\n",
    "                segs = batch.segs\n",
    "                cls = batch.clss\n",
    "                mask = batch.mask\n",
    "                mask_cls = batch.mask_cls\n",
    "                \n",
    "                sentenceScores, mask = model(src, seg, cls, mask, mask_cls)\n",
    "                lossVec = loss(sentenceScores, labels.float())\n",
    "                lossMag = (lossVec*mask.float()).sum()\n",
    "                validLoss += lossMag\n",
    "                validBatchSize += batch.batch_size\n",
    "        \n",
    "        timeEnded_V = time.time()\n",
    "        trainTime = timeStart   - timeEnded_T\n",
    "        validTime = timeEnded_T - timeEnded_V\n",
    "        trainLoss /= trainBatchSize\n",
    "        validLoss /= validBatchSize\n",
    "        print(\"Step %s; lr: %7.7f; training loss: %4.2f;\" +\n",
    "             \" trained %6.0f sec; valid loss: %4.2f; validated %6.0f sec\", step, \n",
    "              step, optim.learning_rate, trainLoss, trainTime, validLoss, \n",
    "             validTime)\n",
    "    # saving the model \n",
    "    modelStateDict  = model.state_dict()\n",
    "    # don't have args, check if it's okay when loaded\n",
    "    checkpoint = {\n",
    "        'model' : modelStateDict,\n",
    "        'optim' : optim\n",
    "    }\n",
    "    \n",
    "    checkpointPath = os.path.join(savePath, 'model_step_last.pt')\n",
    "    print(\"Saving checkpoint at\", checkpointPath)\n",
    "    torch.save(checkpoint, checkpointPath)\n",
    "    return checkpoint, checkpointPath\n",
    "    \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Builder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_rouge(evaluated_ngrams, reference_ngrams):\n",
    "    reference_count = len(reference_ngrams)\n",
    "    evaluated_count = len(evaluated_ngrams)\n",
    "\n",
    "    overlapping_ngrams = evaluated_ngrams.intersection(reference_ngrams)\n",
    "    overlapping_count = len(overlapping_ngrams)\n",
    "\n",
    "    if evaluated_count == 0:\n",
    "        precision = 0.0\n",
    "    else:\n",
    "        precision = overlapping_count / evaluated_count\n",
    "\n",
    "    if reference_count == 0:\n",
    "        recall = 0.0\n",
    "    else:\n",
    "        recall = overlapping_count / reference_count\n",
    "\n",
    "    f1_score = 2.0 * ((precision * recall) / (precision + recall + 1e-8))\n",
    "    return {\"f\": f1_score, \"p\": precision, \"r\": recall}\n",
    "\n",
    "def greedy_selection(doc_sent_list, abstract_sent_list, summary_size):\n",
    "    def _rouge_clean(s):\n",
    "        return re.sub(r'[^a-zA-Z0-9 ]', '', s)\n",
    "\n",
    "    max_rouge = 0.0\n",
    "    abstract = sum(abstract_sent_list, [])\n",
    "    abstract = _rouge_clean(' '.join(abstract)).split()\n",
    "    sents = [_rouge_clean(' '.join(s)).split() for s in doc_sent_list]\n",
    "    evaluated_1grams = [_get_word_ngrams(1, [sent]) for sent in sents]\n",
    "    reference_1grams = _get_word_ngrams(1, [abstract])\n",
    "    evaluated_2grams = [_get_word_ngrams(2, [sent]) for sent in sents]\n",
    "    reference_2grams = _get_word_ngrams(2, [abstract])\n",
    "\n",
    "    selected = []\n",
    "    for s in range(summary_size):\n",
    "        cur_max_rouge = max_rouge\n",
    "        cur_id = -1\n",
    "        for i in range(len(sents)):\n",
    "            if (i in selected):\n",
    "                continue\n",
    "            c = selected + [i]\n",
    "            candidates_1 = [evaluated_1grams[idx] for idx in c]\n",
    "            candidates_1 = set.union(*map(set, candidates_1))\n",
    "            candidates_2 = [evaluated_2grams[idx] for idx in c]\n",
    "            candidates_2 = set.union(*map(set, candidates_2))\n",
    "            rouge_1 = cal_rouge(candidates_1, reference_1grams)['f']\n",
    "            rouge_2 = cal_rouge(candidates_2, reference_2grams)['f']\n",
    "            rouge_score = rouge_1 + rouge_2\n",
    "            if rouge_score > cur_max_rouge:\n",
    "                cur_max_rouge = rouge_score\n",
    "                cur_id = i\n",
    "        if (cur_id == -1):\n",
    "            return selected\n",
    "        selected.append(cur_id)\n",
    "        max_rouge = cur_max_rouge\n",
    "\n",
    "    return sorted(selected)\n",
    "\n",
    "def combination_selection(doc_sent_list, abstract_sent_list, summary_size):\n",
    "    def _rouge_clean(s):\n",
    "        return re.sub(r'[^a-zA-Z0-9 ]', '', s)\n",
    "\n",
    "    max_rouge = 0.0\n",
    "    max_idx = (0, 0)\n",
    "    abstract = sum(abstract_sent_list, [])\n",
    "    abstract = _rouge_clean(' '.join(abstract)).split()\n",
    "    sents = [_rouge_clean(' '.join(s)).split() for s in doc_sent_list]\n",
    "    evaluated_1grams = [_get_word_ngrams(1, [sent]) for sent in sents]\n",
    "    reference_1grams = _get_word_ngrams(1, [abstract])\n",
    "    evaluated_2grams = [_get_word_ngrams(2, [sent]) for sent in sents]\n",
    "    reference_2grams = _get_word_ngrams(2, [abstract])\n",
    "\n",
    "    impossible_sents = []\n",
    "    for s in range(summary_size + 1):\n",
    "        combinations = itertools.combinations([i for i in range(len(sents)) if i not in impossible_sents], s + 1)\n",
    "        for c in combinations:\n",
    "            candidates_1 = [evaluated_1grams[idx] for idx in c]\n",
    "            candidates_1 = set.union(*map(set, candidates_1))\n",
    "            candidates_2 = [evaluated_2grams[idx] for idx in c]\n",
    "            candidates_2 = set.union(*map(set, candidates_2))\n",
    "            rouge_1 = cal_rouge(candidates_1, reference_1grams)['f']\n",
    "            rouge_2 = cal_rouge(candidates_2, reference_2grams)['f']\n",
    "\n",
    "            rouge_score = rouge_1 + rouge_2\n",
    "            if (s == 0 and rouge_score == 0):\n",
    "                impossible_sents.append(c[0])\n",
    "            if rouge_score > max_rouge:\n",
    "                max_idx = c\n",
    "                max_rouge = rouge_score\n",
    "    return sorted(list(max_idx))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Batch(object):\n",
    "    def _pad(self, data, pad_id, width=-1):\n",
    "        if (width == -1):\n",
    "            width = max(len(d) for d in data)\n",
    "        rtn_data = [d + [pad_id] * (width - len(d)) for d in data]\n",
    "        return rtn_data\n",
    "\n",
    "    def __init__(self, data=None, device=None,  is_test=False):\n",
    "        \"\"\"Create a Batch from a list of examples.\"\"\"\n",
    "        if data is not None:\n",
    "            self.batch_size = len(data)\n",
    "            pre_src = [x[0] for x in data]\n",
    "            pre_labels = [x[1] for x in data]\n",
    "            pre_segs = [x[2] for x in data]\n",
    "            pre_clss = [x[3] for x in data]\n",
    "\n",
    "            src = torch.tensor(self._pad(pre_src, 0))\n",
    "\n",
    "            labels = torch.tensor(self._pad(pre_labels, 0))\n",
    "            segs = torch.tensor(self._pad(pre_segs, 0))\n",
    "            mask = ~(src == 0)\n",
    "\n",
    "            clss = torch.tensor(self._pad(pre_clss, -1))\n",
    "            mask_cls = ~(clss == -1)\n",
    "            clss[clss == -1] = 0\n",
    "\n",
    "            setattr(self, 'clss', clss.to(device))\n",
    "            setattr(self, 'mask_cls', mask_cls.to(device))\n",
    "            setattr(self, 'src', src.to(device))\n",
    "            setattr(self, 'labels', labels.to(device))\n",
    "            setattr(self, 'segs', segs.to(device))\n",
    "            setattr(self, 'mask', mask.to(device))\n",
    "\n",
    "            if (is_test):\n",
    "                src_str = [x[-2] for x in data]\n",
    "                setattr(self, 'src_str', src_str)\n",
    "                tgt_str = [x[-1] for x in data]\n",
    "                setattr(self, 'tgt_str', tgt_str)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.batch_size\n",
    "\n",
    "\n",
    "def batch(data, batch_size):\n",
    "    \"\"\"Yield elements from data in chunks of batch_size.\"\"\"\n",
    "    minibatch, size_so_far = [], 0\n",
    "    for ex in data:\n",
    "        minibatch.append(ex)\n",
    "        size_so_far = simple_batch_size_fn(ex, len(minibatch))\n",
    "        if size_so_far == batch_size:\n",
    "            yield minibatch\n",
    "            minibatch, size_so_far = [], 0\n",
    "        elif size_so_far > batch_size:\n",
    "            yield minibatch[:-1]\n",
    "            minibatch, size_so_far = minibatch[-1:], simple_batch_size_fn(ex, 1)\n",
    "    if minibatch:\n",
    "        yield minibatch\n",
    "\n",
    "\n",
    "def load_dataset(corpus_type, shuffle, bert_data_path=\"../bert_data/\", pre=\"cnndm\"):\n",
    "    \"\"\"\n",
    "    Dataset generator. Don't do extra stuff here, like printing,\n",
    "    because they will be postponed to the first loading time.\n",
    "    Args:\n",
    "        corpus_type: 'train' or 'valid'\n",
    "    Returns:\n",
    "        A list of dataset, the dataset(s) are lazily loaded.\n",
    "    \"\"\"\n",
    "    assert corpus_type in [\"train\", \"valid\", \"test\"]\n",
    "\n",
    "    def _lazy_dataset_loader(pt_file, corpus_type):\n",
    "        dataset = torch.load(pt_file)\n",
    "        logger.info('Loading %s dataset from %s, number of examples: %d' %\n",
    "                    (corpus_type, pt_file, len(dataset)))\n",
    "        return dataset\n",
    "\n",
    "    # Sort the glob output by file name (by increasing indexes).\n",
    "    allFiles = os.listdir(bert_data_path)\n",
    "    pts = [file for file in allFiles if file[-3:] == '.pt']\n",
    "    \n",
    "    if pts:\n",
    "        if (shuffle):\n",
    "            random.shuffle(pts)\n",
    "\n",
    "        for pt in pts:\n",
    "            yield _lazy_dataset_loader(pt, corpus_type)\n",
    "    else:\n",
    "        # Only one inputters.*Dataset, simple!\n",
    "        pt = bert_data_path + pre + '.' + corpus_type + '.pt'\n",
    "        yield _lazy_dataset_loader(pt, corpus_type)\n",
    "\n",
    "\n",
    "def simple_batch_size_fn(new, count):\n",
    "    src, labels = new[0], new[1]\n",
    "    global max_n_sents, max_n_tokens, max_size\n",
    "    if count == 1:\n",
    "        max_size = 0\n",
    "        max_n_sents=0\n",
    "        max_n_tokens=0\n",
    "    max_n_sents = max(max_n_sents, len(src))\n",
    "    max_size = max(max_size, max_n_sents)\n",
    "    src_elements = count * max_size\n",
    "    return src_elements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loader Class + Iterators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataloader(object):\n",
    "    def __init__(self, args, datasets,  batch_size,\n",
    "                 device, shuffle, is_test):\n",
    "        self.args = args\n",
    "        self.datasets = datasets\n",
    "        self.batch_size = batch_size\n",
    "        self.device = device\n",
    "        self.shuffle = shuffle\n",
    "        self.is_test = is_test\n",
    "        self.cur_iter = self._next_dataset_iterator(datasets)\n",
    "\n",
    "        assert self.cur_iter is not None\n",
    "\n",
    "    def __iter__(self):\n",
    "        dataset_iter = (d for d in self.datasets)\n",
    "        while self.cur_iter is not None:\n",
    "            for batch in self.cur_iter:\n",
    "                yield batch\n",
    "            self.cur_iter = self._next_dataset_iterator(dataset_iter)\n",
    "\n",
    "\n",
    "    def _next_dataset_iterator(self, dataset_iter):\n",
    "        try:\n",
    "            # Drop the current dataset for decreasing memory\n",
    "            if hasattr(self, \"cur_dataset\"):\n",
    "                self.cur_dataset = None\n",
    "                gc.collect()\n",
    "                del self.cur_dataset\n",
    "                gc.collect()\n",
    "\n",
    "            self.cur_dataset = next(dataset_iter)\n",
    "        except StopIteration:\n",
    "            return None\n",
    "\n",
    "        return DataIterator(args = self.args,\n",
    "            dataset=self.cur_dataset,  batch_size=self.batch_size,\n",
    "            device=self.device, shuffle=self.shuffle, is_test=self.is_test)\n",
    "\n",
    "\n",
    "class DataIterator(object):\n",
    "    def __init__(self, args, dataset,  batch_size,  device=None, is_test=False,\n",
    "                 shuffle=True):\n",
    "        self.args = args\n",
    "        self.batch_size, self.is_test, self.dataset = batch_size, is_test, dataset\n",
    "        self.iterations = 0\n",
    "        self.device = device\n",
    "        self.shuffle = shuffle\n",
    "\n",
    "        self.sort_key = lambda x: len(x[1])\n",
    "\n",
    "        self._iterations_this_epoch = 0\n",
    "\n",
    "    def data(self):\n",
    "        if self.shuffle:\n",
    "            random.shuffle(self.dataset)\n",
    "        xs = self.dataset\n",
    "        return xs\n",
    "\n",
    "\n",
    "    def preprocess(self, ex, is_test):\n",
    "        src = ex['src']\n",
    "        if('labels' in ex):\n",
    "            labels = ex['labels']\n",
    "        else:\n",
    "            labels = ex['src_sent_labels']\n",
    "\n",
    "        segs = ex['segs']\n",
    "        if(not self.args.use_interval):\n",
    "            segs=[0]*len(segs)\n",
    "        clss = ex['clss']\n",
    "        src_txt = ex['src_txt']\n",
    "        tgt_txt = ex['tgt_txt']\n",
    "\n",
    "        if(is_test):\n",
    "            return src,labels,segs, clss, src_txt, tgt_txt\n",
    "        else:\n",
    "            return src,labels,segs, clss\n",
    "\n",
    "    def batch_buffer(self, data, batch_size):\n",
    "        minibatch, size_so_far = [], 0\n",
    "        for ex in data:\n",
    "            if(len(ex['src'])==0):\n",
    "                continue\n",
    "            ex = self.preprocess(ex, self.is_test)\n",
    "            if(ex is None):\n",
    "                continue\n",
    "            minibatch.append(ex)\n",
    "            size_so_far = simple_batch_size_fn(ex, len(minibatch))\n",
    "            if size_so_far == batch_size:\n",
    "                yield minibatch\n",
    "                minibatch, size_so_far = [], 0\n",
    "            elif size_so_far > batch_size:\n",
    "                yield minibatch[:-1]\n",
    "                minibatch, size_so_far = minibatch[-1:], simple_batch_size_fn(ex, 1)\n",
    "        if minibatch:\n",
    "            yield minibatch\n",
    "\n",
    "    def create_batches(self):\n",
    "        \"\"\" Create batches \"\"\"\n",
    "        data = self.data()\n",
    "        for buffer in self.batch_buffer(data, self.batch_size * 50):\n",
    "\n",
    "            p_batch = sorted(buffer, key=lambda x: len(x[3]))\n",
    "            p_batch = batch(p_batch, self.batch_size)\n",
    "\n",
    "            p_batch = list(p_batch)\n",
    "            if (self.shuffle):\n",
    "                random.shuffle(p_batch)\n",
    "            for b in p_batch:\n",
    "                yield b\n",
    "\n",
    "    def __iter__(self):\n",
    "        while True:\n",
    "            self.batches = self.create_batches()\n",
    "            for idx, minibatch in enumerate(self.batches):\n",
    "                # fast-forward if loaded from state\n",
    "                if self._iterations_this_epoch > idx:\n",
    "                    continue\n",
    "                self.iterations += 1\n",
    "                self._iterations_this_epoch += 1\n",
    "                batch = Batch(minibatch, self.device, self.is_test)\n",
    "\n",
    "                yield batch\n",
    "            return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Optimizers class \"\"\"\n",
    "def use_gpu(opt):\n",
    "    \"\"\"\n",
    "    Creates a boolean if gpu used\n",
    "    \"\"\"\n",
    "    return (hasattr(opt, 'gpu_ranks') and len(opt.gpu_ranks) > 0) or \\\n",
    "           (hasattr(opt, 'gpu') and opt.gpu > -1)\n",
    "\n",
    "def build_optim(model, opt, checkpoint):\n",
    "    \"\"\" Build optimizer \"\"\"\n",
    "    saved_optimizer_state_dict = None\n",
    "\n",
    "    if opt.train_from:\n",
    "        optim = checkpoint['optim']\n",
    "        # We need to save a copy of optim.optimizer.state_dict() for setting\n",
    "        # the, optimizer state later on in Stage 2 in this method, since\n",
    "        # the method optim.set_parameters(model.parameters()) will overwrite\n",
    "        # optim.optimizer, and with ith the values stored in\n",
    "        # optim.optimizer.state_dict()\n",
    "        saved_optimizer_state_dict = optim.optimizer.state_dict()\n",
    "    else:\n",
    "        optim = Optimizer(\n",
    "            opt.optim, opt.learning_rate, opt.max_grad_norm,\n",
    "            lr_decay=opt.learning_rate_decay,\n",
    "            start_decay_steps=opt.start_decay_steps,\n",
    "            decay_steps=opt.decay_steps,\n",
    "            beta1=opt.adam_beta1,\n",
    "            beta2=opt.adam_beta2,\n",
    "            adagrad_accum=opt.adagrad_accumulator_init,\n",
    "            decay_method=opt.decay_method,\n",
    "            warmup_steps=opt.warmup_steps)\n",
    "\n",
    "    # Stage 1:\n",
    "    # Essentially optim.set_parameters (re-)creates and optimizer using\n",
    "    # model.paramters() as parameters that will be stored in the\n",
    "    # optim.optimizer.param_groups field of the torch optimizer class.\n",
    "    # Importantly, this method does not yet load the optimizer state, as\n",
    "    # essentially it builds a new optimizer with empty optimizer state and\n",
    "    # parameters from the model.\n",
    "    optim.set_parameters(model.named_parameters())\n",
    "\n",
    "    if opt.train_from:\n",
    "        # Stage 2: In this stage, which is only performed when loading an\n",
    "        # optimizer from a checkpoint, we load the saved_optimizer_state_dict\n",
    "        # into the re-created optimizer, to set the optim.optimizer.state\n",
    "        # field, which was previously empty. For this, we use the optimizer\n",
    "        # state saved in the \"saved_optimizer_state_dict\" variable for\n",
    "        # this purpose.\n",
    "        # See also: https://github.com/pytorch/pytorch/issues/2830\n",
    "        optim.optimizer.load_state_dict(saved_optimizer_state_dict)\n",
    "        # Convert back the state values to cuda type if applicable\n",
    "        if use_gpu(opt):\n",
    "            for state in optim.optimizer.state.values():\n",
    "                for k, v in state.items():\n",
    "                    if torch.is_tensor(v):\n",
    "                        state[k] = v.cuda()\n",
    "\n",
    "        # We want to make sure that indeed we have a non-empty optimizer state\n",
    "        # when we loaded an existing model. This should be at least the case\n",
    "        # for Adam, which saves \"exp_avg\" and \"exp_avg_sq\" state\n",
    "        # (Exponential moving average of gradient and squared gradient values)\n",
    "        if (optim.method == 'adam') and (len(optim.optimizer.state) < 1):\n",
    "            raise RuntimeError(\n",
    "                \"Error: loaded Adam optimizer from existing model\" +\n",
    "                \" but optimizer state is empty\")\n",
    "\n",
    "    return optim\n",
    "\n",
    "\n",
    "class MultipleOptimizer(object):\n",
    "    \"\"\" Implement multiple optimizers needed for sparse adam \"\"\"\n",
    "\n",
    "    def __init__(self, op):\n",
    "        \"\"\" ? \"\"\"\n",
    "        self.optimizers = op\n",
    "\n",
    "    def zero_grad(self):\n",
    "        \"\"\" ? \"\"\"\n",
    "        for op in self.optimizers:\n",
    "            op.zero_grad()\n",
    "\n",
    "    def step(self):\n",
    "        \"\"\" ? \"\"\"\n",
    "        for op in self.optimizers:\n",
    "            op.step()\n",
    "\n",
    "    @property\n",
    "    def state(self):\n",
    "        \"\"\" ? \"\"\"\n",
    "        return {k: v for op in self.optimizers for k, v in op.state.items()}\n",
    "\n",
    "    def state_dict(self):\n",
    "        \"\"\" ? \"\"\"\n",
    "        return [op.state_dict() for op in self.optimizers]\n",
    "\n",
    "    def load_state_dict(self, state_dicts):\n",
    "        \"\"\" ? \"\"\"\n",
    "        assert len(state_dicts) == len(self.optimizers)\n",
    "        for i in range(len(state_dicts)):\n",
    "            self.optimizers[i].load_state_dict(state_dicts[i])\n",
    "\n",
    "\n",
    "class Optimizer(object):\n",
    "    \"\"\"\n",
    "    Controller class for optimization. Mostly a thin\n",
    "    wrapper for `optim`, but also useful for implementing\n",
    "    rate scheduling beyond what is currently available.\n",
    "    Also implements necessary methods for training RNNs such\n",
    "    as grad manipulations.\n",
    "    Args:\n",
    "      method (:obj:`str`): one of [sgd, adagrad, adadelta, adam]\n",
    "      lr (float): learning rate\n",
    "      lr_decay (float, optional): learning rate decay multiplier\n",
    "      start_decay_steps (int, optional): step to start learning rate decay\n",
    "      beta1, beta2 (float, optional): parameters for adam\n",
    "      adagrad_accum (float, optional): initialization parameter for adagrad\n",
    "      decay_method (str, option): custom decay options\n",
    "      warmup_steps (int, option): parameter for `noam` decay\n",
    "    We use the default parameters for Adam that are suggested by\n",
    "    the original paper https://arxiv.org/pdf/1412.6980.pdf\n",
    "    These values are also used by other established implementations,\n",
    "    e.g. https://www.tensorflow.org/api_docs/python/tf/train/AdamOptimizer\n",
    "    https://keras.io/optimizers/\n",
    "    Recently there are slightly different values used in the paper\n",
    "    \"Attention is all you need\"\n",
    "    https://arxiv.org/pdf/1706.03762.pdf, particularly the value beta2=0.98\n",
    "    was used there however, beta2=0.999 is still arguably the more\n",
    "    established value, so we use that here as well\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, method, learning_rate, max_grad_norm,\n",
    "                 lr_decay=1, start_decay_steps=None, decay_steps=None,\n",
    "                 beta1=0.9, beta2=0.999,\n",
    "                 adagrad_accum=0.0,\n",
    "                 decay_method=None,\n",
    "                 warmup_steps=4000\n",
    "                 ):\n",
    "        self.last_ppl = None\n",
    "        self.learning_rate = learning_rate\n",
    "        self.original_lr = learning_rate\n",
    "        self.max_grad_norm = max_grad_norm\n",
    "        self.method = method\n",
    "        self.lr_decay = lr_decay\n",
    "        self.start_decay_steps = start_decay_steps\n",
    "        self.decay_steps = decay_steps\n",
    "        self.start_decay = False\n",
    "        self._step = 0\n",
    "        self.betas = [beta1, beta2]\n",
    "        self.adagrad_accum = adagrad_accum\n",
    "        self.decay_method = decay_method\n",
    "        self.warmup_steps = warmup_steps\n",
    "\n",
    "    def set_parameters(self, params):\n",
    "        \"\"\" ? \"\"\"\n",
    "        self.params = []\n",
    "        self.sparse_params = []\n",
    "        for k, p in params:\n",
    "            if p.requires_grad:\n",
    "                if self.method != 'sparseadam' or \"embed\" not in k:\n",
    "                    self.params.append(p)\n",
    "                else:\n",
    "                    self.sparse_params.append(p)\n",
    "        if self.method == 'sgd':\n",
    "            self.optimizer = optim.SGD(self.params, lr=self.learning_rate)\n",
    "        elif self.method == 'adagrad':\n",
    "            self.optimizer = optim.Adagrad(self.params, lr=self.learning_rate)\n",
    "            for group in self.optimizer.param_groups:\n",
    "                for p in group['params']:\n",
    "                    self.optimizer.state[p]['sum'] = self.optimizer\\\n",
    "                        .state[p]['sum'].fill_(self.adagrad_accum)\n",
    "        elif self.method == 'adadelta':\n",
    "            self.optimizer = optim.Adadelta(self.params, lr=self.learning_rate)\n",
    "        elif self.method == 'adam':\n",
    "            self.optimizer = optim.Adam(self.params, lr=self.learning_rate,\n",
    "                                        betas=self.betas, eps=1e-9)\n",
    "        elif self.method == 'sparseadam':\n",
    "            self.optimizer = MultipleOptimizer(\n",
    "                [optim.Adam(self.params, lr=self.learning_rate,\n",
    "                            betas=self.betas, eps=1e-8),\n",
    "                 optim.SparseAdam(self.sparse_params, lr=self.learning_rate,\n",
    "                                  betas=self.betas, eps=1e-8)])\n",
    "        else:\n",
    "            raise RuntimeError(\"Invalid optim method: \" + self.method)\n",
    "\n",
    "    def _set_rate(self, learning_rate):\n",
    "        self.learning_rate = learning_rate\n",
    "        if self.method != 'sparseadam':\n",
    "            self.optimizer.param_groups[0]['lr'] = self.learning_rate\n",
    "        else:\n",
    "            for op in self.optimizer.optimizers:\n",
    "                op.param_groups[0]['lr'] = self.learning_rate\n",
    "\n",
    "    def step(self):\n",
    "        \"\"\"Update the model parameters based on current gradients.\n",
    "        Optionally, will employ gradient modification or update learning\n",
    "        rate.\n",
    "        \"\"\"\n",
    "        self._step += 1\n",
    "\n",
    "        # Decay method used in tensor2tensor.\n",
    "        if self.decay_method == \"noam\":\n",
    "            self._set_rate(\n",
    "                self.original_lr *\n",
    "\n",
    "                 min(self._step ** (-0.5),\n",
    "                     self._step * self.warmup_steps**(-1.5)))\n",
    "\n",
    "            # self._set_rate(self.original_lr *self.model_size ** (-0.5) *min(1.0, self._step / self.warmup_steps)*max(self._step, self.warmup_steps)**(-0.5))\n",
    "        # Decay based on start_decay_steps every decay_steps\n",
    "        else:\n",
    "            if ((self.start_decay_steps is not None) and (\n",
    "                     self._step >= self.start_decay_steps)):\n",
    "                self.start_decay = True\n",
    "            if self.start_decay:\n",
    "                if ((self._step - self.start_decay_steps)\n",
    "                   % self.decay_steps == 0):\n",
    "                    self.learning_rate = self.learning_rate * self.lr_decay\n",
    "\n",
    "        if self.method != 'sparseadam':\n",
    "            self.optimizer.param_groups[0]['lr'] = self.learning_rate\n",
    "\n",
    "        if self.max_grad_norm:\n",
    "            clip_grad_norm_(self.params, self.max_grad_norm)\n",
    "        self.optimizer.step()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Bert(nn.Module):\n",
    "    def __init__(self, temp_dir, load_pretrained_bert, bert_config):\n",
    "        super(Bert, self).__init__()\n",
    "        if(load_pretrained_bert):\n",
    "            self.model = BertModel.from_pretrained('bert-base-uncased', cache_dir=temp_dir)\n",
    "        else:\n",
    "            self.model = BertModel(bert_config)\n",
    "\n",
    "    def forward(self, x, segs, mask):\n",
    "        encoded_layers, _ = self.model(x, segs, attention_mask =mask)\n",
    "        contextualEncoding = encoded_layers[-1]\n",
    "        return contextualEncoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Summarizer(nn.Module):\n",
    "    def __init__(self, args, device, load_pretrained_bert = False, bert_config = None):\n",
    "        super(Summarizer, self).__init__()\n",
    "        self.args = args\n",
    "        self.device = device\n",
    "        self.bert = Bert(args.temp_dir, load_pretrained_bert, bert_config)\n",
    "        self.encoder = TransformerInterEncoder(self.bert.model.config.hidden_size, args.ff_size, args.heads,\n",
    "                                                   args.dropout, args.inter_layers)\n",
    "        if args.param_init != 0.0:\n",
    "            for p in self.encoder.parameters():\n",
    "                p.data.uniform_(-args.param_init, args.param_init)\n",
    "        if args.param_init_glorot:\n",
    "            for p in self.encoder.parameters():\n",
    "                if p.dim() > 1:\n",
    "                    xavier_uniform_(p)\n",
    "\n",
    "        self.to(device)\n",
    "    def load_cp(self, pt):\n",
    "        self.load_state_dict(pt['model'], strict=True)\n",
    "\n",
    "    def forward(self, x, segs, clss, mask, mask_cls, sentence_range=None):\n",
    "\n",
    "        top_vec = self.bert(x, segs, mask)\n",
    "        sents_vec = top_vec[torch.arange(top_vec.size(0)).unsqueeze(1), clss]\n",
    "        sents_vec = sents_vec * mask_cls[:, :, None].float()\n",
    "        sent_scores = self.encoder(sents_vec, mask_cls).squeeze(-1)\n",
    "        return sent_scores, mask_cls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Rouge155(object):\n",
    "    \"\"\"\n",
    "    This is a wrapper for the ROUGE 1.5.5 summary evaluation package.\n",
    "    This class is designed to simplify the evaluation process by:\n",
    "        1) Converting summaries into a format ROUGE understands.\n",
    "        2) Generating the ROUGE configuration file automatically based\n",
    "            on filename patterns.\n",
    "    This class can be used within Python like this:\n",
    "    rouge = Rouge155()\n",
    "    rouge.system_dir = 'test/systems'\n",
    "    rouge.model_dir = 'test/models'\n",
    "    # The system filename pattern should contain one group that\n",
    "    # matches the document ID.\n",
    "    rouge.system_filename_pattern = 'SL.P.10.R.11.SL062003-(\\d+).html'\n",
    "    # The model filename pattern has '#ID#' as a placeholder for the\n",
    "    # document ID. If there are multiple model summaries, pyrouge\n",
    "    # will use the provided regex to automatically match them with\n",
    "    # the corresponding system summary. Here, [A-Z] matches\n",
    "    # multiple model summaries for a given #ID#.\n",
    "    rouge.model_filename_pattern = 'SL.P.10.R.[A-Z].SL062003-#ID#.html'\n",
    "    rouge_output = rouge.evaluate()\n",
    "    print(rouge_output)\n",
    "    output_dict = rouge.output_to_dict(rouge_ouput)\n",
    "    print(output_dict)\n",
    "    ->    {'rouge_1_f_score': 0.95652,\n",
    "         'rouge_1_f_score_cb': 0.95652,\n",
    "         'rouge_1_f_score_ce': 0.95652,\n",
    "         'rouge_1_precision': 0.95652,\n",
    "        [...]\n",
    "    To evaluate multiple systems:\n",
    "        rouge = Rouge155()\n",
    "        rouge.system_dir = '/PATH/TO/systems'\n",
    "        rouge.model_dir = 'PATH/TO/models'\n",
    "        for system_id in ['id1', 'id2', 'id3']:\n",
    "            rouge.system_filename_pattern = \\\n",
    "                'SL.P/.10.R.{}.SL062003-(\\d+).html'.format(system_id)\n",
    "            rouge.model_filename_pattern = \\\n",
    "                'SL.P.10.R.[A-Z].SL062003-#ID#.html'\n",
    "            rouge_output = rouge.evaluate(system_id)\n",
    "            print(rouge_output)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, rouge_dir=None, rouge_args=None, temp_dir = None):\n",
    "        \"\"\"\n",
    "        Create a Rouge155 object.\n",
    "            rouge_dir:  Directory containing Rouge-1.5.5.pl\n",
    "            rouge_args: Arguments to pass through to ROUGE if you\n",
    "                        don't want to use the default pyrouge\n",
    "                        arguments.\n",
    "        \"\"\"\n",
    "        self.temp_dir=temp_dir\n",
    "        self.log = log.get_global_console_logger()\n",
    "        self.__set_dir_properties()\n",
    "        self._config_file = None\n",
    "        self._settings_file = self.__get_config_path()\n",
    "        self.__set_rouge_dir(rouge_dir)\n",
    "        self.args = self.__clean_rouge_args(rouge_args)\n",
    "        self._system_filename_pattern = None\n",
    "        self._model_filename_pattern = None\n",
    "\n",
    "    def save_home_dir(self):\n",
    "        config = ConfigParser()\n",
    "        section = 'pyrouge settings'\n",
    "        config.add_section(section)\n",
    "        config.set(section, 'home_dir', self._home_dir)\n",
    "        with open(self._settings_file, 'w') as f:\n",
    "            config.write(f)\n",
    "        self.log.info(\"Set ROUGE home directory to {}.\".format(self._home_dir))\n",
    "\n",
    "    @property\n",
    "    def settings_file(self):\n",
    "        \"\"\"\n",
    "        Path of the setttings file, which stores the ROUGE home dir.\n",
    "        \"\"\"\n",
    "        return self._settings_file\n",
    "\n",
    "    @property\n",
    "    def bin_path(self):\n",
    "        \"\"\"\n",
    "        The full path of the ROUGE binary (although it's technically\n",
    "        a script), i.e. rouge_home_dir/ROUGE-1.5.5.pl\n",
    "        \"\"\"\n",
    "        if self._bin_path is None:\n",
    "            raise Exception(\n",
    "                \"ROUGE path not set. Please set the ROUGE home directory \"\n",
    "                \"and ensure that ROUGE-1.5.5.pl exists in it.\")\n",
    "        return self._bin_path\n",
    "\n",
    "    @property\n",
    "    def system_filename_pattern(self):\n",
    "        \"\"\"\n",
    "        The regular expression pattern for matching system summary\n",
    "        filenames. The regex string.\n",
    "        E.g. \"SL.P.10.R.11.SL062003-(\\d+).html\" will match the system\n",
    "        filenames in the SPL2003/system folder of the ROUGE SPL example\n",
    "        in the \"sample-test\" folder.\n",
    "        Currently, there is no support for multiple systems.\n",
    "        \"\"\"\n",
    "        return self._system_filename_pattern\n",
    "\n",
    "    @system_filename_pattern.setter\n",
    "    def system_filename_pattern(self, pattern):\n",
    "        self._system_filename_pattern = pattern\n",
    "\n",
    "    @property\n",
    "    def model_filename_pattern(self):\n",
    "        \"\"\"\n",
    "        The regular expression pattern for matching model summary\n",
    "        filenames. The pattern needs to contain the string \"#ID#\",\n",
    "        which is a placeholder for the document ID.\n",
    "        E.g. \"SL.P.10.R.[A-Z].SL062003-#ID#.html\" will match the model\n",
    "        filenames in the SPL2003/system folder of the ROUGE SPL\n",
    "        example in the \"sample-test\" folder.\n",
    "        \"#ID#\" is a placeholder for the document ID which has been\n",
    "        matched by the \"(\\d+)\" part of the system filename pattern.\n",
    "        The different model summaries for a given document ID are\n",
    "        matched by the \"[A-Z]\" part.\n",
    "        \"\"\"\n",
    "        return self._model_filename_pattern\n",
    "\n",
    "    @model_filename_pattern.setter\n",
    "    def model_filename_pattern(self, pattern):\n",
    "        self._model_filename_pattern = pattern\n",
    "\n",
    "    @property\n",
    "    def config_file(self):\n",
    "        return self._config_file\n",
    "\n",
    "    @config_file.setter\n",
    "    def config_file(self, path):\n",
    "        config_dir, _ = os.path.split(path)\n",
    "        verify_dir(config_dir, \"configuration file\")\n",
    "        self._config_file = path\n",
    "\n",
    "    def split_sentences(self):\n",
    "        \"\"\"\n",
    "        ROUGE requires texts split into sentences. In case the texts\n",
    "        are not already split, this method can be used.\n",
    "        \"\"\"\n",
    "        from pyrouge.utils.sentence_splitter import PunktSentenceSplitter\n",
    "        self.log.info(\"Splitting sentences.\")\n",
    "        ss = PunktSentenceSplitter()\n",
    "        sent_split_to_string = lambda s: \"\\n\".join(ss.split(s))\n",
    "        process_func = partial(\n",
    "            DirectoryProcessor.process, function=sent_split_to_string)\n",
    "        self.__process_summaries(process_func)\n",
    "\n",
    "    @staticmethod\n",
    "    def convert_summaries_to_rouge_format(input_dir, output_dir):\n",
    "        \"\"\"\n",
    "        Convert all files in input_dir into a format ROUGE understands\n",
    "        and saves the files to output_dir. The input files are assumed\n",
    "        to be plain text with one sentence per line.\n",
    "            input_dir:  Path of directory containing the input files.\n",
    "            output_dir: Path of directory in which the converted files\n",
    "                        will be saved.\n",
    "        \"\"\"\n",
    "        DirectoryProcessor.process(\n",
    "            input_dir, output_dir, Rouge155.convert_text_to_rouge_format)\n",
    "\n",
    "    @staticmethod\n",
    "    def convert_text_to_rouge_format(text, title=\"dummy title\"):\n",
    "        \"\"\"\n",
    "        Convert a text to a format ROUGE understands. The text is\n",
    "        assumed to contain one sentence per line.\n",
    "            text:   The text to convert, containg one sentence per line.\n",
    "            title:  Optional title for the text. The title will appear\n",
    "                    in the converted file, but doesn't seem to have\n",
    "                    any other relevance.\n",
    "        Returns: The converted text as string.\n",
    "        \"\"\"\n",
    "        # sentences = text.split(\"\\n\")\n",
    "        sentences = text.split(\"<q>\")\n",
    "        sent_elems = [\n",
    "            \"<a name=\\\"{i}\\\">[{i}]</a> <a href=\\\"#{i}\\\" id={i}>\"\n",
    "            \"{text}</a>\".format(i=i, text=sent)\n",
    "            for i, sent in enumerate(sentences, start=1)]\n",
    "        html = \"\"\"<html>\n",
    "<head>\n",
    "<title>{title}</title>\n",
    "</head>\n",
    "<body bgcolor=\"white\">\n",
    "{elems}\n",
    "</body>\n",
    "</html>\"\"\".format(title=title, elems=\"\\n\".join(sent_elems))\n",
    "\n",
    "        return html\n",
    "\n",
    "    @staticmethod\n",
    "    def write_config_static(system_dir, system_filename_pattern,\n",
    "                            model_dir, model_filename_pattern,\n",
    "                            config_file_path, system_id=None):\n",
    "        \"\"\"\n",
    "        Write the ROUGE configuration file, which is basically a list\n",
    "        of system summary files and their corresponding model summary\n",
    "        files.\n",
    "        pyrouge uses regular expressions to automatically find the\n",
    "        matching model summary files for a given system summary file\n",
    "        (cf. docstrings for system_filename_pattern and\n",
    "        model_filename_pattern).\n",
    "            system_dir:                 Path of directory containing\n",
    "                                        system summaries.\n",
    "            system_filename_pattern:    Regex string for matching\n",
    "                                        system summary filenames.\n",
    "            model_dir:                  Path of directory containing\n",
    "                                        model summaries.\n",
    "            model_filename_pattern:     Regex string for matching model\n",
    "                                        summary filenames.\n",
    "            config_file_path:           Path of the configuration file.\n",
    "            system_id:                  Optional system ID string which\n",
    "                                        will appear in the ROUGE output.\n",
    "        \"\"\"\n",
    "        system_filenames = [f for f in os.listdir(system_dir)]\n",
    "        system_models_tuples = []\n",
    "\n",
    "        system_filename_pattern = re.compile(system_filename_pattern)\n",
    "        for system_filename in sorted(system_filenames):\n",
    "            match = system_filename_pattern.match(system_filename)\n",
    "            if match:\n",
    "                id = match.groups(0)[0]\n",
    "                model_filenames = [model_filename_pattern.replace('#ID#',id)]\n",
    "                # model_filenames = Rouge155.__get_model_filenames_for_id(\n",
    "                #     id, model_dir, model_filename_pattern)\n",
    "                system_models_tuples.append(\n",
    "                    (system_filename, sorted(model_filenames)))\n",
    "        if not system_models_tuples:\n",
    "            raise Exception(\n",
    "                \"Did not find any files matching the pattern {} \"\n",
    "                \"in the system summaries directory {}.\".format(\n",
    "                    system_filename_pattern.pattern, system_dir))\n",
    "\n",
    "        with codecs.open(config_file_path, 'w', encoding='utf-8') as f:\n",
    "            f.write('<ROUGE-EVAL version=\"1.55\">')\n",
    "            for task_id, (system_filename, model_filenames) in enumerate(\n",
    "                    system_models_tuples, start=1):\n",
    "\n",
    "                eval_string = Rouge155.__get_eval_string(\n",
    "                    task_id, system_id,\n",
    "                    system_dir, system_filename,\n",
    "                    model_dir, model_filenames)\n",
    "                f.write(eval_string)\n",
    "            f.write(\"</ROUGE-EVAL>\")\n",
    "\n",
    "    def write_config(self, config_file_path=None, system_id=None):\n",
    "        \"\"\"\n",
    "        Write the ROUGE configuration file, which is basically a list\n",
    "        of system summary files and their matching model summary files.\n",
    "        This is a non-static version of write_config_file_static().\n",
    "            config_file_path:   Path of the configuration file.\n",
    "            system_id:          Optional system ID string which will\n",
    "                                appear in the ROUGE output.\n",
    "        \"\"\"\n",
    "        if not system_id:\n",
    "            system_id = 1\n",
    "        if (not config_file_path) or (not self._config_dir):\n",
    "            self._config_dir = mkdtemp(dir=self.temp_dir)\n",
    "            config_filename = \"rouge_conf.xml\"\n",
    "        else:\n",
    "            config_dir, config_filename = os.path.split(config_file_path)\n",
    "            verify_dir(config_dir, \"configuration file\")\n",
    "        self._config_file = os.path.join(self._config_dir, config_filename)\n",
    "        Rouge155.write_config_static(\n",
    "            self._system_dir, self._system_filename_pattern,\n",
    "            self._model_dir, self._model_filename_pattern,\n",
    "            self._config_file, system_id)\n",
    "        self.log.info(\n",
    "            \"Written ROUGE configuration to {}\".format(self._config_file))\n",
    "\n",
    "    def evaluate(self, system_id=1, rouge_args=None):\n",
    "        \"\"\"\n",
    "        Run ROUGE to evaluate the system summaries in system_dir against\n",
    "        the model summaries in model_dir. The summaries are assumed to\n",
    "        be in the one-sentence-per-line HTML format ROUGE understands.\n",
    "            system_id:  Optional system ID which will be printed in\n",
    "                        ROUGE's output.\n",
    "        Returns: Rouge output as string.\n",
    "        \"\"\"\n",
    "        self.write_config(system_id=system_id)\n",
    "        options = self.__get_options(rouge_args)\n",
    "        command = [self._bin_path] + options\n",
    "        self.log.info(\n",
    "            \"Running ROUGE with command {}\".format(\" \".join(command)))\n",
    "        rouge_output = check_output(command).decode(\"UTF-8\")\n",
    "        return rouge_output\n",
    "\n",
    "    def convert_and_evaluate(self, system_id=1,\n",
    "                             split_sentences=False, rouge_args=None):\n",
    "        \"\"\"\n",
    "        Convert plain text summaries to ROUGE format and run ROUGE to\n",
    "        evaluate the system summaries in system_dir against the model\n",
    "        summaries in model_dir. Optionally split texts into sentences\n",
    "        in case they aren't already.\n",
    "        This is just a convenience method combining\n",
    "        convert_summaries_to_rouge_format() and evaluate().\n",
    "            split_sentences:    Optional argument specifying if\n",
    "                                sentences should be split.\n",
    "            system_id:          Optional system ID which will be printed\n",
    "                                in ROUGE's output.\n",
    "        Returns: ROUGE output as string.\n",
    "        \"\"\"\n",
    "        if split_sentences:\n",
    "            self.split_sentences()\n",
    "        self.__write_summaries()\n",
    "        rouge_output = self.evaluate(system_id, rouge_args)\n",
    "        return rouge_output\n",
    "\n",
    "    def output_to_dict(self, output):\n",
    "        \"\"\"\n",
    "        Convert the ROUGE output into python dictionary for further\n",
    "        processing.\n",
    "        \"\"\"\n",
    "        #0 ROUGE-1 Average_R: 0.02632 (95%-conf.int. 0.02632 - 0.02632)\n",
    "        pattern = re.compile(\n",
    "            r\"(\\d+) (ROUGE-\\S+) (Average_\\w): (\\d.\\d+) \"\n",
    "            r\"\\(95%-conf.int. (\\d.\\d+) - (\\d.\\d+)\\)\")\n",
    "        results = {}\n",
    "        for line in output.split(\"\\n\"):\n",
    "            match = pattern.match(line)\n",
    "            if match:\n",
    "                sys_id, rouge_type, measure, result, conf_begin, conf_end = \\\n",
    "                    match.groups()\n",
    "                measure = {\n",
    "                    'Average_R': 'recall',\n",
    "                    'Average_P': 'precision',\n",
    "                    'Average_F': 'f_score'\n",
    "                    }[measure]\n",
    "                rouge_type = rouge_type.lower().replace(\"-\", '_')\n",
    "                key = \"{}_{}\".format(rouge_type, measure)\n",
    "                results[key] = float(result)\n",
    "                results[\"{}_cb\".format(key)] = float(conf_begin)\n",
    "                results[\"{}_ce\".format(key)] = float(conf_end)\n",
    "        return results\n",
    "\n",
    "    ###################################################################\n",
    "    # Private methods\n",
    "\n",
    "    def __set_rouge_dir(self, home_dir=None):\n",
    "        \"\"\"\n",
    "        Verfify presence of ROUGE-1.5.5.pl and data folder, and set\n",
    "        those paths.\n",
    "        \"\"\"\n",
    "        if not home_dir:\n",
    "            self._home_dir = self.__get_rouge_home_dir_from_settings()\n",
    "        else:\n",
    "            self._home_dir = home_dir\n",
    "            self.save_home_dir()\n",
    "        self._bin_path = os.path.join(self._home_dir, 'ROUGE-1.5.5.pl')\n",
    "        self.data_dir = os.path.join(self._home_dir, 'data')\n",
    "        if not os.path.exists(self._bin_path):\n",
    "            raise Exception(\n",
    "                \"ROUGE binary not found at {}. Please set the \"\n",
    "                \"correct path by running pyrouge_set_rouge_path \"\n",
    "                \"/path/to/rouge/home.\".format(self._bin_path))\n",
    "\n",
    "    def __get_rouge_home_dir_from_settings(self):\n",
    "        config = ConfigParser()\n",
    "        with open(self._settings_file) as f:\n",
    "            if hasattr(config, \"read_file\"):\n",
    "                config.read_file(f)\n",
    "            else:\n",
    "                # use deprecated python 2.x method\n",
    "                config.readfp(f)\n",
    "        rouge_home_dir = config.get('pyrouge settings', 'home_dir')\n",
    "        return rouge_home_dir\n",
    "\n",
    "    @staticmethod\n",
    "    def __get_eval_string(\n",
    "            task_id, system_id,\n",
    "            system_dir, system_filename,\n",
    "            model_dir, model_filenames):\n",
    "        \"\"\"\n",
    "        ROUGE can evaluate several system summaries for a given text\n",
    "        against several model summaries, i.e. there is an m-to-n\n",
    "        relation between system and model summaries. The system\n",
    "        summaries are listed in the <PEERS> tag and the model summaries\n",
    "        in the <MODELS> tag. pyrouge currently only supports one system\n",
    "        summary per text, i.e. it assumes a 1-to-n relation between\n",
    "        system and model summaries.\n",
    "        \"\"\"\n",
    "        peer_elems = \"<P ID=\\\"{id}\\\">{name}</P>\".format(\n",
    "            id=system_id, name=system_filename)\n",
    "\n",
    "        model_elems = [\"<M ID=\\\"{id}\\\">{name}</M>\".format(\n",
    "            id=chr(65 + i), name=name)\n",
    "            for i, name in enumerate(model_filenames)]\n",
    "\n",
    "        model_elems = \"\\n\\t\\t\\t\".join(model_elems)\n",
    "        eval_string = \"\"\"\n",
    "    <EVAL ID=\"{task_id}\">\n",
    "        <MODEL-ROOT>{model_root}</MODEL-ROOT>\n",
    "        <PEER-ROOT>{peer_root}</PEER-ROOT>\n",
    "        <INPUT-FORMAT TYPE=\"SEE\">\n",
    "        </INPUT-FORMAT>\n",
    "        <PEERS>\n",
    "            {peer_elems}\n",
    "        </PEERS>\n",
    "        <MODELS>\n",
    "            {model_elems}\n",
    "        </MODELS>\n",
    "    </EVAL>\n",
    "\"\"\".format(\n",
    "            task_id=task_id,\n",
    "            model_root=model_dir, model_elems=model_elems,\n",
    "            peer_root=system_dir, peer_elems=peer_elems)\n",
    "        return eval_string\n",
    "\n",
    "    def __process_summaries(self, process_func):\n",
    "        \"\"\"\n",
    "        Helper method that applies process_func to the files in the\n",
    "        system and model folders and saves the resulting files to new\n",
    "        system and model folders.\n",
    "        \"\"\"\n",
    "        temp_dir = mkdtemp(dir=self.temp_dir)\n",
    "        new_system_dir = os.path.join(temp_dir, \"system\")\n",
    "        os.mkdir(new_system_dir)\n",
    "        new_model_dir = os.path.join(temp_dir, \"model\")\n",
    "        os.mkdir(new_model_dir)\n",
    "        self.log.info(\n",
    "            \"Processing summaries. Saving system files to {} and \"\n",
    "            \"model files to {}.\".format(new_system_dir, new_model_dir))\n",
    "        process_func(self._system_dir, new_system_dir)\n",
    "        process_func(self._model_dir, new_model_dir)\n",
    "        self._system_dir = new_system_dir\n",
    "        self._model_dir = new_model_dir\n",
    "\n",
    "    def __write_summaries(self):\n",
    "        self.log.info(\"Writing summaries.\")\n",
    "        self.__process_summaries(self.convert_summaries_to_rouge_format)\n",
    "\n",
    "    @staticmethod\n",
    "    def __get_model_filenames_for_id(id, model_dir, model_filenames_pattern):\n",
    "        pattern = re.compile(model_filenames_pattern.replace('#ID#', id))\n",
    "        model_filenames = [\n",
    "            f for f in os.listdir(model_dir) if pattern.match(f)]\n",
    "        if not model_filenames:\n",
    "            raise Exception(\n",
    "                \"Could not find any model summaries for the system\"\n",
    "                \" summary with ID {}. Specified model filename pattern was: \"\n",
    "                \"{}\".format(id, model_filenames_pattern))\n",
    "        return model_filenames\n",
    "\n",
    "    def __get_options(self, rouge_args=None):\n",
    "        \"\"\"\n",
    "        Get supplied command line arguments for ROUGE or use default\n",
    "        ones.\n",
    "        \"\"\"\n",
    "        if self.args:\n",
    "            options = self.args.split()\n",
    "        elif rouge_args:\n",
    "            options = rouge_args.split()\n",
    "        else:\n",
    "            options = [\n",
    "                '-e', self._data_dir,\n",
    "                '-c', 95,\n",
    "                # '-2',\n",
    "                # '-1',\n",
    "                # '-U',\n",
    "                '-m',\n",
    "                # '-v',\n",
    "                '-r', 1000,\n",
    "                '-n', 2,\n",
    "                # '-w', 1.2,\n",
    "                '-a',\n",
    "                ]\n",
    "            options = list(map(str, options))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        options = self.__add_config_option(options)\n",
    "        return options\n",
    "\n",
    "    def __create_dir_property(self, dir_name, docstring):\n",
    "        \"\"\"\n",
    "        Generate getter and setter for a directory property.\n",
    "        \"\"\"\n",
    "        property_name = \"{}_dir\".format(dir_name)\n",
    "        private_name = \"_\" + property_name\n",
    "        setattr(self, private_name, None)\n",
    "\n",
    "        def fget(self):\n",
    "            return getattr(self, private_name)\n",
    "\n",
    "        def fset(self, path):\n",
    "            verify_dir(path, dir_name)\n",
    "            setattr(self, private_name, path)\n",
    "\n",
    "        p = property(fget=fget, fset=fset, doc=docstring)\n",
    "        setattr(self.__class__, property_name, p)\n",
    "\n",
    "    def __set_dir_properties(self):\n",
    "        \"\"\"\n",
    "        Automatically generate the properties for directories.\n",
    "        \"\"\"\n",
    "        directories = [\n",
    "            (\"home\", \"The ROUGE home directory.\"),\n",
    "            (\"data\", \"The path of the ROUGE 'data' directory.\"),\n",
    "            (\"system\", \"Path of the directory containing system summaries.\"),\n",
    "            (\"model\", \"Path of the directory containing model summaries.\"),\n",
    "            ]\n",
    "        for (dirname, docstring) in directories:\n",
    "            self.__create_dir_property(dirname, docstring)\n",
    "\n",
    "    def __clean_rouge_args(self, rouge_args):\n",
    "        \"\"\"\n",
    "        Remove enclosing quotation marks, if any.\n",
    "        \"\"\"\n",
    "        if not rouge_args:\n",
    "            return\n",
    "        quot_mark_pattern = re.compile('\"(.+)\"')\n",
    "        match = quot_mark_pattern.match(rouge_args)\n",
    "        if match:\n",
    "            cleaned_args = match.group(1)\n",
    "            return cleaned_args\n",
    "        else:\n",
    "            return rouge_args\n",
    "\n",
    "    def __add_config_option(self, options):\n",
    "        return options + [self._config_file]\n",
    "\n",
    "    def __get_config_path(self):\n",
    "        if platform.system() == \"Windows\":\n",
    "            parent_dir = os.getenv(\"APPDATA\")\n",
    "            config_dir_name = \"pyrouge\"\n",
    "        elif os.name == \"posix\":\n",
    "            parent_dir = os.path.expanduser(\"~\")\n",
    "            config_dir_name = \".pyrouge\"\n",
    "        else:\n",
    "            parent_dir = os.path.dirname(__file__)\n",
    "            config_dir_name = \"\"\n",
    "        config_dir = os.path.join(parent_dir, config_dir_name)\n",
    "        if not os.path.exists(config_dir):\n",
    "            os.makedirs(config_dir)\n",
    "        return os.path.join(config_dir, 'settings.ini')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
