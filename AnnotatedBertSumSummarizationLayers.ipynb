{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    },
    "colab": {
      "name": "AnnotatedBertSumSummarizationLayers.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "P9TduVJGW9LH",
        "outputId": "160772ce-7d5f-41ac-f276-76e8e0f29517",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!pip install pytorch_pretrained_bert\n",
        "!pip install pyrouge\n",
        "# !pip install tensorboardX\n",
        "# !pip install multiprocess"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pytorch_pretrained_bert\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d7/e0/c08d5553b89973d9a240605b9c12404bcf8227590de62bae27acbcfe076b/pytorch_pretrained_bert-0.6.2-py3-none-any.whl (123kB)\n",
            "\r\u001b[K     |██▋                             | 10kB 27.6MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 20kB 2.0MB/s eta 0:00:01\r\u001b[K     |████████                        | 30kB 2.7MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 40kB 2.9MB/s eta 0:00:01\r\u001b[K     |█████████████▎                  | 51kB 2.5MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 61kB 2.7MB/s eta 0:00:01\r\u001b[K     |██████████████████▌             | 71kB 3.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 81kB 3.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 92kB 3.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▌     | 102kB 3.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▏  | 112kB 3.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▊| 122kB 3.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 133kB 3.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (1.7.0+cu101)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (2019.12.20)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (2.23.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (1.18.5)\n",
            "Collecting boto3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c7/59/ac498beaa03fe70f123e91bc80552e810422e3305bfc67987799005b80ee/boto3-1.16.16-py2.py3-none-any.whl (129kB)\n",
            "\u001b[K     |████████████████████████████████| 133kB 10.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (4.41.1)\n",
            "Requirement already satisfied: dataclasses in /usr/local/lib/python3.6/dist-packages (from torch>=0.4.1->pytorch_pretrained_bert) (0.7)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch>=0.4.1->pytorch_pretrained_bert) (0.16.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.6/dist-packages (from torch>=0.4.1->pytorch_pretrained_bert) (3.7.4.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch_pretrained_bert) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch_pretrained_bert) (2020.6.20)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch_pretrained_bert) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch_pretrained_bert) (2.10)\n",
            "Collecting botocore<1.20.0,>=1.19.16\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/54/af/deadbd11bb212b03554bdb249fe574e402452459d2d7c3aabba1811bcf84/botocore-1.19.16-py2.py3-none-any.whl (6.8MB)\n",
            "\u001b[K     |████████████████████████████████| 6.8MB 10.0MB/s \n",
            "\u001b[?25hCollecting s3transfer<0.4.0,>=0.3.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/69/79/e6afb3d8b0b4e96cefbdc690f741d7dd24547ff1f94240c997a26fa908d3/s3transfer-0.3.3-py2.py3-none-any.whl (69kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 9.1MB/s \n",
            "\u001b[?25hCollecting jmespath<1.0.0,>=0.7.1\n",
            "  Downloading https://files.pythonhosted.org/packages/07/cb/5f001272b6faeb23c1c9e0acc04d48eaaf5c862c17709d20e3469c6e0139/jmespath-0.10.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.20.0,>=1.19.16->boto3->pytorch_pretrained_bert) (2.8.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.20.0,>=1.19.16->boto3->pytorch_pretrained_bert) (1.15.0)\n",
            "\u001b[31mERROR: botocore 1.19.16 has requirement urllib3<1.26,>=1.25.4; python_version != \"3.4\", but you'll have urllib3 1.24.3 which is incompatible.\u001b[0m\n",
            "Installing collected packages: jmespath, botocore, s3transfer, boto3, pytorch-pretrained-bert\n",
            "Successfully installed boto3-1.16.16 botocore-1.19.16 jmespath-0.10.0 pytorch-pretrained-bert-0.6.2 s3transfer-0.3.3\n",
            "Collecting pyrouge\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/11/85/e522dd6b36880ca19dcf7f262b22365748f56edc6f455e7b6a37d0382c32/pyrouge-0.1.3.tar.gz (60kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 2.2MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pyrouge\n",
            "  Building wheel for pyrouge (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyrouge: filename=pyrouge-0.1.3-cp36-none-any.whl size=191613 sha256=ef3a9048d8e99a40c9f3984c0b93622ae5b99866b3d1b30d54bf4ff87cc508d8\n",
            "  Stored in directory: /root/.cache/pip/wheels/75/d3/0c/e5b04e15b6b87c42e980de3931d2686e14d36e045058983599\n",
            "Successfully built pyrouge\n",
            "Installing collected packages: pyrouge\n",
            "Successfully installed pyrouge-0.1.3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-11-11T11:08:00.475946Z",
          "start_time": "2020-11-11T11:07:59.377780Z"
        },
        "id": "1RAytaZu7BSF"
      },
      "source": [
        "import pytorch_pretrained_bert\n",
        "import math\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import os\n",
        "#from tensorboardX import SummaryWriter\n",
        "\n",
        "import time\n",
        "#import distributed\n",
        "\n",
        "import gc\n",
        "import glob\n",
        "#import hashlib\n",
        "#import itertools\n",
        "import json\n",
        "import re\n",
        "import subprocess\n",
        "import time\n",
        "from os.path import join as pjoin\n",
        "\n",
        "#from multiprocess import Pool\n",
        "from pytorch_pretrained_bert import BertTokenizer\n",
        "import numpy as np\n",
        "import random \n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wdO4grV_EfBX"
      },
      "source": [
        "!mkdir bert_data\n",
        "!mkdir results\n",
        "!mkdir tmp\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "adCrJIQJ7BSL"
      },
      "source": [
        "# Encoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-11-12T03:26:20.879250Z",
          "start_time": "2020-11-12T03:26:20.854223Z"
        },
        "code_folding": [
          11
        ],
        "id": "hb9ItP0k7BSM"
      },
      "source": [
        "'''\n",
        "# verified\n",
        "torch.seed()\n",
        "pe = PositionalEncoding(4,.8,2)\n",
        "a=torch.tensor([[1,0,0,1],[1,1,0,0]])\n",
        "pe(a),pe.getPositonalEncoding(a)\n",
        "torch.seed()\n",
        "pe = PositionalEncoding2(.8,4,2)\n",
        "a=torch.tensor([[1,0,0,1],[1,1,0,0]])\n",
        "pe(a),pe.get_emb(a)\n",
        "'''\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self,embeddingDim,dropout,maxWordLen=5000):\n",
        "        positionalEncoding = torch.zeros(maxWordLen,embeddingDim)\n",
        "        # keep dim 1 array a singleton\n",
        "        position = torch.arange(0,maxWordLen).unsqueeze(1)\n",
        "        # positional encoding is defined to be \n",
        "        # PE(pos,2i)=sin(pos/1e4^(2i/embedding_dim))\n",
        "        # PE(pos,2i+1)=cos(pos/1e4^(2i/embedding_dim))\n",
        "        exponentTerm = torch.arange(0,embeddingDim,2, dtype=torch.float)\\\n",
        "                *(-math.log(1e4*1.0)/embeddingDim)\n",
        "        divisionTerm = torch.exp(exponentTerm)\n",
        "        # all even indices\n",
        "        positionalEncoding[:,0::2] = torch.sin(position.float()*divisionTerm)\n",
        "        # all odd indices\n",
        "        positionalEncoding[:,1::2] = torch.cos(position.float()*divisionTerm)\n",
        "        # keep dim 0 array size 1 --> a single array\n",
        "        positionalEncoding = positionalEncoding.unsqueeze(0)\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        self.register_buffer('positionalEncoding', positionalEncoding)\n",
        "        #dropout is probability of dropout\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "        self.embeddingDim = embeddingDim\n",
        "        \n",
        "    def forward(self,embedding):\n",
        "        # optional: add step -- not sure what it does\n",
        "        # for dropout?\n",
        "        embedding = embedding * math.sqrt(self.embeddingDim)\n",
        "        embedding = embedding + self.positionalEncoding[:, :embedding.size(1)]\n",
        "        embedding = self.dropout(embedding)\n",
        "        return embedding\n",
        "    def getPositonalEncoding(self,embedding):\n",
        "        return self.positionalEncoding[:, :embedding.size(1)]"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-11-12T03:26:23.328770Z",
          "start_time": "2020-11-12T03:26:23.302825Z"
        },
        "code_folding": [
          11
        ],
        "id": "dTQCccgF7BSP"
      },
      "source": [
        "'''\n",
        "# correctness confirmed\n",
        "torch.random.manual_seed(42)\n",
        "pff=PositionwiseFeedForward2(4,2,.1)\n",
        "a=torch.tensor([[1.0,0,0,1],[1.0,1,0,0]])\n",
        "pff(a)\n",
        "torch.random.manual_seed(42)\n",
        "pff=PositionwiseFeedForward(4,2,.1)\n",
        "a=torch.tensor([[1.0,0,0,1],[1.0,1,0,0]])\n",
        "pff(a)\n",
        "'''\n",
        "class PositionwiseFeedForward(nn.Module):\n",
        "    '''\n",
        "    A two-layer Feed-Forward-Network with residual layer norm.\n",
        "    '''\n",
        "    \n",
        "    def __init__(self, modelDim, feedforwardDim, dropout=0.1):\n",
        "        super(PositionwiseFeedForward, self).__init__()\n",
        "        self.linear1 = nn.Linear(modelDim, feedforwardDim)\n",
        "        self.linear2 = nn.Linear(feedforwardDim, modelDim)\n",
        "        self.layerNorm = nn.LayerNorm(modelDim, eps=1e-6)\n",
        "        # activation function\n",
        "        self.gelu = lambda x: \\\n",
        "                0.5 * x * (1 + torch.tanh(math.sqrt(2 / math.pi) * (x + 0.044715 * torch.pow(x, 3))))\n",
        "        self.dropout1 = nn.Dropout(dropout)\n",
        "        self.dropout2 = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        layerNorm = self.layerNorm(x)\n",
        "        linear1 = self.linear1(layerNorm)\n",
        "        gelu = self.gelu(linear1)\n",
        "        hidden = self.dropout1(gelu)\n",
        "        #hidden = self.dropout1(self.gelu(self.linear1(self.layerNorm(x))))\n",
        "        output = self.dropout2(self.linear2(hidden))\n",
        "        return output + x"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-11-12T02:09:23.576646Z",
          "start_time": "2020-11-12T02:09:23.531419Z"
        },
        "code_folding": [
          0
        ],
        "id": "7XU6hyt67BSS"
      },
      "source": [
        "def exampleOfMultiHeadedAttention():\n",
        "    torch.random.manual_seed(42)\n",
        "    m=MultiHeadedAttention(2,4)\n",
        "    a=torch.tensor([[1.0,2,3,1]])\n",
        "    b=torch.tensor([[1.0,0.5,0.3,1]])\n",
        "    c=torch.tensor([[1.0,0.5,0.7,1]])\n",
        "    d={'selfKeys': torch.tensor([[[[ 1.7056, -0.2705]],\\\n",
        "    [[ 0.8711,  1.2850]]]]), 'selfValues': torch.tensor([[[[ 0.0890, -0.4489]],\\\n",
        "    [[-0.4343,  0.5302]]]])}\n",
        "    #m(a,b,c,layerCache={\"memoryKeys\":None},types=\"context\")\n",
        "    m(a,b,c,layerCache=d,types=\"self\")\n",
        "    torch.random.manual_seed(42)\n",
        "    m=MultiHeadedAttention2(2,4)\n",
        "    a=torch.tensor([[1.0,2,3,1]])\n",
        "    b=torch.tensor([[1.0,0.5,0.3,1]])\n",
        "    c=torch.tensor([[1.0,0.5,0.7,1]])\n",
        "    d={'self_keys': torch.tensor([[[[ 1.7056, -0.2705]],\\\n",
        "    [[ 0.8711,  1.2850]]]]), 'self_values': torch.tensor([[[[ 0.0890, -0.4489]],\\\n",
        "    [[-0.4343,  0.5302]]]])}\n",
        "    m(a,b,c,layer_cache=d,type=\"self\")\n",
        "\n",
        "\n",
        "    \n",
        "    torch.random.manual_seed(42)\n",
        "    m=MultiHeadedAttention(2,4)\n",
        "    a=torch.tensor([[1.0,2,3,1]])\n",
        "    b=torch.tensor([[1.0,0.5,0.3,1]])\n",
        "    c=torch.tensor([[1.0,0.5,0.7,1]])\n",
        "    d={'memoryKeys': torch.tensor([[[[ 1.7056, -0.2705]],\\\n",
        "    [[ 0.8711,  1.2850]]]]), 'memoryValues': torch.tensor([[[[ 0.0890, -0.4489]],\\\n",
        "    [[-0.4343,  0.5302]]]])}\n",
        "    #m(a,b,c,layerCache={\"memoryKeys\":None},types=\"context\")\n",
        "    m(a,b,c,layerCache=d,types=\"context\")\n",
        "    torch.random.manual_seed(42)\n",
        "    m=MultiHeadedAttention2(2,4)\n",
        "    a=torch.tensor([[1.0,2,3,1]])\n",
        "    b=torch.tensor([[1.0,0.5,0.3,1]])\n",
        "    c=torch.tensor([[1.0,0.5,0.7,1]])\n",
        "    d={'memory_keys': torch.tensor([[[[ 1.7056, -0.2705]],\\\n",
        "    [[ 0.8711,  1.2850]]]]), 'memory_values': torch.tensor([[[[ 0.0890, -0.4489]],\\\n",
        "    [[-0.4343,  0.5302]]]])}\n",
        "    m(a,b,c,layer_cache=d,type=\"context\")\n",
        "    \n",
        "    \n",
        "    torch.random.manual_seed(42)\n",
        "    m=MultiHeadedAttention2(2,4)\n",
        "    a=torch.tensor([[1.0,2,3,1]])\n",
        "    b=torch.tensor([[1.0,0.5,0.3,1]])\n",
        "    c=torch.tensor([[1.0,0.5,0.7,1]])\n",
        "    d={'memory_keys': None, 'memory_values': None}\n",
        "    m(a,b,c,layer_cache=d,type=\"context\")\n",
        "    torch.random.manual_seed(42)\n",
        "    m=MultiHeadedAttention(2,4)\n",
        "    a=torch.tensor([[1.0,2,3,1]])\n",
        "    b=torch.tensor([[1.0,0.5,0.3,1]])\n",
        "    c=torch.tensor([[1.0,0.5,0.7,1]])\n",
        "    d={'memoryKeys': None, 'memoryValues': None}\n",
        "    m(a,b,c,layerCache=d,types=\"context\")\n",
        "    \n",
        "    \n",
        "    torch.random.manual_seed(42)\n",
        "    m=MultiHeadedAttention2(2,4)\n",
        "    a=torch.tensor([[1.0,2,3,1]])\n",
        "    b=torch.tensor([[1.0,0.5,0.3,1]])\n",
        "    c=torch.tensor([[1.0,0.5,0.7,1]])\n",
        "    d={'self_keys': None, 'self_values': None}\n",
        "    m(a,b,c,layer_cache=d,type=\"self\")\n",
        "    torch.random.manual_seed(42)\n",
        "    m=MultiHeadedAttention(2,4)\n",
        "    a=torch.tensor([[1.0,2,3,1]])\n",
        "    b=torch.tensor([[1.0,0.5,0.3,1]])\n",
        "    c=torch.tensor([[1.0,0.5,0.7,1]])\n",
        "    d={'selfKeys': None, 'selfValues': None}\n",
        "    m(a,b,c,layerCache=d,types=\"self\")\n",
        "    \n",
        "    torch.random.manual_seed(42)\n",
        "    m=MultiHeadedAttention2(2,4)\n",
        "    a=torch.tensor([[1.0,2,3,1]])\n",
        "    b=torch.tensor([[1.0,0.5,0.3,1]])\n",
        "    c=torch.tensor([[1.0,0.5,0.7,1]])\n",
        "    d={'self_keys': None, 'self_values': None}\n",
        "    torch.random.manual_seed(42)\n",
        "    m=MultiHeadedAttention(2,4)\n",
        "    a=torch.tensor([[1.0,2,3,1]])\n",
        "    b=torch.tensor([[1.0,0.5,0.3,1]])\n",
        "    c=torch.tensor([[1.0,0.5,0.7,1]])"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-11-12T03:32:48.277785Z",
          "start_time": "2020-11-12T03:32:48.196248Z"
        },
        "code_folding": [],
        "id": "C4ltuECc7BSW"
      },
      "source": [
        "# example above verified 5 cases\n",
        "\n",
        "class MultiHeadedAttention(nn.Module):\n",
        "    def __init__(self,numHeads, modelDim, dropout=0.1, isFinalLinear=True):\n",
        "        assert modelDim%numHeads == 0\n",
        "        self.dimPerHead =modelDim//numHeads \n",
        "        self.modelDim = modelDim\n",
        "        \n",
        "        super(MultiHeadedAttention,self).__init__()\n",
        "        self.numHeads = numHeads\n",
        "        self.linearKeys = nn.Linear(modelDim,numHeads*self.dimPerHead)\n",
        "        self.linearValues = nn.Linear(modelDim,numHeads*self.dimPerHead)\n",
        "        self.linearQuery = nn.Linear(modelDim,numHeads*self.dimPerHead)\n",
        "        self.softmax = nn.Softmax(dim=-1)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.isFinalLinear = isFinalLinear\n",
        "        if self.isFinalLinear:\n",
        "            self.finalLayer = nn.Linear(modelDim,modelDim)\n",
        "    def forward(self,key,value,query,mask = None,\n",
        "               layerCache = None, types = None):\n",
        "\n",
        "        batchSize = key.size(0) # size(0) is batch size for all three vectors\n",
        "        dimPerHead = self.dimPerHead\n",
        "        numHeads = self.numHeads\n",
        "\n",
        "        \n",
        "        shape = lambda x: x.view(batchSize,-1,numHeads,dimPerHead).transpose(1,2)\n",
        "        # why contiguous: https://discuss.pytorch.org/t/when-and-why-do-we-use-contiguous/47588\n",
        "        # we might not need it\n",
        "        # apparently in the old version, transpose only changes the view of data, but not data itself\n",
        "        # so to force it to change the data, use contiguous \n",
        "        unshape = lambda x: x.transpose(1,2).contiguous().view(batchSize,-1,numHeads*dimPerHead)\n",
        "        \n",
        "        # get key, value, and query\n",
        "        if layerCache is None:\n",
        "     \n",
        "            key = self.linearKeys(key)\n",
        "            value = self.linearValues(value)\n",
        "            query = self.linearQuery(query)\n",
        "            key = shape(key)\n",
        "            value = shape(value)\n",
        "            \n",
        "        else:\n",
        "  \n",
        "            # Note: this is different from the original code. the original code has \n",
        "            # if statement that is already tested, and else statements that will \n",
        "            # never get use\n",
        "            \n",
        "            # concatenate to variable key\" and \"value\" to their respective caches. \n",
        "            if types == \"self\":\n",
        "        \n",
        "                # all query??\n",
        "                key = self.linearKeys(query)\n",
        "                value = self.linearValues(query)\n",
        "                query = self.linearQuery(query)\n",
        "                key = shape(key)\n",
        "                value = shape(value)\n",
        "                device = key.device\n",
        "                \n",
        "                itemPairsToUpdate = [[key,\"selfKeys\"],[value,\"selfValues\"]]\n",
        "                \n",
        "                \n",
        "                for i in range(2):\n",
        "                    variable, variableName = itemPairsToUpdate[i]\n",
        "                    if layerCache[variableName] is not None:\n",
        "                        itemPairsToUpdate[i][0] = torch.cat((layerCache[variableName].to(device),variable), dim=2)\n",
        "                    layerCache[variableName] = itemPairsToUpdate[i][0]\n",
        "                key, value = itemPairsToUpdate[0][0],itemPairsToUpdate[1][0]\n",
        "                \n",
        "            elif types == \"context\":\n",
        "               \n",
        "                # if no cache, create the cache, \n",
        "                # else copy the cache to the variables \n",
        "                query = self.linearQuery(query)\n",
        "                if layerCache[\"memoryKeys\"] is None:\n",
        "                    # checked!\n",
        "                    key = self.linearKeys(key)\n",
        "                    value = self.linearValues(value)\n",
        "                    key = shape(key)\n",
        "                    value = shape(value)\n",
        "\n",
        "                else:\n",
        "                    key, value = layerCache[\"memoryKeys\"], layerCache[\"memoryValues\"]\n",
        "                layerCache[\"memoryKeys\"] = key\n",
        "                layerCache[\"memoryValues\"] = value\n",
        "\n",
        "        query = shape(query)\n",
        "        \n",
        "        '''\n",
        "        # possibly for debugging purpose\n",
        "        keyLength = key.size(2)\n",
        "        queryLength = query.size(2)\n",
        "        '''\n",
        "        \n",
        "        # compute and scale the scores\n",
        "        \n",
        "        # why sqrt?\n",
        "    \n",
        "        query = query / math.sqrt(dimPerHead)\n",
        "    \n",
        "        scores = torch.matmul(query, key.transpose(2, 3))\n",
        "        \n",
        "        if mask is not None:\n",
        "            mask = mask.unsqueeze(1).expand_as(scores)\n",
        "            scores = scores.masked_fill(mask,-1e18) # negative infinity \n",
        "            \n",
        "        # apply attention dropout and compute context vectors \n",
        "        attention = self.softmax(scores)\n",
        "        attentionDropout = self.dropout(attention)\n",
        "        \n",
        "        if self.isFinalLinear:\n",
        "            context = unshape(torch.matmul(attentionDropout,value))\n",
        "            output = self.finalLayer(context)\n",
        "            return output\n",
        "        else:\n",
        "            context = torch.matmul(attentionDropout,value)\n",
        "            return context\n"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-11-12T03:13:36.432307Z",
          "start_time": "2020-11-12T03:13:36.356931Z"
        },
        "code_folding": [
          0
        ],
        "id": "nwPpCA8U7BSZ"
      },
      "source": [
        "class MultiHeadedAttention2(nn.Module):\n",
        "    \"\"\"\n",
        "    Multi-Head Attention module from\n",
        "    \"Attention is All You Need\"\n",
        "    :cite:`DBLP:journals/corr/VaswaniSPUJGKP17`.\n",
        "    Similar to standard `dot` attention but uses\n",
        "    multiple attention distributions simulataneously\n",
        "    to select relevant items.\n",
        "    .. mermaid::\n",
        "       graph BT\n",
        "          A[key]\n",
        "          B[value]\n",
        "          C[query]\n",
        "          O[output]\n",
        "          subgraph Attn\n",
        "            D[Attn 1]\n",
        "            E[Attn 2]\n",
        "            F[Attn N]\n",
        "          end\n",
        "          A --> D\n",
        "          C --> D\n",
        "          A --> E\n",
        "          C --> E\n",
        "          A --> F\n",
        "          C --> F\n",
        "          D --> O\n",
        "          E --> O\n",
        "          F --> O\n",
        "          B --> O\n",
        "    Also includes several additional tricks.\n",
        "    Args:\n",
        "       head_count (int): number of parallel heads\n",
        "       model_dim (int): the dimension of keys/values/queries,\n",
        "           must be divisible by head_count\n",
        "       dropout (float): dropout parameter\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, head_count, model_dim, dropout=0.1, use_final_linear=True):\n",
        "        assert model_dim % head_count == 0\n",
        "        self.dim_per_head = model_dim // head_count\n",
        "        self.model_dim = model_dim\n",
        "\n",
        "        super(MultiHeadedAttention2, self).__init__()\n",
        "        self.head_count = head_count\n",
        "\n",
        "        self.linear_keys = nn.Linear(model_dim,\n",
        "                                     head_count * self.dim_per_head)\n",
        "        self.linear_values = nn.Linear(model_dim,\n",
        "                                       head_count * self.dim_per_head)\n",
        "        self.linear_query = nn.Linear(model_dim,\n",
        "                                      head_count * self.dim_per_head)\n",
        "        self.softmax = nn.Softmax(dim=-1)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.use_final_linear = use_final_linear\n",
        "        if (self.use_final_linear):\n",
        "            self.final_linear = nn.Linear(model_dim, model_dim)\n",
        "\n",
        "    def forward(self, key, value, query, mask=None,\n",
        "                layer_cache=None, type=None, predefined_graph_1=None):\n",
        "        \"\"\"\n",
        "        Compute the context vector and the attention vectors.\n",
        "        Args:\n",
        "           key (`FloatTensor`): set of `key_len`\n",
        "                key vectors `[batch, key_len, dim]`\n",
        "           value (`FloatTensor`): set of `key_len`\n",
        "                value vectors `[batch, key_len, dim]`\n",
        "           query (`FloatTensor`): set of `query_len`\n",
        "                 query vectors  `[batch, query_len, dim]`\n",
        "           mask: binary mask indicating which keys have\n",
        "                 non-zero attention `[batch, query_len, key_len]`\n",
        "        Returns:\n",
        "           (`FloatTensor`, `FloatTensor`) :\n",
        "           * output context vectors `[batch, query_len, dim]`\n",
        "           * one of the attention vectors `[batch, query_len, key_len]`\n",
        "        \"\"\"\n",
        "\n",
        "        # CHECKS\n",
        "        # batch, k_len, d = key.size()\n",
        "        # batch_, k_len_, d_ = value.size()\n",
        "        # aeq(batch, batch_)\n",
        "        # aeq(k_len, k_len_)\n",
        "        # aeq(d, d_)\n",
        "        # batch_, q_len, d_ = query.size()\n",
        "        # aeq(batch, batch_)\n",
        "        # aeq(d, d_)\n",
        "        # aeq(self.model_dim % 8, 0)\n",
        "        # if mask is not None:\n",
        "        #    batch_, q_len_, k_len_ = mask.size()\n",
        "        #    aeq(batch_, batch)\n",
        "        #    aeq(k_len_, k_len)\n",
        "        #    aeq(q_len_ == q_len)\n",
        "        # END CHECKS\n",
        "\n",
        "        batch_size = key.size(0)\n",
        "        dim_per_head = self.dim_per_head\n",
        "        head_count = self.head_count\n",
        "        key_len = key.size(1)\n",
        "        query_len = query.size(1)\n",
        "\n",
        "        def shape(x):\n",
        "            \"\"\"  projection \"\"\"\n",
        "            return x.view(batch_size, -1, head_count, dim_per_head) \\\n",
        "                .transpose(1, 2)\n",
        "\n",
        "        def unshape(x):\n",
        "            \"\"\"  compute context \"\"\"\n",
        "            return x.transpose(1, 2).contiguous() \\\n",
        "                .view(batch_size, -1, head_count * dim_per_head)\n",
        "\n",
        "        # 1) Project key, value, and query.\n",
        "        if layer_cache is not None:\n",
        "            if type == \"self\":\n",
        "                query, key, value = self.linear_query(query), \\\n",
        "                                    self.linear_keys(query), \\\n",
        "                                    self.linear_values(query)\n",
        "\n",
        "                key = shape(key)\n",
        "                value = shape(value)\n",
        "\n",
        "                if layer_cache is not None:\n",
        "                    device = key.device\n",
        "                    if layer_cache[\"self_keys\"] is not None:\n",
        "                        key = torch.cat(\n",
        "                            (layer_cache[\"self_keys\"].to(device), key),\n",
        "                            dim=2)\n",
        "                    if layer_cache[\"self_values\"] is not None:\n",
        "                        value = torch.cat(\n",
        "                            (layer_cache[\"self_values\"].to(device), value),\n",
        "                            dim=2)\n",
        "                    layer_cache[\"self_keys\"] = key\n",
        "                    layer_cache[\"self_values\"] = value\n",
        "            elif type == \"context\":\n",
        "                query = self.linear_query(query)\n",
        "                if layer_cache is not None:\n",
        "                    if layer_cache[\"memory_keys\"] is None:\n",
        "                        key, value = self.linear_keys(key), \\\n",
        "                                     self.linear_values(value)\n",
        "                        key = shape(key)\n",
        "                        value = shape(value)\n",
        "                    else:\n",
        "                        key, value = layer_cache[\"memory_keys\"], \\\n",
        "                                     layer_cache[\"memory_values\"]\n",
        "                    layer_cache[\"memory_keys\"] = key\n",
        "                    layer_cache[\"memory_values\"] = value\n",
        "                else:\n",
        "                    key, value = self.linear_keys(key), \\\n",
        "                                 self.linear_values(value)\n",
        "                    key = shape(key)\n",
        "                    value = shape(value)\n",
        "        else:\n",
        "            key = self.linear_keys(key)\n",
        "            value = self.linear_values(value)\n",
        "            query = self.linear_query(query)\n",
        "            key = shape(key)\n",
        "            value = shape(value)\n",
        "\n",
        "        query = shape(query)\n",
        "\n",
        "        key_len = key.size(2)\n",
        "        query_len = query.size(2)\n",
        "\n",
        "        # 2) Calculate and scale scores.\n",
        "        query = query / math.sqrt(dim_per_head)\n",
        "        scores = torch.matmul(query, key.transpose(2, 3))\n",
        "\n",
        "        if mask is not None:\n",
        "            mask = mask.unsqueeze(1).expand_as(scores)\n",
        "            \n",
        "            scores = scores.masked_fill(mask, -1e18)\n",
        "\n",
        "        # 3) Apply attention dropout and compute context vectors.\n",
        "\n",
        "        attn = self.softmax(scores)\n",
        "\n",
        "        if (not predefined_graph_1 is None):\n",
        "            attn_masked = attn[:, -1] * predefined_graph_1\n",
        "            attn_masked = attn_masked / (torch.sum(attn_masked, 2).unsqueeze(2) + 1e-9)\n",
        "\n",
        "            attn = torch.cat([attn[:, :-1], attn_masked.unsqueeze(1)], 1)\n",
        "\n",
        "        drop_attn = self.dropout(attn)\n",
        "        if (self.use_final_linear):\n",
        "            context = unshape(torch.matmul(drop_attn, value))\n",
        "            output = self.final_linear(context)\n",
        "            return output\n",
        "        else:\n",
        "            context = torch.matmul(drop_attn, value)\n",
        "            return context"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-11-12T03:13:36.609131Z",
          "start_time": "2020-11-12T03:13:36.588054Z"
        },
        "code_folding": [
          0
        ],
        "id": "zZr1rkq-7BSc"
      },
      "source": [
        "class PositionwiseFeedForward2(nn.Module):\n",
        "    \"\"\" A two-layer Feed-Forward-Network with residual layer norm.\n",
        "    Args:\n",
        "        d_model (int): the size of input for the first-layer of the FFN.\n",
        "        d_ff (int): the hidden layer size of the second-layer\n",
        "            of the FNN.\n",
        "        dropout (float): dropout probability in :math:`[0, 1)`.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
        "        super(PositionwiseFeedForward2, self).__init__()\n",
        "        self.w_1 = nn.Linear(d_model, d_ff)\n",
        "        self.w_2 = nn.Linear(d_ff, d_model)\n",
        "        self.layer_norm = nn.LayerNorm(d_model, eps=1e-6)\n",
        "        self.actv = gelu\n",
        "        self.dropout_1 = nn.Dropout(dropout)\n",
        "        self.dropout_2 = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        inter = self.dropout_1(self.actv(self.w_1(self.layer_norm(x))))\n",
        "        output = self.dropout_2(self.w_2(inter))\n",
        "        return output + x\n",
        "def gelu(x):\n",
        "    return 0.5 * x * (1 + torch.tanh(math.sqrt(2 / math.pi) * (x + 0.044715 * torch.pow(x, 3))))"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-11-12T03:21:13.342757Z",
          "start_time": "2020-11-12T03:21:13.324267Z"
        },
        "code_folding": [],
        "id": "fcWb4Cxs7BSg"
      },
      "source": [
        "'''\n",
        "torch.random.manual_seed(42)\n",
        "t = TransformerEncoderLayer2(4,2,2,.1)\n",
        "a=torch.tensor([[1.0,2,3,1],[1.0,2,3,1]])\n",
        "b=torch.tensor([[1.0,0.5,0.3,1],[1.0,0.5,0.3,1]])\n",
        "c=torch.tensor([[1.0,0.5,0.7,1]])\n",
        "d=torch.tensor([[True]])\n",
        "t(0,a,b,d)\n",
        "torch.random.manual_seed(42)\n",
        "t = TransformerEncoderLayer(2,4,2,.1)\n",
        "a=torch.tensor([[1.0,2,3,1],[1.0,2,3,1]])\n",
        "b=torch.tensor([[1.0,0.5,0.3,1],[1.0,0.5,0.3,1]])\n",
        "c=torch.tensor([[1.0,0.5,0.7,1]])\n",
        "d=torch.tensor([[True]])\n",
        "t(0,a,b,d)\n",
        "'''\n",
        "class TransformerEncoderLayer(nn.Module):\n",
        "    def __init__(self, numHeads, modelDim, feedForwardDim, dropout):\n",
        "        super(TransformerEncoderLayer, self).__init__()\n",
        "        self.selfAttention = MultiHeadedAttention(numHeads,modelDim,dropout=dropout)\n",
        "        self.feedForward = PositionwiseFeedForward(modelDim,feedForwardDim,dropout)\n",
        "        # output = (input - Expectation[input])/sqrt(variance(input)+eps)*gamma+beta\n",
        "        # gamma and beta learnable \n",
        "        # normalizing over the last dim\n",
        "        self.layerNorm = nn.LayerNorm(modelDim,eps=1e-6)\n",
        "        #dropout is probability of dropout\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "    def forward(self,iteration,query,inputs,mask):\n",
        "        # if it is not the first iteration, then normalize the layer \n",
        "        inputsNorm = self.layerNorm(inputs) if iteration != 0 else inputs \n",
        "        # keep dim 1 a singleton\n",
        "        mask=mask.unsqueeze(1)\n",
        "        # why?\n",
        "        contextEncoding = self.selfAttention(inputsNorm,inputsNorm,inputsNorm,mask=mask)\n",
        "        # why do we add input back?\n",
        "        contextEncodingWDropout = self.dropout(contextEncoding) + inputs \n",
        "        return self.feedForward(contextEncodingWDropout)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-11-12T03:25:37.255424Z",
          "start_time": "2020-11-12T03:25:37.220106Z"
        },
        "code_folding": [
          0
        ],
        "id": "kLtHJlEL7BSi"
      },
      "source": [
        "class PositionalEncoding2(nn.Module):\n",
        "\n",
        "    def __init__(self, dropout, dim, max_len=5000):\n",
        "        pe = torch.zeros(max_len, dim)\n",
        "        position = torch.arange(0, max_len).unsqueeze(1)\n",
        "        div_term = torch.exp((torch.arange(0, dim, 2, dtype=torch.float) *\n",
        "                              -(math.log(10000.0) / dim)))\n",
        "        pe[:, 0::2] = torch.sin(position.float() * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position.float() * div_term)\n",
        "        pe = pe.unsqueeze(0)\n",
        "        super(PositionalEncoding2, self).__init__()\n",
        "        self.register_buffer('pe', pe)\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "        self.dim = dim\n",
        "    def forward(self, emb, step=None):\n",
        "        emb = emb * math.sqrt(self.dim)\n",
        "        if (step):\n",
        "            emb = emb + self.pe[:, step][:, None, :]\n",
        "\n",
        "        else:\n",
        "            emb = emb + self.pe[:, :emb.size(1)]\n",
        "        emb = self.dropout(emb)\n",
        "        return emb\n",
        "\n",
        "    def get_emb(self, emb):\n",
        "        return self.pe[:, :emb.size(1)]\n"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-11-12T03:34:34.456069Z",
          "start_time": "2020-11-12T03:34:34.384751Z"
        },
        "id": "Tvqeo4oG7BSo",
        "outputId": "11ead097-0950-4e2f-ed6b-dcf26980b7a0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "torch.random.manual_seed(42)\n",
        "t = InterSentencesTransformerEncoderLayer(4,2,2,.1,1)\n",
        "a=torch.tensor([[[1.0,2,3,1],[1.0,2,3,1]]])\n",
        "d=torch.tensor([[True]])\n",
        "t(a,d)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[[-1.3448,  0.3613,  1.3798, -0.3963],\n",
            "         [-1.2437, -0.6751,  1.2393,  0.6795]]],\n",
            "       grad_fn=<NativeLayerNormBackward>) tensor([[[ 0.4179,  0.2501],\n",
            "         [ 0.4903, -0.0544]]], grad_fn=<AddBackward0>) tensor([[[ 0.2767,  0.1498],\n",
            "         [ 0.3374, -0.0260]]], grad_fn=<MulBackward0>) tensor([[[ 0.3074,  0.1664],\n",
            "         [ 0.3749, -0.0289]]], grad_fn=<MulBackward0>)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.4031, 0.3847]], grad_fn=<MulBackward0>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 115
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-11-12T03:34:50.740868Z",
          "start_time": "2020-11-12T03:34:50.704484Z"
        },
        "id": "YN-DRvaO7BSt"
      },
      "source": [
        "#inter-sentence transformer focuses on learning relationship between sentences to produce a document level summary\n",
        "# the input to this transformer is output of Bert, which can be viewed as contextual encoding. \n",
        "# modelDim is the output of the Bert's hidden size, or Bert's transformer's model dimension size \n",
        "# (model dimension size is the word used in the self hidden is all you need)\n",
        "class InterSentencesTransformerEncoderLayer(nn.Module):\n",
        "    def __init__(self, modelDim, feedforwardDim, numHeads, dropout,numInterSentencesLayers=0):\n",
        "        super(InterSentencesTransformerEncoderLayer, self).__init__()\n",
        "        self.modelDim = modelDim\n",
        "        self.numInterSentencesLayers =  numInterSentencesLayers\n",
        "        self.positionalEmbedding = PositionalEncoding(modelDim,dropout)\n",
        "        self.interSentencesTransformers = nn.ModuleList(\n",
        "            [TransformerEncoderLayer(numHeads, modelDim, feedforwardDim, dropout)\n",
        "             for _ in range(numInterSentencesLayers)])\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.layerNorm = nn.LayerNorm(modelDim, eps=1e-6)\n",
        "        self.linearLayer = nn.Linear(modelDim, 1, bias=True)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "    # top_vector in the original code: top vectors? topic vectors?\n",
        "    # topicVectors is the output of Bert, some sort of contextual embedding\n",
        "    # we will use contextualEncoding instead of top_vector\n",
        "    def forward(self,contextualEncoding,mask):\n",
        "        batchSize, nSentences = contextualEncoding.size(0), contextualEncoding.size(1)\n",
        "        positionalEmbedding = self.positionalEmbedding.positionalEncoding[:,:nSentences]\n",
        "        # mask takes [:,:,None] to account for batches?\n",
        "        # x will be the contextualEncoding undergoing transformer operations\n",
        "        x = contextualEncoding * mask[:,:,None].float() + positionalEmbedding\n",
        "        for i in range(self.numInterSentencesLayers):\n",
        "            x = self.interSentencesTransformers[i](i,x,x,~mask)\n",
        "        x = self.layerNorm(x)\n",
        "        sentencesScores = self.sigmoid(self.linearLayer(x))\n",
        "        sentencesScores = sentencesScores.squeeze(-1)*mask.float()\n",
        "        return sentencesScores"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-11-12T03:36:00.375607Z",
          "start_time": "2020-11-12T03:36:00.354611Z"
        },
        "scrolled": true,
        "id": "Mq3EJYzz7BSw",
        "outputId": "e3a79e45-a113-466c-b448-674d4a05e5c7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "t=InterSentencesTransformerEncoderLayer(2,4,2,.1,1)\n",
        "print(t)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "InterSentencesTransformerEncoderLayer(\n",
            "  (positionalEmbedding): PositionalEncoding(\n",
            "    (dropout): Dropout(p=0.1, inplace=False)\n",
            "  )\n",
            "  (interSentencesTransformers): ModuleList(\n",
            "    (0): TransformerEncoderLayer(\n",
            "      (selfAttention): MultiHeadedAttention(\n",
            "        (linearKeys): Linear(in_features=2, out_features=2, bias=True)\n",
            "        (linearValues): Linear(in_features=2, out_features=2, bias=True)\n",
            "        (linearQuery): Linear(in_features=2, out_features=2, bias=True)\n",
            "        (softmax): Softmax(dim=-1)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "        (finalLayer): Linear(in_features=2, out_features=2, bias=True)\n",
            "      )\n",
            "      (feedForward): PositionwiseFeedForward(\n",
            "        (linear1): Linear(in_features=2, out_features=4, bias=True)\n",
            "        (linear2): Linear(in_features=4, out_features=2, bias=True)\n",
            "        (layerNorm): LayerNorm((2,), eps=1e-06, elementwise_affine=True)\n",
            "        (dropout1): Dropout(p=0.1, inplace=False)\n",
            "        (dropout2): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "      (layerNorm): LayerNorm((2,), eps=1e-06, elementwise_affine=True)\n",
            "      (dropout): Dropout(p=0.1, inplace=False)\n",
            "    )\n",
            "  )\n",
            "  (dropout): Dropout(p=0.1, inplace=False)\n",
            "  (layerNorm): LayerNorm((2,), eps=1e-06, elementwise_affine=True)\n",
            "  (linearLayer): Linear(in_features=2, out_features=1, bias=True)\n",
            "  (sigmoid): Sigmoid()\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4nwbBhuh7BS6"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hHMovbIF7BTD"
      },
      "source": [
        "def build_optim(args, model, checkpoint):\n",
        "    \"\"\" Build optimizer \"\"\"\n",
        "    saved_optimizer_state_dict = None\n",
        "    optim = None\n",
        "\n",
        "    if args[\"train_from\"] != '':\n",
        "        print(\"We made a checkpoint\")\n",
        "        optim = checkpoint['optim']\n",
        "        saved_optimizer_state_dict = optim.optimizer.state_dict()\n",
        "    else:\n",
        "        print(\"we created an optimizer\")\n",
        "        optim = Optimizer(\n",
        "            args[\"optim\"], args[\"lr\"], args[\"max_grad_norm\"],\n",
        "            beta1=args[\"beta1\"], beta2=args[\"beta2\"],\n",
        "            decay_method=args[\"decay_method\"],\n",
        "            warmup_steps=args[\"warmup_steps\"])\n",
        "\n",
        "    optim.set_parameters(list(model.named_parameters()))\n",
        "\n",
        "    if args[\"train_from\"] != '':\n",
        "        optim.optimizer.load_state_dict(saved_optimizer_state_dict)\n",
        "        if args[\"visible_gpus\"] != '-1':\n",
        "            for state in optim.optimizer.state.values():\n",
        "                for k, v in state.items():\n",
        "                    if torch.is_tensor(v):\n",
        "                        state[k] = v.cuda()\n",
        "\n",
        "        if (optim.method == 'adam') and (len(optim.optimizer.state) < 1):\n",
        "            raise RuntimeError(\n",
        "                \"Error: loaded Adam optimizer from existing model\" +\n",
        "                \" but optimizer state is empty\")\n",
        "\n",
        "    return optim"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S4cqr6Eg7BTG"
      },
      "source": [
        "from pytorch_pretrained_bert import BertModel, BertConfig\n",
        "\n",
        "class Bert(nn.Module):\n",
        "    def __init__(self, temp_dir, load_pretrained_bert, bert_config):\n",
        "        super(Bert, self).__init__()\n",
        "        if(load_pretrained_bert):\n",
        "            self.model = BertModel.from_pretrained('bert-base-uncased', cache_dir=temp_dir)\n",
        "        else:\n",
        "            self.model = BertModel(bert_config)\n",
        "\n",
        "    def forward(self, x, segs, mask):\n",
        "        encoded_layers, _ = self.model(x, segs, attention_mask =mask)\n",
        "        contextualEncoding = encoded_layers[-1]\n",
        "        return contextualEncoding\n",
        "class Summarizer(nn.Module):\n",
        "    def __init__(self, args, device, load_pretrained_bert = False, bert_config = None):\n",
        "        super(Summarizer, self).__init__()\n",
        "        self.args = args\n",
        "        self.device = device\n",
        "        self.bert = Bert(args[\"temp_dir\"], load_pretrained_bert, bert_config)\n",
        "        self.encoder = InterSentencesTransformerEncoderLayer(self.bert.model.config.hidden_size, args[\"ff_size\"], args[\"heads\"],\n",
        "                                                   args[\"dropout\"], args[\"inter_layers\"])\n",
        "        if args[\"param_init\"] != 0.0:\n",
        "            for p in self.encoder.parameters():\n",
        "                p.data.uniform_(-args[\"param_init\"], args[\"param_init\"])\n",
        "        if args[\"param_init_glorot\"]:\n",
        "            for p in self.encoder.parameters():\n",
        "                if p.dim() > 1:\n",
        "                    xavier_uniform_(p)\n",
        "\n",
        "        self.to(device)\n",
        "    def load_cp(self, pt):\n",
        "        self.load_state_dict(pt['model'], strict=True)\n",
        "\n",
        "    def forward(self, x, segs, clss, mask, mask_cls, sentence_range=None):\n",
        "\n",
        "        contextualEncoding = self.bert(x, segs, mask)\n",
        "        sents_vec = contextualEncoding[torch.arange(contextualEncoding.size(0)).unsqueeze(1), clss]\n",
        "        sents_vec = sents_vec * mask_cls[:, :, None].float()\n",
        "        sent_scores = self.encoder(sents_vec, mask_cls).squeeze(-1)\n",
        "        return sent_scores, mask_cls"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aq4XplGA7BTM"
      },
      "source": [
        "# Data Builder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m2EAJVQK7BTN"
      },
      "source": [
        "def cal_rouge(evaluated_ngrams, reference_ngrams):\n",
        "    reference_count = len(reference_ngrams)\n",
        "    evaluated_count = len(evaluated_ngrams)\n",
        "\n",
        "    overlapping_ngrams = evaluated_ngrams.intersection(reference_ngrams)\n",
        "    overlapping_count = len(overlapping_ngrams)\n",
        "\n",
        "    if evaluated_count == 0:\n",
        "        precision = 0.0\n",
        "    else:\n",
        "        precision = overlapping_count / evaluated_count\n",
        "\n",
        "    if reference_count == 0:\n",
        "        recall = 0.0\n",
        "    else:\n",
        "        recall = overlapping_count / reference_count\n",
        "\n",
        "    f1_score = 2.0 * ((precision * recall) / (precision + recall + 1e-8))\n",
        "    return {\"f\": f1_score, \"p\": precision, \"r\": recall}\n",
        "\n",
        "def greedy_selection(doc_sent_list, abstract_sent_list, summary_size):\n",
        "    def _rouge_clean(s):\n",
        "        return re.sub(r'[^a-zA-Z0-9 ]', '', s)\n",
        "\n",
        "    max_rouge = 0.0\n",
        "    abstract = sum(abstract_sent_list, [])\n",
        "    abstract = _rouge_clean(' '.join(abstract)).split()\n",
        "    sents = [_rouge_clean(' '.join(s)).split() for s in doc_sent_list]\n",
        "    evaluated_1grams = [_get_word_ngrams(1, [sent]) for sent in sents]\n",
        "    reference_1grams = _get_word_ngrams(1, [abstract])\n",
        "    evaluated_2grams = [_get_word_ngrams(2, [sent]) for sent in sents]\n",
        "    reference_2grams = _get_word_ngrams(2, [abstract])\n",
        "\n",
        "    selected = []\n",
        "    for s in range(summary_size):\n",
        "        cur_max_rouge = max_rouge\n",
        "        cur_id = -1\n",
        "        for i in range(len(sents)):\n",
        "            if (i in selected):\n",
        "                continue\n",
        "            c = selected + [i]\n",
        "            candidates_1 = [evaluated_1grams[idx] for idx in c]\n",
        "            candidates_1 = set.union(*map(set, candidates_1))\n",
        "            candidates_2 = [evaluated_2grams[idx] for idx in c]\n",
        "            candidates_2 = set.union(*map(set, candidates_2))\n",
        "            rouge_1 = cal_rouge(candidates_1, reference_1grams)['f']\n",
        "            rouge_2 = cal_rouge(candidates_2, reference_2grams)['f']\n",
        "            rouge_score = rouge_1 + rouge_2\n",
        "            if rouge_score > cur_max_rouge:\n",
        "                cur_max_rouge = rouge_score\n",
        "                cur_id = i\n",
        "        if (cur_id == -1):\n",
        "            return selected\n",
        "        selected.append(cur_id)\n",
        "        max_rouge = cur_max_rouge\n",
        "\n",
        "    return sorted(selected)\n",
        "\n",
        "def combination_selection(doc_sent_list, abstract_sent_list, summary_size):\n",
        "    def _rouge_clean(s):\n",
        "        return re.sub(r'[^a-zA-Z0-9 ]', '', s)\n",
        "\n",
        "    max_rouge = 0.0\n",
        "    max_idx = (0, 0)\n",
        "    abstract = sum(abstract_sent_list, [])\n",
        "    abstract = _rouge_clean(' '.join(abstract)).split()\n",
        "    sents = [_rouge_clean(' '.join(s)).split() for s in doc_sent_list]\n",
        "    evaluated_1grams = [_get_word_ngrams(1, [sent]) for sent in sents]\n",
        "    reference_1grams = _get_word_ngrams(1, [abstract])\n",
        "    evaluated_2grams = [_get_word_ngrams(2, [sent]) for sent in sents]\n",
        "    reference_2grams = _get_word_ngrams(2, [abstract])\n",
        "\n",
        "    impossible_sents = []\n",
        "    for s in range(summary_size + 1):\n",
        "        combinations = itertools.combinations([i for i in range(len(sents)) if i not in impossible_sents], s + 1)\n",
        "        for c in combinations:\n",
        "            candidates_1 = [evaluated_1grams[idx] for idx in c]\n",
        "            candidates_1 = set.union(*map(set, candidates_1))\n",
        "            candidates_2 = [evaluated_2grams[idx] for idx in c]\n",
        "            candidates_2 = set.union(*map(set, candidates_2))\n",
        "            rouge_1 = cal_rouge(candidates_1, reference_1grams)['f']\n",
        "            rouge_2 = cal_rouge(candidates_2, reference_2grams)['f']\n",
        "\n",
        "            rouge_score = rouge_1 + rouge_2\n",
        "            if (s == 0 and rouge_score == 0):\n",
        "                impossible_sents.append(c[0])\n",
        "            if rouge_score > max_rouge:\n",
        "                max_idx = c\n",
        "                max_rouge = rouge_score\n",
        "    return sorted(list(max_idx))"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gioY0r7w7BTQ"
      },
      "source": [
        "# Data Loader / BATCH\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X51AdEJG7BTQ"
      },
      "source": [
        "\n",
        "class Batch(object):\n",
        "    def _pad(self, data, pad_id, width=-1):\n",
        "        if (width == -1):\n",
        "            width = max(len(d) for d in data)\n",
        "        rtn_data = [d + [pad_id] * (width - len(d)) for d in data]\n",
        "        return rtn_data\n",
        "\n",
        "    def __init__(self, data=None, device=None,  is_test=False):\n",
        "        \"\"\"Create a Batch from a list of examples.\"\"\"\n",
        "        if data is not None and data != []:\n",
        "            self.bad_batch = False\n",
        "            self.batch_size = len(data)\n",
        "            pre_src = [x[0] for x in data]\n",
        "            pre_labels = [x[1] for x in data]\n",
        "            pre_segs = [x[2] for x in data]\n",
        "            pre_clss = [x[3] for x in data]\n",
        "\n",
        "            src = torch.tensor(self._pad(pre_src, 0))\n",
        "\n",
        "            labels = torch.tensor(self._pad(pre_labels, 0))\n",
        "            segs = torch.tensor(self._pad(pre_segs, 0))\n",
        "            mask = ~(src == 0)\n",
        "\n",
        "            clss = torch.tensor(self._pad(pre_clss, -1))\n",
        "            mask_cls = ~(clss == -1)\n",
        "            clss[clss == -1] = 0\n",
        "\n",
        "            setattr(self, 'clss', clss.to(device))\n",
        "            setattr(self, 'mask_cls', mask_cls.to(device))\n",
        "            setattr(self, 'src', src.to(device))\n",
        "            setattr(self, 'labels', labels.to(device))\n",
        "            setattr(self, 'segs', segs.to(device))\n",
        "            setattr(self, 'mask', mask.to(device))\n",
        "\n",
        "            if (is_test):\n",
        "                src_str = [x[-2] for x in data]\n",
        "                setattr(self, 'src_str', src_str)\n",
        "                tgt_str = [x[-1] for x in data]\n",
        "                setattr(self, 'tgt_str', tgt_str)\n",
        "        else:\n",
        "          self.bad_batch = True\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.batch_size\n",
        "\n",
        "\n",
        "def batch(data, batch_size):\n",
        "    \"\"\"Yield elements from data in chunks of batch_size.\"\"\"\n",
        "    minibatch, size_so_far = [], 0\n",
        "    for ex in data:\n",
        "        minibatch.append(ex)\n",
        "        size_so_far = simple_batch_size_fn(ex, len(minibatch))\n",
        "        if size_so_far == batch_size:\n",
        "            yield minibatch\n",
        "            minibatch, size_so_far = [], 0\n",
        "        elif size_so_far > batch_size:\n",
        "            yield minibatch[:-1]\n",
        "            minibatch, size_so_far = minibatch[-1:], simple_batch_size_fn(ex, 1)\n",
        "    if minibatch:\n",
        "        yield minibatch\n",
        "\n",
        "\n",
        "def load_dataset(args, corpus_type, shuffle, bert_data_path=\"bert_data/\", pre=\"cnndm\"):\n",
        "    \"\"\"\n",
        "    Dataset generator. Don't do extra stuff here, like printing,\n",
        "    because they will be postponed to the first loading time.\n",
        "    Args:\n",
        "        corpus_type: 'train' or 'valid'\n",
        "    Returns:\n",
        "        A list of dataset, the dataset(s) are lazily loaded.\n",
        "    \"\"\"\n",
        "    assert corpus_type in [\"train\", \"valid\", \"test\"]\n",
        "\n",
        "    def _lazy_dataset_loader(pt_file, corpus_type):\n",
        "        dataset = torch.load(pt_file)\n",
        "        print('Loading %s dataset from %s, number of examples: %d' %\n",
        "                    (corpus_type, pt_file, len(dataset)))\n",
        "        return dataset\n",
        "\n",
        "    # Sort the glob output by file name (by increasing indexes).\n",
        "    allFiles = os.listdir(bert_data_path)\n",
        "    pts = [bert_data_path+file for file in allFiles if file[-3:] == '.pt']\n",
        "    if pts:\n",
        "        if (shuffle):\n",
        "            random.shuffle(pts)\n",
        "\n",
        "        for pt in pts:\n",
        "            print(pt)\n",
        "            yield _lazy_dataset_loader(pt, corpus_type)\n",
        "    else:\n",
        "        # Only one inputters.*Dataset, simple!\n",
        "        pt = bert_data_path + pre + '.' + corpus_type + '.pt'\n",
        "        yield _lazy_dataset_loader(pt, corpus_type)\n",
        "\n",
        "\n",
        "def simple_batch_size_fn(new, count):\n",
        "    src, labels = new[0], new[1]\n",
        "    global max_n_sents, max_n_tokens, max_size\n",
        "    if count == 1:\n",
        "        max_size = 0\n",
        "        max_n_sents=0\n",
        "        max_n_tokens=0\n",
        "    max_n_sents = max(max_n_sents, len(src))\n",
        "    max_size = max(max_size, max_n_sents)\n",
        "    src_elements = count * max_size\n",
        "    return src_elements"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LEqHX-IQ7BTT"
      },
      "source": [
        "# Data Loader Class + Iterators"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TyAjWcqd7BTT"
      },
      "source": [
        "class Dataloader(object):\n",
        "    def __init__(self, args, datasets,  batch_size,\n",
        "                 device, shuffle, is_test):\n",
        "        self.args = args\n",
        "        self.datasets = datasets\n",
        "        self.batch_size = batch_size\n",
        "        self.device = device\n",
        "        self.shuffle = shuffle\n",
        "        self.is_test = is_test\n",
        "        self.cur_iter = self._next_dataset_iterator(datasets)\n",
        "\n",
        "        assert self.cur_iter is not None\n",
        "\n",
        "    def __iter__(self):\n",
        "        dataset_iter = (d for d in self.datasets)\n",
        "        while self.cur_iter is not None:\n",
        "            for batch in self.cur_iter:\n",
        "                yield batch\n",
        "            self.cur_iter = self._next_dataset_iterator(dataset_iter)\n",
        "\n",
        "\n",
        "    def _next_dataset_iterator(self, dataset_iter):\n",
        "        try:\n",
        "            # Drop the current dataset for decreasing memory\n",
        "            if hasattr(self, \"cur_dataset\"):\n",
        "                self.cur_dataset = None\n",
        "                gc.collect()\n",
        "                del self.cur_dataset\n",
        "                gc.collect()\n",
        "\n",
        "            self.cur_dataset = next(dataset_iter)\n",
        "        except StopIteration:\n",
        "            return None\n",
        "\n",
        "        return DataIterator(args = self.args,\n",
        "            dataset=self.cur_dataset,  batch_size=self.batch_size,\n",
        "            device=self.device, shuffle=self.shuffle, is_test=self.is_test)\n",
        "\n",
        "\n",
        "class DataIterator(object):\n",
        "    def __init__(self, args, dataset,  batch_size,  device=None, is_test=False,\n",
        "                 shuffle=True):\n",
        "        self.args = args\n",
        "        self.batch_size, self.is_test, self.dataset = batch_size, is_test, dataset\n",
        "        self.iterations = 0\n",
        "        self.device = device\n",
        "        self.shuffle = shuffle\n",
        "\n",
        "        self.sort_key = lambda x: len(x[1])\n",
        "\n",
        "        self._iterations_this_epoch = 0\n",
        "\n",
        "    def data(self):\n",
        "        if self.shuffle:\n",
        "            random.shuffle(self.dataset)\n",
        "        xs = self.dataset\n",
        "        return xs\n",
        "\n",
        "\n",
        "    def preprocess(self, ex, is_test):\n",
        "        src = ex['src']\n",
        "        if('labels' in ex):\n",
        "            labels = ex['labels']\n",
        "        else:\n",
        "            labels = ex['src_sent_labels']\n",
        "\n",
        "        segs = ex['segs']\n",
        "        if(not self.args[\"use_interval\"]):\n",
        "            segs=[0]*len(segs)\n",
        "        clss = ex['clss']\n",
        "        src_txt = ex['src_txt']\n",
        "        tgt_txt = ex['tgt_txt']\n",
        "\n",
        "        if(is_test):\n",
        "            return src,labels,segs, clss, src_txt, tgt_txt\n",
        "        else:\n",
        "            return src,labels,segs, clss\n",
        "\n",
        "    def batch_buffer(self, data, batch_size):\n",
        "        minibatch, size_so_far = [], 0\n",
        "        for ex in data:\n",
        "            if(len(ex['src'])==0):\n",
        "                continue\n",
        "            ex = self.preprocess(ex, self.is_test)\n",
        "            if(ex is None):\n",
        "                continue\n",
        "            minibatch.append(ex)\n",
        "            size_so_far = simple_batch_size_fn(ex, len(minibatch))\n",
        "            if size_so_far == batch_size:\n",
        "                yield minibatch\n",
        "                minibatch, size_so_far = [], 0\n",
        "            elif size_so_far > batch_size:\n",
        "                yield minibatch[:-1]\n",
        "                minibatch, size_so_far = minibatch[-1:], simple_batch_size_fn(ex, 1)\n",
        "        if minibatch:\n",
        "            yield minibatch\n",
        "\n",
        "    def create_batches(self):\n",
        "        \"\"\" Create batches \"\"\"\n",
        "        data = self.data()\n",
        "        for buffer in self.batch_buffer(data, self.batch_size * 50):\n",
        "\n",
        "            p_batch = sorted(buffer, key=lambda x: len(x[3]))\n",
        "            p_batch = batch(p_batch, self.batch_size)\n",
        "\n",
        "            p_batch = list(p_batch)\n",
        "            if (self.shuffle):\n",
        "                random.shuffle(p_batch)\n",
        "            for b in p_batch:\n",
        "                yield b\n",
        "\n",
        "    def __iter__(self):\n",
        "        while True:\n",
        "            self.batches = self.create_batches()\n",
        "            for idx, minibatch in enumerate(self.batches):\n",
        "                # fast-forward if loaded from state\n",
        "                if self._iterations_this_epoch > idx:\n",
        "                    continue\n",
        "                self.iterations += 1\n",
        "                self._iterations_this_epoch += 1\n",
        "                batch = Batch(minibatch, self.device, self.is_test)\n",
        "\n",
        "                yield batch\n",
        "            return"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O4lAWRGL7BTW"
      },
      "source": [
        "# Optimizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tKe2tnfV7BTX"
      },
      "source": [
        "import torch.optim as optim\n",
        "from torch.nn.utils import clip_grad_norm_\n",
        "\n",
        "\"\"\" Optimizers class \"\"\"\n",
        "def use_gpu(opt):\n",
        "    \"\"\"\n",
        "    Creates a boolean if gpu used\n",
        "    \"\"\"\n",
        "    return (hasattr(opt, 'gpu_ranks') and len(opt.gpu_ranks) > 0) or \\\n",
        "           (hasattr(opt, 'gpu') and opt.gpu > -1)\n",
        "\n",
        "def build_optim2(model, opt, checkpoint):\n",
        "    \"\"\" Build optimizer \"\"\"\n",
        "    saved_optimizer_state_dict = None\n",
        "\n",
        "    if opt.train_from:\n",
        "        optim = checkpoint['optim']\n",
        "        # We need to save a copy of optim.optimizer.state_dict() for setting\n",
        "        # the, optimizer state later on in Stage 2 in this method, since\n",
        "        # the method optim.set_parameters(model.parameters()) will overwrite\n",
        "        # optim.optimizer, and with ith the values stored in\n",
        "        # optim.optimizer.state_dict()\n",
        "        saved_optimizer_state_dict = optim.optimizer.state_dict()\n",
        "    else:\n",
        "        optim = Optimizer(\n",
        "            opt.optim, opt.learning_rate, opt.max_grad_norm,\n",
        "            lr_decay=opt.learning_rate_decay,\n",
        "            start_decay_steps=opt.start_decay_steps,\n",
        "            decay_steps=opt.decay_steps,\n",
        "            beta1=opt.adam_beta1,\n",
        "            beta2=opt.adam_beta2,\n",
        "            adagrad_accum=opt.adagrad_accumulator_init,\n",
        "            decay_method=opt.decay_method,\n",
        "            warmup_steps=opt.warmup_steps)\n",
        "\n",
        "    # Stage 1:\n",
        "    # Essentially optim.set_parameters (re-)creates and optimizer using\n",
        "    # model.paramters() as parameters that will be stored in the\n",
        "    # optim.optimizer.param_groups field of the torch optimizer class.\n",
        "    # Importantly, this method does not yet load the optimizer state, as\n",
        "    # essentially it builds a new optimizer with empty optimizer state and\n",
        "    # parameters from the model.\n",
        "    optim.set_parameters(model.named_parameters())\n",
        "\n",
        "    if opt.train_from:\n",
        "        # Stage 2: In this stage, which is only performed when loading an\n",
        "        # optimizer from a checkpoint, we load the saved_optimizer_state_dict\n",
        "        # into the re-created optimizer, to set the optim.optimizer.state\n",
        "        # field, which was previously empty. For this, we use the optimizer\n",
        "        # state saved in the \"saved_optimizer_state_dict\" variable for\n",
        "        # this purpose.\n",
        "        # See also: https://github.com/pytorch/pytorch/issues/2830\n",
        "        optim.optimizer.load_state_dict(saved_optimizer_state_dict)\n",
        "        # Convert back the state values to cuda type if applicable\n",
        "        if use_gpu(opt):\n",
        "            for state in optim.optimizer.state.values():\n",
        "                for k, v in state.items():\n",
        "                    if torch.is_tensor(v):\n",
        "                        state[k] = v.cuda()\n",
        "\n",
        "        # We want to make sure that indeed we have a non-empty optimizer state\n",
        "        # when we loaded an existing model. This should be at least the case\n",
        "        # for Adam, which saves \"exp_avg\" and \"exp_avg_sq\" state\n",
        "        # (Exponential moving average of gradient and squared gradient values)\n",
        "        if (optim.method == 'adam') and (len(optim.optimizer.state) < 1):\n",
        "            raise RuntimeError(\n",
        "                \"Error: loaded Adam optimizer from existing model\" +\n",
        "                \" but optimizer state is empty\")\n",
        "\n",
        "    return optim\n",
        "\n",
        "\n",
        "class MultipleOptimizer(object):\n",
        "    \"\"\" Implement multiple optimizers needed for sparse adam \"\"\"\n",
        "\n",
        "    def __init__(self, op):\n",
        "        \"\"\" ? \"\"\"\n",
        "        self.optimizers = op\n",
        "\n",
        "    def zero_grad(self):\n",
        "        \"\"\" ? \"\"\"\n",
        "        for op in self.optimizers:\n",
        "            op.zero_grad()\n",
        "\n",
        "    def step(self):\n",
        "        \"\"\" ? \"\"\"\n",
        "        for op in self.optimizers:\n",
        "            op.step()\n",
        "\n",
        "    @property\n",
        "    def state(self):\n",
        "        \"\"\" ? \"\"\"\n",
        "        return {k: v for op in self.optimizers for k, v in op.state.items()}\n",
        "\n",
        "    def state_dict(self):\n",
        "        \"\"\" ? \"\"\"\n",
        "        return [op.state_dict() for op in self.optimizers]\n",
        "\n",
        "    def load_state_dict(self, state_dicts):\n",
        "        \"\"\" ? \"\"\"\n",
        "        assert len(state_dicts) == len(self.optimizers)\n",
        "        for i in range(len(state_dicts)):\n",
        "            self.optimizers[i].load_state_dict(state_dicts[i])\n",
        "\n",
        "\n",
        "class Optimizer(object):\n",
        "    \"\"\"\n",
        "    Controller class for optimization. Mostly a thin\n",
        "    wrapper for `optim`, but also useful for implementing\n",
        "    rate scheduling beyond what is currently available.\n",
        "    Also implements necessary methods for training RNNs such\n",
        "    as grad manipulations.\n",
        "    Args:\n",
        "      method (:obj:`str`): one of [sgd, adagrad, adadelta, adam]\n",
        "      lr (float): learning rate\n",
        "      lr_decay (float, optional): learning rate decay multiplier\n",
        "      start_decay_steps (int, optional): step to start learning rate decay\n",
        "      beta1, beta2 (float, optional): parameters for adam\n",
        "      adagrad_accum (float, optional): initialization parameter for adagrad\n",
        "      decay_method (str, option): custom decay options\n",
        "      warmup_steps (int, option): parameter for `noam` decay\n",
        "    We use the default parameters for Adam that are suggested by\n",
        "    the original paper https://arxiv.org/pdf/1412.6980.pdf\n",
        "    These values are also used by other established implementations,\n",
        "    e.g. https://www.tensorflow.org/api_docs/python/tf/train/AdamOptimizer\n",
        "    https://keras.io/optimizers/\n",
        "    Recently there are slightly different values used in the paper\n",
        "    \"Attention is all you need\"\n",
        "    https://arxiv.org/pdf/1706.03762.pdf, particularly the value beta2=0.98\n",
        "    was used there however, beta2=0.999 is still arguably the more\n",
        "    established value, so we use that here as well\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, method, learning_rate, max_grad_norm,\n",
        "                 lr_decay=1, start_decay_steps=None, decay_steps=None,\n",
        "                 beta1=0.9, beta2=0.999,\n",
        "                 adagrad_accum=0.0,\n",
        "                 decay_method=None,\n",
        "                 warmup_steps=4000\n",
        "                 ):\n",
        "        self.last_ppl = None\n",
        "        self.learning_rate = learning_rate\n",
        "        self.original_lr = learning_rate\n",
        "        self.max_grad_norm = max_grad_norm\n",
        "        self.method = method\n",
        "        self.lr_decay = lr_decay\n",
        "        self.start_decay_steps = start_decay_steps\n",
        "        self.decay_steps = decay_steps\n",
        "        self.start_decay = False\n",
        "        self._step = 0\n",
        "        self.betas = [beta1, beta2]\n",
        "        self.adagrad_accum = adagrad_accum\n",
        "        self.decay_method = decay_method\n",
        "        self.warmup_steps = warmup_steps\n",
        "\n",
        "    def set_parameters(self, params):\n",
        "        \"\"\" ? \"\"\"\n",
        "        self.params = []\n",
        "        self.sparse_params = []\n",
        "        for k, p in params:\n",
        "            if p.requires_grad:\n",
        "                if self.method != 'sparseadam' or \"embed\" not in k:\n",
        "                    self.params.append(p)\n",
        "                else:\n",
        "                    self.sparse_params.append(p)\n",
        "        if self.method == 'sgd':\n",
        "            self.optimizer = optim.SGD(self.params, lr=self.learning_rate)\n",
        "        elif self.method == 'adagrad':\n",
        "            self.optimizer = optim.Adagrad(self.params, lr=self.learning_rate)\n",
        "            for group in self.optimizer.param_groups:\n",
        "                for p in group['params']:\n",
        "                    self.optimizer.state[p]['sum'] = self.optimizer\\\n",
        "                        .state[p]['sum'].fill_(self.adagrad_accum)\n",
        "        elif self.method == 'adadelta':\n",
        "            self.optimizer = optim.Adadelta(self.params, lr=self.learning_rate)\n",
        "        elif self.method == 'adam':\n",
        "            self.optimizer = optim.Adam(self.params, lr=self.learning_rate,\n",
        "                                        betas=self.betas, eps=1e-9)\n",
        "        elif self.method == 'sparseadam':\n",
        "            self.optimizer = MultipleOptimizer(\n",
        "                [optim.Adam(self.params, lr=self.learning_rate,\n",
        "                            betas=self.betas, eps=1e-8),\n",
        "                 optim.SparseAdam(self.sparse_params, lr=self.learning_rate,\n",
        "                                  betas=self.betas, eps=1e-8)])\n",
        "        else:\n",
        "            raise RuntimeError(\"Invalid optim method: \" + self.method)\n",
        "\n",
        "    def _set_rate(self, learning_rate):\n",
        "        self.learning_rate = learning_rate\n",
        "        if self.method != 'sparseadam':\n",
        "            self.optimizer.param_groups[0]['lr'] = self.learning_rate\n",
        "        else:\n",
        "            for op in self.optimizer.optimizers:\n",
        "                op.param_groups[0]['lr'] = self.learning_rate\n",
        "\n",
        "    def step(self):\n",
        "        \"\"\"Update the model parameters based on current gradients.\n",
        "        Optionally, will employ gradient modification or update learning\n",
        "        rate.\n",
        "        \"\"\"\n",
        "        self._step += 1\n",
        "\n",
        "        # Decay method used in tensor2tensor.\n",
        "        if self.decay_method == \"noam\":\n",
        "            self._set_rate(\n",
        "                self.original_lr *\n",
        "\n",
        "                 min(self._step ** (-0.5),\n",
        "                     self._step * self.warmup_steps**(-1.5)))\n",
        "\n",
        "            # self._set_rate(self.original_lr *self.model_size ** (-0.5) *min(1.0, self._step / self.warmup_steps)*max(self._step, self.warmup_steps)**(-0.5))\n",
        "        # Decay based on start_decay_steps every decay_steps\n",
        "        else:\n",
        "            if ((self.start_decay_steps is not None) and (\n",
        "                     self._step >= self.start_decay_steps)):\n",
        "                self.start_decay = True\n",
        "            if self.start_decay:\n",
        "                if ((self._step - self.start_decay_steps)\n",
        "                   % self.decay_steps == 0):\n",
        "                    self.learning_rate = self.learning_rate * self.lr_decay\n",
        "\n",
        "        if self.method != 'sparseadam':\n",
        "            self.optimizer.param_groups[0]['lr'] = self.learning_rate\n",
        "\n",
        "        if self.max_grad_norm:\n",
        "            clip_grad_norm_(self.params, self.max_grad_norm)\n",
        "        self.optimizer.step()\n"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WTqMBREs7BTa"
      },
      "source": [
        "# Eval"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IFkD2rB67BTa"
      },
      "source": [
        "class Rouge155(object):\n",
        "    \"\"\"\n",
        "    This is a wrapper for the ROUGE 1.5.5 summary evaluation package.\n",
        "    This class is designed to simplify the evaluation process by:\n",
        "        1) Converting summaries into a format ROUGE understands.\n",
        "        2) Generating the ROUGE configuration file automatically based\n",
        "            on filename patterns.\n",
        "    This class can be used within Python like this:\n",
        "    rouge = Rouge155()\n",
        "    rouge.system_dir = 'test/systems'\n",
        "    rouge.model_dir = 'test/models'\n",
        "    # The system filename pattern should contain one group that\n",
        "    # matches the document ID.\n",
        "    rouge.system_filename_pattern = 'SL.P.10.R.11.SL062003-(\\d+).html'\n",
        "    # The model filename pattern has '#ID#' as a placeholder for the\n",
        "    # document ID. If there are multiple model summaries, pyrouge\n",
        "    # will use the provided regex to automatically match them with\n",
        "    # the corresponding system summary. Here, [A-Z] matches\n",
        "    # multiple model summaries for a given #ID#.\n",
        "    rouge.model_filename_pattern = 'SL.P.10.R.[A-Z].SL062003-#ID#.html'\n",
        "    rouge_output = rouge.evaluate()\n",
        "    print(rouge_output)\n",
        "    output_dict = rouge.output_to_dict(rouge_ouput)\n",
        "    print(output_dict)\n",
        "    ->    {'rouge_1_f_score': 0.95652,\n",
        "         'rouge_1_f_score_cb': 0.95652,\n",
        "         'rouge_1_f_score_ce': 0.95652,\n",
        "         'rouge_1_precision': 0.95652,\n",
        "        [...]\n",
        "    To evaluate multiple systems:\n",
        "        rouge = Rouge155()\n",
        "        rouge.system_dir = '/PATH/TO/systems'\n",
        "        rouge.model_dir = 'PATH/TO/models'\n",
        "        for system_id in ['id1', 'id2', 'id3']:\n",
        "            rouge.system_filename_pattern = \\\n",
        "                'SL.P/.10.R.{}.SL062003-(\\d+).html'.format(system_id)\n",
        "            rouge.model_filename_pattern = \\\n",
        "                'SL.P.10.R.[A-Z].SL062003-#ID#.html'\n",
        "            rouge_output = rouge.evaluate(system_id)\n",
        "            print(rouge_output)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, rouge_dir=None, rouge_args=None, temp_dir = None):\n",
        "        \"\"\"\n",
        "        Create a Rouge155 object.\n",
        "            rouge_dir:  Directory containing Rouge-1.5.5.pl\n",
        "            rouge_args: Arguments to pass through to ROUGE if you\n",
        "                        don't want to use the default pyrouge\n",
        "                        arguments.\n",
        "        \"\"\"\n",
        "        self.temp_dir=temp_dir\n",
        "        self.log = log.get_global_console_logger()\n",
        "        self.__set_dir_properties()\n",
        "        self._config_file = None\n",
        "        self._settings_file = self.__get_config_path()\n",
        "        self.__set_rouge_dir(rouge_dir)\n",
        "        self.args = self.__clean_rouge_args(rouge_args)\n",
        "        self._system_filename_pattern = None\n",
        "        self._model_filename_pattern = None\n",
        "\n",
        "    def save_home_dir(self):\n",
        "        config = ConfigParser()\n",
        "        section = 'pyrouge settings'\n",
        "        config.add_section(section)\n",
        "        config.set(section, 'home_dir', self._home_dir)\n",
        "        with open(self._settings_file, 'w') as f:\n",
        "            config.write(f)\n",
        "        self.log.info(\"Set ROUGE home directory to {}.\".format(self._home_dir))\n",
        "\n",
        "    @property\n",
        "    def settings_file(self):\n",
        "        \"\"\"\n",
        "        Path of the setttings file, which stores the ROUGE home dir.\n",
        "        \"\"\"\n",
        "        return self._settings_file\n",
        "\n",
        "    @property\n",
        "    def bin_path(self):\n",
        "        \"\"\"\n",
        "        The full path of the ROUGE binary (although it's technically\n",
        "        a script), i.e. rouge_home_dir/ROUGE-1.5.5.pl\n",
        "        \"\"\"\n",
        "        if self._bin_path is None:\n",
        "            raise Exception(\n",
        "                \"ROUGE path not set. Please set the ROUGE home directory \"\n",
        "                \"and ensure that ROUGE-1.5.5.pl exists in it.\")\n",
        "        return self._bin_path\n",
        "\n",
        "    @property\n",
        "    def system_filename_pattern(self):\n",
        "        \"\"\"\n",
        "        The regular expression pattern for matching system summary\n",
        "        filenames. The regex string.\n",
        "        E.g. \"SL.P.10.R.11.SL062003-(\\d+).html\" will match the system\n",
        "        filenames in the SPL2003/system folder of the ROUGE SPL example\n",
        "        in the \"sample-test\" folder.\n",
        "        Currently, there is no support for multiple systems.\n",
        "        \"\"\"\n",
        "        return self._system_filename_pattern\n",
        "\n",
        "    @system_filename_pattern.setter\n",
        "    def system_filename_pattern(self, pattern):\n",
        "        self._system_filename_pattern = pattern\n",
        "\n",
        "    @property\n",
        "    def model_filename_pattern(self):\n",
        "        \"\"\"\n",
        "        The regular expression pattern for matching model summary\n",
        "        filenames. The pattern needs to contain the string \"#ID#\",\n",
        "        which is a placeholder for the document ID.\n",
        "        E.g. \"SL.P.10.R.[A-Z].SL062003-#ID#.html\" will match the model\n",
        "        filenames in the SPL2003/system folder of the ROUGE SPL\n",
        "        example in the \"sample-test\" folder.\n",
        "        \"#ID#\" is a placeholder for the document ID which has been\n",
        "        matched by the \"(\\d+)\" part of the system filename pattern.\n",
        "        The different model summaries for a given document ID are\n",
        "        matched by the \"[A-Z]\" part.\n",
        "        \"\"\"\n",
        "        return self._model_filename_pattern\n",
        "\n",
        "    @model_filename_pattern.setter\n",
        "    def model_filename_pattern(self, pattern):\n",
        "        self._model_filename_pattern = pattern\n",
        "\n",
        "    @property\n",
        "    def config_file(self):\n",
        "        return self._config_file\n",
        "\n",
        "    @config_file.setter\n",
        "    def config_file(self, path):\n",
        "        config_dir, _ = os.path.split(path)\n",
        "        verify_dir(config_dir, \"configuration file\")\n",
        "        self._config_file = path\n",
        "\n",
        "    def split_sentences(self):\n",
        "        \"\"\"\n",
        "        ROUGE requires texts split into sentences. In case the texts\n",
        "        are not already split, this method can be used.\n",
        "        \"\"\"\n",
        "        from pyrouge.utils.sentence_splitter import PunktSentenceSplitter\n",
        "        self.log.info(\"Splitting sentences.\")\n",
        "        ss = PunktSentenceSplitter()\n",
        "        sent_split_to_string = lambda s: \"\\n\".join(ss.split(s))\n",
        "        process_func = partial(\n",
        "            DirectoryProcessor.process, function=sent_split_to_string)\n",
        "        self.__process_summaries(process_func)\n",
        "\n",
        "    @staticmethod\n",
        "    def convert_summaries_to_rouge_format(input_dir, output_dir):\n",
        "        \"\"\"\n",
        "        Convert all files in input_dir into a format ROUGE understands\n",
        "        and saves the files to output_dir. The input files are assumed\n",
        "        to be plain text with one sentence per line.\n",
        "            input_dir:  Path of directory containing the input files.\n",
        "            output_dir: Path of directory in which the converted files\n",
        "                        will be saved.\n",
        "        \"\"\"\n",
        "        DirectoryProcessor.process(\n",
        "            input_dir, output_dir, Rouge155.convert_text_to_rouge_format)\n",
        "\n",
        "    @staticmethod\n",
        "    def convert_text_to_rouge_format(text, title=\"dummy title\"):\n",
        "        \"\"\"\n",
        "        Convert a text to a format ROUGE understands. The text is\n",
        "        assumed to contain one sentence per line.\n",
        "            text:   The text to convert, containg one sentence per line.\n",
        "            title:  Optional title for the text. The title will appear\n",
        "                    in the converted file, but doesn't seem to have\n",
        "                    any other relevance.\n",
        "        Returns: The converted text as string.\n",
        "        \"\"\"\n",
        "        # sentences = text.split(\"\\n\")\n",
        "        sentences = text.split(\"<q>\")\n",
        "        sent_elems = [\n",
        "            \"<a name=\\\"{i}\\\">[{i}]</a> <a href=\\\"#{i}\\\" id={i}>\"\n",
        "            \"{text}</a>\".format(i=i, text=sent)\n",
        "            for i, sent in enumerate(sentences, start=1)]\n",
        "        html = \"\"\"<html>\n",
        "<head>\n",
        "<title>{title}</title>\n",
        "</head>\n",
        "<body bgcolor=\"white\">\n",
        "{elems}\n",
        "</body>\n",
        "</html>\"\"\".format(title=title, elems=\"\\n\".join(sent_elems))\n",
        "\n",
        "        return html\n",
        "\n",
        "    @staticmethod\n",
        "    def write_config_static(system_dir, system_filename_pattern,\n",
        "                            model_dir, model_filename_pattern,\n",
        "                            config_file_path, system_id=None):\n",
        "        \"\"\"\n",
        "        Write the ROUGE configuration file, which is basically a list\n",
        "        of system summary files and their corresponding model summary\n",
        "        files.\n",
        "        pyrouge uses regular expressions to automatically find the\n",
        "        matching model summary files for a given system summary file\n",
        "        (cf. docstrings for system_filename_pattern and\n",
        "        model_filename_pattern).\n",
        "            system_dir:                 Path of directory containing\n",
        "                                        system summaries.\n",
        "            system_filename_pattern:    Regex string for matching\n",
        "                                        system summary filenames.\n",
        "            model_dir:                  Path of directory containing\n",
        "                                        model summaries.\n",
        "            model_filename_pattern:     Regex string for matching model\n",
        "                                        summary filenames.\n",
        "            config_file_path:           Path of the configuration file.\n",
        "            system_id:                  Optional system ID string which\n",
        "                                        will appear in the ROUGE output.\n",
        "        \"\"\"\n",
        "        system_filenames = [f for f in os.listdir(system_dir)]\n",
        "        system_models_tuples = []\n",
        "\n",
        "        system_filename_pattern = re.compile(system_filename_pattern)\n",
        "        for system_filename in sorted(system_filenames):\n",
        "            match = system_filename_pattern.match(system_filename)\n",
        "            if match:\n",
        "                id = match.groups(0)[0]\n",
        "                model_filenames = [model_filename_pattern.replace('#ID#',id)]\n",
        "                # model_filenames = Rouge155.__get_model_filenames_for_id(\n",
        "                #     id, model_dir, model_filename_pattern)\n",
        "                system_models_tuples.append(\n",
        "                    (system_filename, sorted(model_filenames)))\n",
        "        if not system_models_tuples:\n",
        "            raise Exception(\n",
        "                \"Did not find any files matching the pattern {} \"\n",
        "                \"in the system summaries directory {}.\".format(\n",
        "                    system_filename_pattern.pattern, system_dir))\n",
        "\n",
        "        with codecs.open(config_file_path, 'w', encoding='utf-8') as f:\n",
        "            f.write('<ROUGE-EVAL version=\"1.55\">')\n",
        "            for task_id, (system_filename, model_filenames) in enumerate(\n",
        "                    system_models_tuples, start=1):\n",
        "\n",
        "                eval_string = Rouge155.__get_eval_string(\n",
        "                    task_id, system_id,\n",
        "                    system_dir, system_filename,\n",
        "                    model_dir, model_filenames)\n",
        "                f.write(eval_string)\n",
        "            f.write(\"</ROUGE-EVAL>\")\n",
        "\n",
        "    def write_config(self, config_file_path=None, system_id=None):\n",
        "        \"\"\"\n",
        "        Write the ROUGE configuration file, which is basically a list\n",
        "        of system summary files and their matching model summary files.\n",
        "        This is a non-static version of write_config_file_static().\n",
        "            config_file_path:   Path of the configuration file.\n",
        "            system_id:          Optional system ID string which will\n",
        "                                appear in the ROUGE output.\n",
        "        \"\"\"\n",
        "        if not system_id:\n",
        "            system_id = 1\n",
        "        if (not config_file_path) or (not self._config_dir):\n",
        "            self._config_dir = mkdtemp(dir=self.temp_dir)\n",
        "            config_filename = \"rouge_conf.xml\"\n",
        "        else:\n",
        "            config_dir, config_filename = os.path.split(config_file_path)\n",
        "            verify_dir(config_dir, \"configuration file\")\n",
        "        self._config_file = os.path.join(self._config_dir, config_filename)\n",
        "        Rouge155.write_config_static(\n",
        "            self._system_dir, self._system_filename_pattern,\n",
        "            self._model_dir, self._model_filename_pattern,\n",
        "            self._config_file, system_id)\n",
        "        self.log.info(\n",
        "            \"Written ROUGE configuration to {}\".format(self._config_file))\n",
        "\n",
        "    def evaluate(self, system_id=1, rouge_args=None):\n",
        "        \"\"\"\n",
        "        Run ROUGE to evaluate the system summaries in system_dir against\n",
        "        the model summaries in model_dir. The summaries are assumed to\n",
        "        be in the one-sentence-per-line HTML format ROUGE understands.\n",
        "            system_id:  Optional system ID which will be printed in\n",
        "                        ROUGE's output.\n",
        "        Returns: Rouge output as string.\n",
        "        \"\"\"\n",
        "        self.write_config(system_id=system_id)\n",
        "        options = self.__get_options(rouge_args)\n",
        "        command = [self._bin_path] + options\n",
        "        self.log.info(\n",
        "            \"Running ROUGE with command {}\".format(\" \".join(command)))\n",
        "        rouge_output = check_output(command).decode(\"UTF-8\")\n",
        "        return rouge_output\n",
        "\n",
        "    def convert_and_evaluate(self, system_id=1,\n",
        "                             split_sentences=False, rouge_args=None):\n",
        "        \"\"\"\n",
        "        Convert plain text summaries to ROUGE format and run ROUGE to\n",
        "        evaluate the system summaries in system_dir against the model\n",
        "        summaries in model_dir. Optionally split texts into sentences\n",
        "        in case they aren't already.\n",
        "        This is just a convenience method combining\n",
        "        convert_summaries_to_rouge_format() and evaluate().\n",
        "            split_sentences:    Optional argument specifying if\n",
        "                                sentences should be split.\n",
        "            system_id:          Optional system ID which will be printed\n",
        "                                in ROUGE's output.\n",
        "        Returns: ROUGE output as string.\n",
        "        \"\"\"\n",
        "        if split_sentences:\n",
        "            self.split_sentences()\n",
        "        self.__write_summaries()\n",
        "        rouge_output = self.evaluate(system_id, rouge_args)\n",
        "        return rouge_output\n",
        "\n",
        "    def output_to_dict(self, output):\n",
        "        \"\"\"\n",
        "        Convert the ROUGE output into python dictionary for further\n",
        "        processing.\n",
        "        \"\"\"\n",
        "        #0 ROUGE-1 Average_R: 0.02632 (95%-conf.int. 0.02632 - 0.02632)\n",
        "        pattern = re.compile(\n",
        "            r\"(\\d+) (ROUGE-\\S+) (Average_\\w): (\\d.\\d+) \"\n",
        "            r\"\\(95%-conf.int. (\\d.\\d+) - (\\d.\\d+)\\)\")\n",
        "        results = {}\n",
        "        for line in output.split(\"\\n\"):\n",
        "            match = pattern.match(line)\n",
        "            if match:\n",
        "                sys_id, rouge_type, measure, result, conf_begin, conf_end = \\\n",
        "                    match.groups()\n",
        "                measure = {\n",
        "                    'Average_R': 'recall',\n",
        "                    'Average_P': 'precision',\n",
        "                    'Average_F': 'f_score'\n",
        "                    }[measure]\n",
        "                rouge_type = rouge_type.lower().replace(\"-\", '_')\n",
        "                key = \"{}_{}\".format(rouge_type, measure)\n",
        "                results[key] = float(result)\n",
        "                results[\"{}_cb\".format(key)] = float(conf_begin)\n",
        "                results[\"{}_ce\".format(key)] = float(conf_end)\n",
        "        return results\n",
        "\n",
        "    ###################################################################\n",
        "    # Private methods\n",
        "\n",
        "    def __set_rouge_dir(self, home_dir=None):\n",
        "        \"\"\"\n",
        "        Verfify presence of ROUGE-1.5.5.pl and data folder, and set\n",
        "        those paths.\n",
        "        \"\"\"\n",
        "        if not home_dir:\n",
        "            self._home_dir = self.__get_rouge_home_dir_from_settings()\n",
        "        else:\n",
        "            self._home_dir = home_dir\n",
        "            self.save_home_dir()\n",
        "        self._bin_path = os.path.join(self._home_dir, 'ROUGE-1.5.5.pl')\n",
        "        self.data_dir = os.path.join(self._home_dir, 'data')\n",
        "        if not os.path.exists(self._bin_path):\n",
        "            raise Exception(\n",
        "                \"ROUGE binary not found at {}. Please set the \"\n",
        "                \"correct path by running pyrouge_set_rouge_path \"\n",
        "                \"/path/to/rouge/home.\".format(self._bin_path))\n",
        "\n",
        "    def __get_rouge_home_dir_from_settings(self):\n",
        "        config = ConfigParser()\n",
        "        with open(self._settings_file) as f:\n",
        "            if hasattr(config, \"read_file\"):\n",
        "                config.read_file(f)\n",
        "            else:\n",
        "                # use deprecated python 2.x method\n",
        "                config.readfp(f)\n",
        "        rouge_home_dir = config.get('pyrouge settings', 'home_dir')\n",
        "        return rouge_home_dir\n",
        "\n",
        "    @staticmethod\n",
        "    def __get_eval_string(\n",
        "            task_id, system_id,\n",
        "            system_dir, system_filename,\n",
        "            model_dir, model_filenames):\n",
        "        \"\"\"\n",
        "        ROUGE can evaluate several system summaries for a given text\n",
        "        against several model summaries, i.e. there is an m-to-n\n",
        "        relation between system and model summaries. The system\n",
        "        summaries are listed in the <PEERS> tag and the model summaries\n",
        "        in the <MODELS> tag. pyrouge currently only supports one system\n",
        "        summary per text, i.e. it assumes a 1-to-n relation between\n",
        "        system and model summaries.\n",
        "        \"\"\"\n",
        "        peer_elems = \"<P ID=\\\"{id}\\\">{name}</P>\".format(\n",
        "            id=system_id, name=system_filename)\n",
        "\n",
        "        model_elems = [\"<M ID=\\\"{id}\\\">{name}</M>\".format(\n",
        "            id=chr(65 + i), name=name)\n",
        "            for i, name in enumerate(model_filenames)]\n",
        "\n",
        "        model_elems = \"\\n\\t\\t\\t\".join(model_elems)\n",
        "        eval_string = \"\"\"\n",
        "    <EVAL ID=\"{task_id}\">\n",
        "        <MODEL-ROOT>{model_root}</MODEL-ROOT>\n",
        "        <PEER-ROOT>{peer_root}</PEER-ROOT>\n",
        "        <INPUT-FORMAT TYPE=\"SEE\">\n",
        "        </INPUT-FORMAT>\n",
        "        <PEERS>\n",
        "            {peer_elems}\n",
        "        </PEERS>\n",
        "        <MODELS>\n",
        "            {model_elems}\n",
        "        </MODELS>\n",
        "    </EVAL>\n",
        "\"\"\".format(\n",
        "            task_id=task_id,\n",
        "            model_root=model_dir, model_elems=model_elems,\n",
        "            peer_root=system_dir, peer_elems=peer_elems)\n",
        "        return eval_string\n",
        "\n",
        "    def __process_summaries(self, process_func):\n",
        "        \"\"\"\n",
        "        Helper method that applies process_func to the files in the\n",
        "        system and model folders and saves the resulting files to new\n",
        "        system and model folders.\n",
        "        \"\"\"\n",
        "        temp_dir = mkdtemp(dir=self.temp_dir)\n",
        "        new_system_dir = os.path.join(temp_dir, \"system\")\n",
        "        os.mkdir(new_system_dir)\n",
        "        new_model_dir = os.path.join(temp_dir, \"model\")\n",
        "        os.mkdir(new_model_dir)\n",
        "        self.log.info(\n",
        "            \"Processing summaries. Saving system files to {} and \"\n",
        "            \"model files to {}.\".format(new_system_dir, new_model_dir))\n",
        "        process_func(self._system_dir, new_system_dir)\n",
        "        process_func(self._model_dir, new_model_dir)\n",
        "        self._system_dir = new_system_dir\n",
        "        self._model_dir = new_model_dir\n",
        "\n",
        "    def __write_summaries(self):\n",
        "        self.log.info(\"Writing summaries.\")\n",
        "        self.__process_summaries(self.convert_summaries_to_rouge_format)\n",
        "\n",
        "    @staticmethod\n",
        "    def __get_model_filenames_for_id(id, model_dir, model_filenames_pattern):\n",
        "        pattern = re.compile(model_filenames_pattern.replace('#ID#', id))\n",
        "        model_filenames = [\n",
        "            f for f in os.listdir(model_dir) if pattern.match(f)]\n",
        "        if not model_filenames:\n",
        "            raise Exception(\n",
        "                \"Could not find any model summaries for the system\"\n",
        "                \" summary with ID {}. Specified model filename pattern was: \"\n",
        "                \"{}\".format(id, model_filenames_pattern))\n",
        "        return model_filenames\n",
        "\n",
        "    def __get_options(self, rouge_args=None):\n",
        "        \"\"\"\n",
        "        Get supplied command line arguments for ROUGE or use default\n",
        "        ones.\n",
        "        \"\"\"\n",
        "        if self.args:\n",
        "            options = self.args.split()\n",
        "        elif rouge_args:\n",
        "            options = rouge_args.split()\n",
        "        else:\n",
        "            options = [\n",
        "                '-e', self._data_dir,\n",
        "                '-c', 95,\n",
        "                # '-2',\n",
        "                # '-1',\n",
        "                # '-U',\n",
        "                '-m',\n",
        "                # '-v',\n",
        "                '-r', 1000,\n",
        "                '-n', 2,\n",
        "                # '-w', 1.2,\n",
        "                '-a',\n",
        "                ]\n",
        "            options = list(map(str, options))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        options = self.__add_config_option(options)\n",
        "        return options\n",
        "\n",
        "    def __create_dir_property(self, dir_name, docstring):\n",
        "        \"\"\"\n",
        "        Generate getter and setter for a directory property.\n",
        "        \"\"\"\n",
        "        property_name = \"{}_dir\".format(dir_name)\n",
        "        private_name = \"_\" + property_name\n",
        "        setattr(self, private_name, None)\n",
        "\n",
        "        def fget(self):\n",
        "            return getattr(self, private_name)\n",
        "\n",
        "        def fset(self, path):\n",
        "            verify_dir(path, dir_name)\n",
        "            setattr(self, private_name, path)\n",
        "\n",
        "        p = property(fget=fget, fset=fset, doc=docstring)\n",
        "        setattr(self.__class__, property_name, p)\n",
        "\n",
        "    def __set_dir_properties(self):\n",
        "        \"\"\"\n",
        "        Automatically generate the properties for directories.\n",
        "        \"\"\"\n",
        "        directories = [\n",
        "            (\"home\", \"The ROUGE home directory.\"),\n",
        "            (\"data\", \"The path of the ROUGE 'data' directory.\"),\n",
        "            (\"system\", \"Path of the directory containing system summaries.\"),\n",
        "            (\"model\", \"Path of the directory containing model summaries.\"),\n",
        "            ]\n",
        "        for (dirname, docstring) in directories:\n",
        "            self.__create_dir_property(dirname, docstring)\n",
        "\n",
        "    def __clean_rouge_args(self, rouge_args):\n",
        "        \"\"\"\n",
        "        Remove enclosing quotation marks, if any.\n",
        "        \"\"\"\n",
        "        if not rouge_args:\n",
        "            return\n",
        "        quot_mark_pattern = re.compile('\"(.+)\"')\n",
        "        match = quot_mark_pattern.match(rouge_args)\n",
        "        if match:\n",
        "            cleaned_args = match.group(1)\n",
        "            return cleaned_args\n",
        "        else:\n",
        "            return rouge_args\n",
        "\n",
        "    def __add_config_option(self, options):\n",
        "        return options + [self._config_file]\n",
        "\n",
        "    def __get_config_path(self):\n",
        "        if platform.system() == \"Windows\":\n",
        "            parent_dir = os.getenv(\"APPDATA\")\n",
        "            config_dir_name = \"pyrouge\"\n",
        "        elif os.name == \"posix\":\n",
        "            parent_dir = os.path.expanduser(\"~\")\n",
        "            config_dir_name = \".pyrouge\"\n",
        "        else:\n",
        "            parent_dir = os.path.dirname(__file__)\n",
        "            config_dir_name = \"\"\n",
        "        config_dir = os.path.join(parent_dir, config_dir_name)\n",
        "        if not os.path.exists(config_dir):\n",
        "            os.makedirs(config_dir)\n",
        "        return os.path.join(config_dir, 'settings.ini')"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K1-5e40z-MRn"
      },
      "source": [
        ""
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8lz0YY97WzwF"
      },
      "source": [
        "#Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rz7qzH4kYihY"
      },
      "source": [
        "def IterFunc(corpus_type='train'):\n",
        "        return Dataloader(args, load_dataset(args, corpus_type, shuffle=True), args[\"batch_size\"], device,\n",
        "                                                 shuffle=True, is_test=False)"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5E8CD-1m7BS7"
      },
      "source": [
        "loss = torch.nn.BCELoss(reduction='none')\n",
        "\n",
        "# def train(model, optim, trainSteps, trainIterFunc, validIterFunc, savePath, seed=42):\n",
        "def train(args, model, savePath, seed=42):\n",
        "    '''\n",
        "    optim : optimizer\n",
        "    savePath: where the model of the last will be saved.\n",
        "    '''\n",
        "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    def trainIterFunc():\n",
        "        tmp = load_dataset(args, corpus_type='train', shuffle=True)\n",
        "        return Dataloader(args, tmp, args[\"batch_size\"], device,\n",
        "                                                 shuffle=True, is_test=False)\n",
        "    def validIterFunc():\n",
        "        tmp = load_dataset(args, corpus_type='valid', shuffle=True)\n",
        "        return Dataloader(args, tmp, args[\"batch_size\"], device,\n",
        "                                                 shuffle=True, is_test=False)\n",
        "        \n",
        "    torch.manual_seed(seed)\n",
        "    random.seed(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    \n",
        "\n",
        "    \n",
        "    model = Summarizer(args, device, load_pretrained_bert=True)\n",
        "    n_gpu = torch.cuda.current_device()\n",
        "    \n",
        "    \n",
        "    if args[\"train_from\"] != '':\n",
        "        print('Loading checkpoint from %s' % args[\"train_from\"])\n",
        "        checkpoint = torch.load(args[\"train_from\"],\n",
        "                                map_location=lambda storage, loc: storage)\n",
        "        opt = vars(checkpoint['opt'])\n",
        "        for k in opt.keys():\n",
        "            if (k in model_flags):\n",
        "                setattr(args, k, opt[k])\n",
        "        model.load_cp(checkpoint)\n",
        "        optim = build_optim(args, model, checkpoint)\n",
        "    else:\n",
        "        optim = build_optim(args, model, None)\n",
        "    \n",
        "    \n",
        "    for step in range(optim._step + 1, args[\"train_steps\"]):\n",
        "        print(\"step\", step)\n",
        "        trainIter = trainIterFunc()\n",
        "        # optim.zero_grad()\n",
        "        trainLoss, trainBatchSize = 0, 0\n",
        "        validLoss, validBatchSize = 0, 0\n",
        "        timeStart = time.time()\n",
        "        \n",
        "        # training \n",
        "        model.train()\n",
        "        for i, batch in enumerate(trainIter):\n",
        "            if batch.bad_batch:\n",
        "              print(\"This is a bad batch\")\n",
        "              continue\n",
        "            print(\"i (batch step)\", i)\n",
        "            labels = batch.labels\n",
        "            segs = batch.segs\n",
        "            cls = batch.clss\n",
        "            mask = batch.mask\n",
        "            mask_cls = batch.mask_cls\n",
        "            src = batch.src\n",
        "\n",
        "\n",
        "            sentenceScores, mask = model(src, segs, cls, mask, mask_cls)\n",
        "            lossVec = loss(sentenceScores, labels.float())\n",
        "            lossMag = (lossVec*mask.float()).sum()\n",
        "            (lossMag/lossMag.numel()).backward()\n",
        "            optim.step()\n",
        "            lossMag.detach()\n",
        "            \n",
        "            trainLoss += lossMag\n",
        "            trainBatchSize += batch.batch_size\n",
        "        \n",
        "        timeEnded_T = time.time()\n",
        "        \n",
        "        # validating \n",
        "        model.eval()\n",
        "        print(\"starting to validate\")\n",
        "        with torch.no_grad():\n",
        "            validIter = validIterFunc() \n",
        "            for batch in validIter:\n",
        "                if batch.bad_batch:\n",
        "                  print(\"This is a bad batch\")\n",
        "                  continue\n",
        "                labels = batch.labels\n",
        "                segs = batch.segs\n",
        "                cls = batch.clss\n",
        "                mask = batch.mask\n",
        "                mask_cls = batch.mask_cls\n",
        "                src = batch.src\n",
        "                \n",
        "                sentenceScores, mask = model(src, segs, cls, mask, mask_cls)\n",
        "                lossVec = loss(sentenceScores, labels.float())\n",
        "                lossMag = (lossVec*mask.float()).sum()\n",
        "                validLoss += lossMag\n",
        "                validBatchSize += batch.batch_size\n",
        "        \n",
        "        timeEnded_V = time.time()\n",
        "        trainTime = timeStart   - timeEnded_T\n",
        "        validTime = timeEnded_T - timeEnded_V\n",
        "        trainLoss /= trainBatchSize\n",
        "        validLoss /= validBatchSize\n",
        "        print(\"Step %s; lr: %7.7f; training loss: %4.2f;\" +\n",
        "             \" trained %6.0f sec; valid loss: %4.2f; validated %6.0f sec\", step, \n",
        "              step, optim.learning_rate, trainLoss, trainTime, validLoss, \n",
        "             validTime)\n",
        "    # saving the model \n",
        "    modelStateDict  = model.state_dict()\n",
        "    # don't have args, check if it's okay when loaded\n",
        "    checkpoint = {\n",
        "        'model' : modelStateDict,\n",
        "        'optim' : optim\n",
        "    }\n",
        "    \n",
        "    checkpointPath = os.path.join(savePath, 'model_step_last.pt')\n",
        "    print(\"Saving checkpoint at\", checkpointPath)\n",
        "    torch.save(checkpoint, checkpointPath)\n",
        "    return checkpoint, checkpointPath\n",
        "    \n",
        "            "
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Rfu4zWoS9sv",
        "outputId": "56158f3d-1edc-4ba3-80e1-82d03f61c6d0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "torch.cuda.empty_cache()\n",
        "import torch.cuda as cutorch\n",
        "\n",
        "print(torch.cuda.memory_stats())"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "OrderedDict([('active.all.allocated', 0), ('active.all.current', 0), ('active.all.freed', 0), ('active.all.peak', 0), ('active.large_pool.allocated', 0), ('active.large_pool.current', 0), ('active.large_pool.freed', 0), ('active.large_pool.peak', 0), ('active.small_pool.allocated', 0), ('active.small_pool.current', 0), ('active.small_pool.freed', 0), ('active.small_pool.peak', 0), ('active_bytes.all.allocated', 0), ('active_bytes.all.current', 0), ('active_bytes.all.freed', 0), ('active_bytes.all.peak', 0), ('active_bytes.large_pool.allocated', 0), ('active_bytes.large_pool.current', 0), ('active_bytes.large_pool.freed', 0), ('active_bytes.large_pool.peak', 0), ('active_bytes.small_pool.allocated', 0), ('active_bytes.small_pool.current', 0), ('active_bytes.small_pool.freed', 0), ('active_bytes.small_pool.peak', 0), ('allocated_bytes.all.allocated', 0), ('allocated_bytes.all.current', 0), ('allocated_bytes.all.freed', 0), ('allocated_bytes.all.peak', 0), ('allocated_bytes.large_pool.allocated', 0), ('allocated_bytes.large_pool.current', 0), ('allocated_bytes.large_pool.freed', 0), ('allocated_bytes.large_pool.peak', 0), ('allocated_bytes.small_pool.allocated', 0), ('allocated_bytes.small_pool.current', 0), ('allocated_bytes.small_pool.freed', 0), ('allocated_bytes.small_pool.peak', 0), ('allocation.all.allocated', 0), ('allocation.all.current', 0), ('allocation.all.freed', 0), ('allocation.all.peak', 0), ('allocation.large_pool.allocated', 0), ('allocation.large_pool.current', 0), ('allocation.large_pool.freed', 0), ('allocation.large_pool.peak', 0), ('allocation.small_pool.allocated', 0), ('allocation.small_pool.current', 0), ('allocation.small_pool.freed', 0), ('allocation.small_pool.peak', 0), ('inactive_split.all.allocated', 0), ('inactive_split.all.current', 0), ('inactive_split.all.freed', 0), ('inactive_split.all.peak', 0), ('inactive_split.large_pool.allocated', 0), ('inactive_split.large_pool.current', 0), ('inactive_split.large_pool.freed', 0), ('inactive_split.large_pool.peak', 0), ('inactive_split.small_pool.allocated', 0), ('inactive_split.small_pool.current', 0), ('inactive_split.small_pool.freed', 0), ('inactive_split.small_pool.peak', 0), ('inactive_split_bytes.all.allocated', 0), ('inactive_split_bytes.all.current', 0), ('inactive_split_bytes.all.freed', 0), ('inactive_split_bytes.all.peak', 0), ('inactive_split_bytes.large_pool.allocated', 0), ('inactive_split_bytes.large_pool.current', 0), ('inactive_split_bytes.large_pool.freed', 0), ('inactive_split_bytes.large_pool.peak', 0), ('inactive_split_bytes.small_pool.allocated', 0), ('inactive_split_bytes.small_pool.current', 0), ('inactive_split_bytes.small_pool.freed', 0), ('inactive_split_bytes.small_pool.peak', 0), ('num_alloc_retries', 0), ('num_ooms', 0), ('reserved_bytes.all.allocated', 0), ('reserved_bytes.all.current', 0), ('reserved_bytes.all.freed', 0), ('reserved_bytes.all.peak', 0), ('reserved_bytes.large_pool.allocated', 0), ('reserved_bytes.large_pool.current', 0), ('reserved_bytes.large_pool.freed', 0), ('reserved_bytes.large_pool.peak', 0), ('reserved_bytes.small_pool.allocated', 0), ('reserved_bytes.small_pool.current', 0), ('reserved_bytes.small_pool.freed', 0), ('reserved_bytes.small_pool.peak', 0), ('segment.all.allocated', 0), ('segment.all.current', 0), ('segment.all.freed', 0), ('segment.all.peak', 0), ('segment.large_pool.allocated', 0), ('segment.large_pool.current', 0), ('segment.large_pool.freed', 0), ('segment.large_pool.peak', 0), ('segment.small_pool.allocated', 0), ('segment.small_pool.current', 0), ('segment.small_pool.freed', 0), ('segment.small_pool.peak', 0)])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "sQ85GZ5a7BS-",
        "outputId": "5e39c8e3-f523-41ed-f0fd-ee62be78ec2f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# python train.py -mode train -encoder classifier -dropout 0.1 \n",
        "# -bert_data_path ../bert_data/cnndm -model_path ../models/bert_classifier -lr 2e-3\n",
        "# -visible_gpus 0,1,2  -gpu_ranks 0,1,2 -world_size 3 -report_every 50 -save_checkpoint_steps 1000 \n",
        "# -batch_size 3000 -decay_method noam -train_steps 50000 -accum_count 2 -log_file ../logs/bert_classifier -use_interval true -warmup_steps 10000\n",
        "\n",
        "# torch.random.manual_seed(42)\n",
        "# t = TransformerInterEncoder2(4,2,2,.1,1)\n",
        "# a=torch.tensor([[[1.0,2,3,1],[1.0,2,3,1]]])\n",
        "# d=torch.tensor([[True]])\n",
        "# t(a,d)\n",
        "\n",
        "\n",
        "argDict = {\n",
        "    \"temp_dir\": \"tmp\",\n",
        "    \"ff_size\": 2,\n",
        "    \"heads\": 1,\n",
        "    \"dropout\": 0.1,\n",
        "    \"inter_layers\": 1,\n",
        "    \"param_init\": 0,\n",
        "    \"param_init_glorot\": False,\n",
        "    \"train_from\": \"\",\n",
        "    \"train_steps\":1, \n",
        "    \"use_interval\": True, \n",
        "    \"batch_size\": 2500,\n",
        "    \"lr\":1,\n",
        "    \"beta1\":.9,\n",
        "    \"beta2\":.999,\n",
        "    \"max_grad_norm\": 0,\n",
        "    \"visible_gpus\": -1, \n",
        "    \"warmup_steps\": 10,\n",
        "    \"decay_method\":'',\n",
        "    \"optim\": 'adam'\n",
        "}\n",
        "device = \"cpu\" \n",
        "savePath = \"results/\"\n",
        "model = Summarizer(argDict, device, load_pretrained_bert=True)\n",
        "blah = train(argDict, model,savePath)\n",
        "# optim = build_optim(argDict, model, None)\n",
        "# config = BertConfig.from_json_file(\"testBert.json\")\n",
        "# summ = Summarizer(argDict, device, bert_config=config)\n",
        "# model = Summarizer(argDict, device, load_pretrained_bert=True)\n",
        "\n",
        "\n",
        "# train(model, optim, trainSteps, trainIterFunc, validIterFunc, savePath)\n"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "we created an optimizer\n",
            "Saving checkpoint at results/model_step_last.pt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RMXFIu5DXb4v"
      },
      "source": [
        ""
      ],
      "execution_count": 23,
      "outputs": []
    }
  ]
}